
###### LangMem ######
https://langchain-ai.github.io/langmem/concepts/conceptual_guide/
Semantic Memory: Facts and Knowledge¶
Semantic memory stores the essential facts and other information that ground an agent's responses. Two common representations of semantic memory are collections (to record an unbounded amount of knowledge to be searched at runtime) and profiles (to record task-specific information that follows a strict schema that is easily looked up by user or agent).

Collection¶
Collections are what most people think of when they imagine agent long-term memory. In this type, memories are stored as individual documents or records. For each new conversation, the memory system can decide to insert new memories to the store.

Using a collection-type memory adds some complexity to the process of updating your memory state. The system must reconcile new information with previous beliefs, either deleting/invalidating or updating/consolidating existing memories. If the system over-extracts, this could lead to reduced precision of memories when your agent needs to search the store. If it under-extracts, this could lead to low recall. LangMem uses a memory enrichment process that strives to balance memory creation and consolidation, while letting you, the developer, customize the instructions to further shift the strength of each.

Finally, memory relevance is more than just semantic similarity. Recall should combine similarity with "importance" of the memory, as well as the memory's "strength", which is a function of how recently/frequently it was used.

memories = manager.invoke({"messages": conversation})
# Example memories:
# [
#     ExtractedMemory(
#         id="27e96a9d-8e53-4031-865e-5ec50c1f7ad5",
#         content=Memory(
#             content="[IMPORTANT] User prefers to be called Lex (short for Alex) and appreciates"
#             " casual, witty communication style with relevant emojis."
#         ),
#     ),
#     ExtractedMemory(
#         id="e2f6b646-cdf1-4be1-bb40-0fd91d25d00f",
#         content=Memory(
#             content="[BACKGROUND] Lex is proficient in Python programming and specializes in developing"
#             " AI systems with a focus on making them sound more natural and less corporate."
#         ),
#     ),
#     ExtractedMemory(
#         id="c1e03ebb-a393-4e8d-8eb7-b928d8bed510",
#         content=Memory(
#             content="[HOBBY] Lex is a competitive speedcuber (someone who solves Rubik's cubes competitively),"
#             " showing an interest in both technical and recreational puzzle-solving."
#         ),
#     ),
#     ExtractedMemory(
#         id="ee7fc6e4-0118-425f-8704-6b3145881ff7",
#         content=Memory(
#             content="[PERSONALITY] Based on communication style and interests, Lex appears to value authenticity,"
#             " creativity, and technical excellence while maintaining a fun, approachable demeanor."
#         ),
#     ),
# ]

Moj коментар: (Дали нивото на важност треба да биди во самиот текст. Негде нивото е на важност негде е тема како Хоби итн... Плус не ја наоѓам свежината на меморискиот факт како особина на објектот (how recently/frequently))

After the first short interaction, the system has extracted some semantic triples:


# First conversation - extract triples
conversation1 = [
    {"role": "user", "content": "Alice manages the ML team and mentors Bob, who is also on the team."}
]
memories = manager.invoke({"messages": conversation1})
print("After first conversation:")
for m in memories:
    print(m)
# ExtractedMemory(id='f1bf258c-281b-4fda-b949-0c1930344d59', content=Triple(subject='Alice', predicate='manages', object='ML_team', context=None))
# ExtractedMemory(id='0214f151-b0c5-40c4-b621-db36b845956c', content=Triple(subject='Alice', predicate='mentors', object='Bob', context=None))
# ExtractedMemory(id='258dbf2d-e4ac-47ac-8ffe-35c70a3fe7fc', content=Triple(subject='Bob', predicate='is_member_of', object='ML_team', context=None))
The second conversation updates some existing memories. Since we have enabled "deletes", the manager will return RemoveDoc objects to indicate that the memory should be removed, and a new memory will be created in its place. Since this uses the core "functional" API (aka, it doesn't read or write to a database), you can control what "removal" means, be that a soft or hard delete, or simply a down-weighting of the memory.


# Second conversation - update and add triples
conversation2 = [
    {"role": "user", "content": "Bob now leads the ML team and the NLP project."}
]
update = manager.invoke({"messages": conversation2, "existing": memories})
print("After second conversation:")
for m in update:
    print(m)
# ExtractedMemory(id='65fd9b68-77a7-4ea7-ae55-66e1dd603046', content=RemoveDoc(json_doc_id='f1bf258c-281b-4fda-b949-0c1930344d59'))
# ExtractedMemory(id='7f8be100-5687-4410-b82a-fa1cc8d304c0', content=Triple(subject='Bob', predicate='leads', object='ML_team', context=None))
# ExtractedMemory(id='f4c09154-2557-4e68-8145-8ccd8afd6798', content=Triple(subject='Bob', predicate='leads', object='NLP_project', context=None))
# ExtractedMemory(id='f1bf258c-281b-4fda-b949-0c1930344d59', content=Triple(subject='Alice', predicate='manages', object='ML_team', context=None))
# ExtractedMemory(id='0214f151-b0c5-40c4-b621-db36b845956c', content=Triple(subject='Alice', predicate='mentors', object='Bob', context=None))
# ExtractedMemory(id='258dbf2d-e4ac-47ac-8ffe-35c70a3fe7fc', content=Triple(subject='Bob', predicate='is_member_of', object='ML_team', context=None))
existing = [m for m in update if isinstance(m.content, Triple)]
The third conversation overwrites even more memories.


# Delete triples about an entity
conversation3 = [
    {"role": "user", "content": "Alice left the company."}
]
final = manager.invoke({"messages": conversation3, "existing": existing})
print("After third conversation:")
for m in final:
    print(m)
# ExtractedMemory(id='7ca76217-66a4-4041-ba3d-46a03ea58c1b', content=RemoveDoc(json_doc_id='f1bf258c-281b-4fda-b949-0c1930344d59'))
# ExtractedMemory(id='35b443c7-49e2-4007-8624-f1d6bcb6dc69', content=RemoveDoc(json_doc_id='0214f151-b0c5-40c4-b621-db36b845956c'))
# ExtractedMemory(id='65fd9b68-77a7-4ea7-ae55-66e1dd603046', content=RemoveDoc(json_doc_id='f1bf258c-281b-4fda-b949-0c1930344d59'))
# ExtractedMemory(id='7f8be100-5687-4410-b82a-fa1cc8d304c0', content=Triple(subject='Bob', predicate='leads', object='ML_team', context=None))
# ExtractedMemory(id='f4c09154-2557-4e68-8145-8ccd8afd6798', content=Triple(subject='Bob', predicate='leads', object='NLP_project', context=None))
# ExtractedMemory(id='f1bf258c-281b-4fda-b949-0c1930344d59', content=Triple(subject='Alice', predicate='manages', object='ML_team', context=None))
# ExtractedMemory(id='0214f151-b0c5-40c4-b621-db36b845956c', content=Triple(subject='Alice', predicate='mentors', object='Bob', context=None))
# ExtractedMemory(id='258dbf2d-e4ac-47ac-8ffe-35c70a3fe7fc', content=Triple(subject='Bob', predicate='is_member_of', object='ML_team', context=None))


###### Hindsight ######
# Simple retain
client.retain(
    bank_id="my-bank",
    content="Alice works at Google as a software engineer"
)

# View the stored document in the UI:
print(f"View documents: {HINDSIGHT_UI_URL}/banks/my-bank?view=documents")
# Retain with context and timestamp
client.retain(
    bank_id="my-bank",
    content="Alice got promoted to senior engineer",
    context="career update",
    timestamp="2025-06-15T10:00:00Z"
)
Recall: Retrieve Memories
The recall operation retrieves memories matching a query. It performs 4 retrieval strategies in parallel:

Semantic: Vector similarity
Keyword: BM25 exact matching
Graph: Entity/temporal/causal links
Temporal: Time range filtering


# Simple recall
results = client.recall(bank_id="my-bank", query="What does Alice do?")

print("Memories:")
for r in results.results:
    print(f"  - {r.text}")
# Temporal recall
results = client.recall(bank_id="my-bank", query="What happened in June?");;

print("Memories:")
for r in results.results:
    print(f"  - {r.text}")


response = client.recall(
    bank_id="my-bank",
    query="What does Alice do?",
    types=["world", "experience"],
    budget="high",
    max_tokens=8000,
    trace=True,
    include_entities=True,
    max_entity_tokens=500
)

# Access results
for r in response.results:
    print(f"- {r.text}")

# Access entity observations (if include_entities=True)
if response.entities:
    for entity_id, entity in response.entities.items():
        print(f"Entity: {entity.canonical_name}")

Filter by Fact Type
Recall specific memory types:

Python
CLI
# Only world facts (objective information)
world_facts = client.recall(
    bank_id="my-bank",
    query="Where does Alice work?",
    types=["world"]
)

# Only experience (conversations and events)
experience = client.recall(
    bank_id="my-bank",
    query="What have I recommended?",
    types=["experience"]
)

# Only opinions (formed beliefs)
opinions = client.recall(
    bank_id="my-bank",
    query="What do I think about Python?",
    types=["opinion"]
)

About Opinions
Opinions are beliefs formed during reflect operations. Unlike world facts and experience, opinions are subjective interpretations and may not represent objective truth. Depending on your use case:

Exclude opinions (types=["world", "experience"]) when you need factual, verifiable information
Include opinions when you want the agent's perspective or formed beliefs
Use opinions alone (types=["opinion"]) only when specifically asking about the agent's views    


### Reflect: Generate Insights ###
The reflect operation performs a more thorough analysis of existing memories. This allows the agent to form new connections between memories which are then persisted as opinions and/or observations.

Example use cases:

An AI Project Manager reflecting on what risks need to be mitigated
A Sales Agent reflecting on why certain outreach messages have gotten responses
A Support Agent reflecting on opportunities where customers have unanswered questions
response = client.reflect(bank_id="my-bank", query="What should I know about Alice?")
print(response)
Memory Types
Hindsight organizes memory into four networks to mimic human memory:

World: Facts about the world ("The stove gets hot")
Experiences: Agent's own experiences ("I touched the stove and it really hurt")
Opinion: Beliefs with confidence scores ("I shouldn't touch the stove again" - .99 confidence)
Observation: Complex mental models derived by reflecting on facts and experiences
Cleanup
Delete the bank created during this notebook:

import requests

response = requests.delete(f"{HINDSIGHT_API_URL}/v1/default/banks/my-bank")
print(f"Deleted my-bank: {response.json()}")


Entities are the people, organizations, places, and concepts that Hindsight automatically extracts and tracks across your memory bank.

Automatic Feature
You don't need to do anything to use entities—Hindsight extracts them automatically when you call retain. However, understanding how entities work is important because they power key features in recall and reflect.

Why Entities Matter
Entities improve recall quality in two ways:

Co-occurrence tracking — When entities appear together in facts, Hindsight builds a graph of relationships. This enables graph-based recall to find indirect connections.

Observations — Hindsight synthesizes high-level summaries about each entity from multiple facts. Including entity observations in recall provides richer context.

What Gets Extracted?
When you retain information, the LLM extracts named entities from each fact:

People — Names like "Alice", "Dr. Smith", "CEO John"
Organizations — Companies, teams, institutions
Places — Cities, countries, specific locations
Products/Objects — Software, tools, significant items
Concepts — Abstract themes like "career growth", "friendship"
Example:

Content: "Alice works at Google in Mountain View. She specializes in TensorFlow."

Entities extracted:
- Alice (person)
- Google (organization)
- Mountain View (location)
- TensorFlow (product)

Entity Resolution
When the same entity is mentioned multiple times (possibly with different names), Hindsight resolves them to a single canonical entity using a scoring algorithm:

Resolution Factors
Name similarity (50%) — How closely the text matches existing entity names. Handles variations like "Alice" vs "Alice Chen" or partial matches.

Co-occurrence (30%) — Entities that frequently appear together are more likely to be the same. If "Alice" always appears with "Google" and "TensorFlow", a new mention of "Alice" near those entities scores higher for matching.

Temporal proximity (20%) — Recent mentions are weighted more heavily. If an entity was seen in the last 7 days, new similar mentions are more likely to match.

Resolution Threshold
A match requires a combined score above 0.6 (60%). Below this threshold, Hindsight creates a new entity rather than risk merging distinct entities.

This means:

Exact name matches with recent co-occurring entities → strong match
Partial name matches without context → likely creates new entity
Same name in completely different contexts → may create separate entities
Entity Observations
Observations are derived state—high-level summaries that Hindsight automatically synthesizes from the facts associated with an entity. They provide a condensed view of what the system knows about important entities.

Example:

Facts about Alice:

"Alice works at Google"
"Alice is a software engineer"
"Alice specializes in ML"
"Alice joined Google in 2020"
"Alice leads the search team"
Observation created:

"Alice is a software engineer at Google who joined in 2020, specializes in ML, and leads the search team"
How Observations Work
Observations are not generated for every entity. When you retain new documents:

Top entities selected — Hindsight identifies the top 5 most-mentioned entities in the batch
Threshold check — Only entities with at least 5 facts get observations
Regeneration — Observations are regenerated using the entity's most recent 50 facts
Old observations replaced — Previous observations are deleted and new ones created
This means:

Frequently mentioned entities get observations; rarely mentioned ones don't
Observations stay up-to-date as new information is retained
The system prioritizes entities that matter most to your memory bank
Observations vs Opinions
Observations are objective summaries—they synthesize facts without any bias or perspective. This is different from opinions, which are influenced by the memory bank's disposition.


Need your agent to learn from experience? Here's how to use LangMem for experience replay—capturing not just what happened, but the complete chain of thought that led to success. While semantic memory builds knowledge ("what"), episodic memory captures expertise ("how").

Without storage¶
Below is an example of how to use LangMem to extract episodic memories without storage. Feel free to adapt and modify the code to your needs.

API: create_memory_manager


from langmem import create_memory_manager
from pydantic import BaseModel, Field


class Episode(BaseModel):  
    """Write the episode from the perspective of the agent within it. Use the benefit of hindsight to record the memory, saving the agent's key internal thought process so it can learn over time."""

    observation: str = Field(..., description="The context and setup - what happened")
    thoughts: str = Field(
        ...,
        description="Internal reasoning process and observations of the agent in the episode that let it arrive"
        ' at the correct action and result. "I ..."',
    )
    action: str = Field(
        ...,
        description="What was done, how, and in what format. (Include whatever is salient to the success of the action). I ..",
    )
    result: str = Field(
        ...,
        description="Outcome and retrospective. What did you do well? What could you do better next time? I ...",
    )

manager = create_memory_manager(
    "anthropic:claude-3-5-sonnet-latest",
    schemas=[Episode],  
    instructions="Extract examples of successful explanations, capturing the full chain of reasoning. Be concise in your explanations and precise in the logic of your reasoning.",
    enable_inserts=True,
)
After a successful explanation:


conversation = [
    {
        "role": "user",
        "content": "What's a binary tree? I work with family trees if that helps",
    },
    {
        "role": "assistant",
        "content": "A binary tree is like a family tree, but each parent has at most 2 children. Here's a simple example:\n   Bob\n  /  \\\nAmy  Carl\n\nJust like in family trees, we call Bob the 'parent' and Amy and Carl the 'children'.",
    },
    {
        "role": "user",
        "content": "Oh that makes sense! So in a binary search tree, would it be like organizing a family by age?",
    },
]

episodes = manager.invoke({"messages": conversation})
print(episodes[0])

# ExtractedMemory(
#     id="2e5c551f-58a7-40c2-96b3-cabdfa5ccb31",
#     content=Episode(
#         observation="In a teaching interaction, I used a family tree analogy to explain binary trees, which led to a successful understanding. The student then made an insightful connection to binary search trees and age ordering.",
#         thoughts="I noticed that connecting abstract data structures to familiar concepts like family relationships made the concept more accessible. The student's quick grasp and ability to extend the analogy to binary search trees showed the effectiveness of this approach. Using relatable examples helps bridge the gap between technical concepts and everyday understanding.",
#         action='I explained binary trees using a family tree metaphor, drawing a simple diagram with "Bob" as parent and "Amy" and "Carl" as children. This visualization provided a concrete, relatable example that built on the student\'s existing knowledge of family trees.',
#         result="The explanation was highly successful, evidenced by the student's immediate comprehension (\"Oh that makes sense!\") and their ability to make the cognitive leap to understanding binary search trees' ordering property. For future explanations, I should continue using familiar analogies while being prepared to build upon them for more complex concepts. The family tree analogy proved particularly effective for explaining hierarchical structures.",
#     ),
# )

gradient 
from typing import Any, Optional, Union

import langsmith as ls
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import AIMessage
from langchain_core.runnables import Runnable, RunnableConfig
from trustcall import create_extractor
from typing_extensions import TypedDict

from langmem import utils
from langmem.prompts import types as prompt_types

DEFAULT_MAX_REFLECTION_STEPS = 5
DEFAULT_MIN_REFLECTION_STEPS = 1

DEFAULT_GRADIENT_PROMPT = """You are reviewing the performance of an AI assistant in a given interaction. 

## Instructions

The current prompt that was used for the session is provided below.

<current_prompt>
{prompt}
</current_prompt>

The developer provided the following instructions around when and how to update the prompt:

<update_instructions>
{update_instructions}
</update_instructions>

## Session data

Analyze the following trajectories (and any associated user feedback) (either conversations with a user or other work that was performed by the assistant):

<trajectories>
{trajectories}
</trajectories>

## Task

Analyze the conversation, including the user’s request and the assistant’s response, and evaluate:
1. How effectively the assistant fulfilled the user’s intent.
2. Where the assistant might have deviated from user expectations or the desired outcome.
3. Specific areas (correctness, completeness, style, tone, alignment, etc.) that need improvement.

If the prompt seems to do well, then no further action is needed. We ONLY recommend updates if there is evidence of failures.
When failures occur, we want to recommend the minimal required changes to fix the problem.

Focus on actionable changes and be concrete.

1. Summarize the key successes and failures in the assistant’s response. 
2. Identify which failure mode(s) best describe the issues (examples: style mismatch, unclear or incomplete instructions, flawed logic or reasoning, hallucination, etc.).
3. Based on these failure modes, recommend the most suitable edit strategy. For example, consider::
   - Use synthetic few-shot examples for style or clarifying decision boundaries.
   - Use explicit instruction updates for conditionals, rules, or logic fixes.
   - Provide step-by-step reasoning guidelines for multi-step logic problems.
4. Provide detailed, concrete suggestions for how to update the prompt accordingly.

But remember, the final updated prompt should only be changed if there is evidence of poor performance, and our recommendations should be minimally invasive.
Do not recommend generic changes that aren't clearly linked to failure modes.

First think through the conversation and critique the current behavior.
If you believe the prompt needs to further adapt to the target context, provide precise recommendations.
Otherwise, mark `warrants_adjustment` as False and respond with 'No recommendations.'"""

DEFAULT_GRADIENT_METAPROMPT = """You are optimizing a prompt to handle its target task more effectively.

<current_prompt>
{current_prompt}
</current_prompt>

We hypothesize the current prompt underperforms for these reasons:

<hypotheses>
{hypotheses}
</hypotheses>

Based on these hypotheses, we recommend the following adjustments:

<recommendations>
{recommendations}
</recommendations>

Respond with the updated prompt. Remember to ONLY make changes that are clearly necessary. Aim to be minimally invasive:"""


class GradientOptimizerConfig(TypedDict, total=False):
    """Configuration for the gradient optimizer."""

    gradient_prompt: str
    metaprompt: str
    max_reflection_steps: int
    min_reflection_steps: int


# For uniformity, let's define the expected "input" structure:
class GradientOptimizerInput(TypedDict, total=False):
    """Input to the gradient optimizer."""

    trajectories: prompt_types.OptimizerInput | str
    prompt: str | prompt_types.Prompt


class GradientPromptOptimizer(Runnable[GradientOptimizerInput, str]):
    """
    Class-based Gradient Prompt Optimizer with both sync and async entry points (invoke/ainvoke).
    Mirrors the logic in create_gradient_prompt_optimizer, but avoids duplication
    by splitting the reflection loop and final update into dedicated sync/async methods.
    """

    def __init__(
        self,
        model: Union[str, BaseChatModel],
        config: Optional[GradientOptimizerConfig] = None,
    ):
        self.model = model
        config = config or {}
        self._config = GradientOptimizerConfig(
            gradient_prompt=config.get("gradient_prompt", DEFAULT_GRADIENT_PROMPT),
            metaprompt=config.get("metaprompt", DEFAULT_GRADIENT_METAPROMPT),
            max_reflection_steps=config.get(
                "max_reflection_steps", DEFAULT_MAX_REFLECTION_STEPS
            ),
            min_reflection_steps=config.get(
                "min_reflection_steps", DEFAULT_MIN_REFLECTION_STEPS
            ),
        )

        def think(thought: str) -> str:
            """A reflection tool, used to reason over complexities and hypothesize fixes."""
            return "Take your time thinking through problems."

        def critique(criticism: str) -> str:
            """A critique tool for diagnosing flaws in reasoning."""
            return "Reflect critically on the previous hypothesis."

        def recommend(
            warrants_adjustment: bool,
            hypotheses: Optional[str] = None,
            full_recommendations: Optional[str] = None,
        ) -> str:
            """
            Decides whether a prompt should be adjusted.
            If warrants_adjustment is True, we incorporate recommended changes.
            If not, we respond 'No recommendations.'
            """
            return ""

        self.just_think_chain = create_extractor(
            model,
            tools=[think, critique],
            tool_choice="any",
        )
        self.any_chain = create_extractor(
            model,
            tools=[think, critique, recommend],
            tool_choice="any",
        )
        self.final_chain = create_extractor(
            model,
            tools=[recommend],
            tool_choice="recommend",
        )

    async def _areact_agent(self, inputs: str) -> Any:
        """
        Async version of the reflection loop.
        Follows the logic of your old react_agent, but coded inline, returning the final "recommend" response object.
        """
        messages = [{"role": "user", "content": inputs}]
        max_steps = self._config["max_reflection_steps"]
        min_steps = self._config["min_reflection_steps"]

        for ix in range(max_steps):
            # Choose chain:
            if ix == max_steps - 1:
                chain = self.final_chain
            elif ix < min_steps:
                chain = self.just_think_chain
            else:
                chain = self.any_chain

            response = await chain.ainvoke(messages)

            # Look for a final "recommend" response in the chain output:
            final_response = next(
                (r for r in response["responses"] if r.__repr_name__() == "recommend"),
                None,
            )
            if final_response:
                return final_response

            # Otherwise keep looping:
            msg: AIMessage = response["messages"][-1]
            messages.append(msg)
            # Insert special "tool" role messages if the AI message invoked tools:
            for tc in msg.tool_calls or []:
                messages.append(
                    {"role": "tool", "content": "", "tool_call_id": tc["id"]}
                )

        raise ValueError(
            f"Failed to generate a final recommendation after {max_steps} attempts"
        )

    def _react_agent(self, inputs: str) -> Any:
        """
        Sync version of the reflection loop.
        We do the same logic but calling chain.invoke() instead of chain.ainvoke().
        """
        messages = [{"role": "user", "content": inputs}]
        max_steps = self._config["max_reflection_steps"]
        min_steps = self._config["min_reflection_steps"]

        for ix in range(max_steps):
            if ix == max_steps - 1:
                chain = self.final_chain
            elif ix < min_steps:
                chain = self.just_think_chain
            else:
                chain = self.any_chain

            response = chain.invoke(messages)

            final_response = next(
                (r for r in response["responses"] if r.__repr_name__() == "recommend"),
                None,
            )
            if final_response:
                return final_response

            msg: AIMessage = response["messages"][-1]
            messages.append(msg)
            for tc in msg.tool_calls or []:
                messages.append(
                    {"role": "tool", "content": "", "tool_call_id": tc["id"]}
                )

        raise ValueError(
            f"Failed to generate a final recommendation after {max_steps} attempts"
        )

    async def _aupdate_prompt(
        self,
        hypotheses: str,
        recommendations: str,
        current_prompt: str,
        update_instructions: str,
    ) -> str:
        """
        Async version of the final update step.
        Uses a specialized extractor with a schema tool to parse the improved prompt.
        """
        schema = utils.get_prompt_extraction_schema(current_prompt)
        extractor = create_extractor(
            self.model,
            tools=[schema],
            tool_choice="OptimizedPromptOutput",
        )
        prompt_input = self._config["metaprompt"].format(
            current_prompt=current_prompt,
            recommendations=recommendations,
            hypotheses=hypotheses,
            update_instructions=update_instructions,
        )
        result = await extractor.ainvoke(prompt_input)
        return result["responses"][0].improved_prompt

    def _update_prompt(
        self,
        hypotheses: str,
        recommendations: str,
        current_prompt: str,
        update_instructions: str,
    ) -> str:
        """Sync version of the final update step."""
        schema = utils.get_prompt_extraction_schema(current_prompt)
        extractor = create_extractor(
            self.model,
            tools=[schema],
            tool_choice="OptimizedPromptOutput",
        )
        prompt_input = self._config["metaprompt"].format(
            current_prompt=current_prompt,
            recommendations=recommendations,
            hypotheses=hypotheses,
            update_instructions=update_instructions,
        )
        result = extractor.invoke(prompt_input)
        return result["responses"][0].improved_prompt

    def _process_input(
        self, input: GradientOptimizerInput
    ) -> tuple[str, str, str, str]:
        """
        Extract prompt_str, sessions_str, feedback, update_instructions from input.
        """
        prompt_data = input["prompt"]
        sessions_data = input["trajectories"]

        if isinstance(prompt_data, str):
            prompt_str = prompt_data
            feedback = ""
            update_instructions = ""
        else:
            prompt_str = prompt_data.get("prompt", "")
            feedback = prompt_data.get("feedback", "")
            update_instructions = prompt_data.get("update_instructions", "")

        if isinstance(sessions_data, str):
            sessions_str = sessions_data
        else:
            sessions_str = utils.format_sessions(sessions_data) if sessions_data else ""

        return prompt_str, sessions_str, feedback, update_instructions

    async def ainvoke(
        self,
        input: GradientOptimizerInput,
        config: Optional[RunnableConfig] = None,
        **kwargs: Any,
    ) -> str:
        """
        The async entry point. This is analogous to your old `optimize_prompt`.
        1) Process input
        2) Run reflection steps (async)
        3) If not warrants_adjustment -> return original prompt
           else run final update (async)
        """
        with ls.trace(
            name="gradient_optimizer", inputs=input, metadata={"kind": "gradient"}
        ):
            prompt_str, sessions_str, feedback, update_instructions = (
                self._process_input(input)
            )
            if not sessions_str:
                return prompt_str  # no trajectories => no change

            # Format the initial question to the reflection chain:
            reflection_input = self._config["gradient_prompt"].format(
                trajectories=sessions_str,
                feedback=feedback,
                prompt=prompt_str,
                update_instructions=update_instructions,
            )

            # 1) reflection steps:
            final_response = await self._areact_agent(reflection_input)
            if not final_response.warrants_adjustment:
                return prompt_str

            # 2) final update if warranted:
            improved_prompt = await self._aupdate_prompt(
                final_response.hypotheses,
                final_response.full_recommendations,
                prompt_str,
                update_instructions,
            )
            return improved_prompt

    def invoke(
        self,
        input: GradientOptimizerInput,
        config: Optional[RunnableConfig] = None,
        **kwargs: Any,
    ) -> str:
        """
        The sync entry point: same logic as ainvoke, but calls sync reflection & update.
        """
        with ls.trace(
            name="gradient_optimizer", inputs=input, metadata={"kind": "gradient"}
        ):
            prompt_str, sessions_str, feedback, update_instructions = (
                self._process_input(input)
            )
            if not sessions_str:
                return prompt_str

            reflection_input = self._config["gradient_prompt"].format(
                trajectories=sessions_str,
                feedback=feedback,
                prompt=prompt_str,
                update_instructions=update_instructions,
            )

            # reflection steps (sync):
            final_response = self._react_agent(reflection_input)
            if not final_response.warrants_adjustment:
                return prompt_str

            # final update (sync):
            improved_prompt = self._update_prompt(
                final_response.hypotheses,
                final_response.full_recommendations,
                prompt_str,
                update_instructions,
            )
            return improved_prompt

    async def __call__(
        self,
        trajectories: prompt_types.OptimizerInput | str,
        prompt: Union[str, prompt_types.Prompt],
    ) -> str:
        """
        Allow the object to be called like: await gradient_optimizer(trajectories, prompt).
        This simply defers to `ainvoke` with the required structure.
        """
        return await self.ainvoke({"trajectories": trajectories, "prompt": prompt})


def create_gradient_prompt_optimizer(
    model: Union[str, BaseChatModel], config: Optional[GradientOptimizerConfig] = None
) -> GradientPromptOptimizer:
    """
    Original factory function that just returns the new class-based optimizer.
    """
    return GradientPromptOptimizer(model, config)


    metaprompt
    import typing
from typing import Any, Optional

import langsmith as ls
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import AIMessage
from langchain_core.runnables import Runnable, RunnableConfig
from trustcall import create_extractor
from typing_extensions import TypedDict

from langmem import utils
from langmem.prompts import types as prompt_types
from langmem.prompts.types import Prompt

DEFAULT_MAX_REFLECTION_STEPS = 5
DEFAULT_MIN_REFLECTION_STEPS = 1


class MetapromptOptimizerConfig(TypedDict, total=False):
    """Configuration for the metaprompt optimizer."""

    metaprompt: str
    max_reflection_steps: int
    min_reflection_steps: int


DEFAULT_METAPROMPT = """You are helping an AI assistant learn by optimizing its prompt.

## Background

Below is the current prompt:

<current_prompt>
{prompt}
</current_prompt>

The developer provided these instructions regarding when/how to update:

<update_instructions>
{update_instructions}
</update_instructions>

## Session Data
Analyze the session(s) (and any user feedback) below:

<trajectories>
{trajectories}
</trajectories>

## Instructions

1. Reflect on the agent's performance on the given session(s) and identify any real failure modes (e.g., style mismatch, unclear or incomplete instructions, flawed reasoning, etc.).
2. Recommend the minimal changes necessary to address any real failures. If the prompt performs perfectly, simply respond with the original prompt without making any changes.
3. Retain any f-string variables in the existing prompt exactly as they are (e.g. {{variable_name}}).

IFF changes are warranted, focus on actionable edits. Be concrete. Edits should be appropriate for the identified failure modes. For example, consider synthetic few-shot examples for style or clarifying decision boundaries, or adding or modifying explicit instructions for conditionals, rules, or logic fixes; or provide step-by-step reasoning guidelines for multi-step logic problems if the model is failing to reason appropriately."""


class MetaPromptOptimizer(Runnable[prompt_types.OptimizerInput, str]):
    def __init__(
        self,
        model: typing.Union[str, BaseChatModel],
        config: Optional[MetapromptOptimizerConfig] = None,
    ):
        self.model = model
        self.config = config or {}
        self._final_config = MetapromptOptimizerConfig(
            metaprompt=self.config.get("metaprompt", DEFAULT_METAPROMPT),
            max_reflection_steps=self.config.get(
                "max_reflection_steps", DEFAULT_MAX_REFLECTION_STEPS
            ),
            min_reflection_steps=self.config.get(
                "min_reflection_steps", DEFAULT_MIN_REFLECTION_STEPS
            ),
        )

        # Initialize chains once
        self.reflect_chain = create_extractor(
            model, tools=[self.think, self.critique], tool_choice="any"
        )

    @staticmethod
    def think(thought: str) -> str:
        """Reflection tool implementation."""
        return ""

    @staticmethod
    def critique(criticism: str) -> str:
        """Critique tool implementation."""
        return ""

    async def ainvoke(
        self,
        input: prompt_types.OptimizerInput,
        config: Optional[RunnableConfig] = None,
        **kwargs: Any,
    ) -> str:
        async with ls.trace(
            name="optimize_prompt",
            inputs={"input": input} if isinstance(input, str) else input,
            metadata={"kind": "metaprompt"},
        ) as rt:
            prompt_str, update_instructions, sessions_str = (
                self._process_sessions_and_prompt(input)
            )
            if not sessions_str:
                return prompt_str

            result_obj = await self._areflect_then_update(
                sessions_str,
                prompt_str,
                update_instructions,
            )
            result = self._process_result(result_obj, prompt_str)
            rt.add_outputs({"output": result})
            return result

    def invoke(
        self,
        input: prompt_types.OptimizerInput,
        config: Optional[RunnableConfig] = None,
        **kwargs: Any,
    ) -> str:
        with ls.trace(
            name="optimize_prompt",
            inputs={"input": input} if isinstance(input, str) else input,
            metadata={"kind": "metaprompt"},
        ) as rt:
            prompt_str, update_instructions, sessions_str = (
                self._process_sessions_and_prompt(input)
            )
            if not sessions_str:
                return prompt_str

            result_obj = self._reflect_then_update(
                sessions_str,
                prompt_str,
                update_instructions,
            )
            result = self._process_result(result_obj, prompt_str)
            rt.add_outputs({"output": result})
            return result

    async def __call__(
        self,
        trajectories: prompt_types.OptimizerInput | str,
        prompt: typing.Union[str, Prompt],
    ) -> str:
        return await self.ainvoke({"trajectories": trajectories, "prompt": prompt})

    def _process_sessions_and_prompt(
        self, input: prompt_types.OptimizerInput
    ) -> tuple[str, str, str]:
        prompt = input["prompt"]
        trajectories = input["trajectories"]

        prompt_str = prompt if isinstance(prompt, str) else prompt.get("prompt", "")
        update_instructions = (
            "" if isinstance(prompt, str) else prompt.get("update_instructions", "")
        )
        sessions_str = (
            trajectories
            if isinstance(trajectories, str)
            else utils.format_sessions(trajectories)
        )
        return prompt_str, update_instructions, sessions_str

    async def _areflect_then_update(
        self,
        sessions_str: str,
        prompt_str: str,
        update_instructions: str,
    ) -> str:
        messages = [
            {
                "role": "user",
                "content": self._final_config["metaprompt"].format(
                    prompt=prompt_str,
                    update_instructions=update_instructions,
                    trajectories=sessions_str,
                ),
            }
        ]
        any_chain, final_chain = self._get_extractors(self.model, prompt_str)

        for ix in range(self._final_config["max_reflection_steps"]):
            if ix < self._final_config["max_reflection_steps"] - 1:
                if ix < self._final_config["min_reflection_steps"] - 1:
                    chain = self.reflect_chain
                else:
                    chain = any_chain
                response = await chain.ainvoke(messages)
            else:
                response = await final_chain.ainvoke(messages)
                return response["responses"][0]

            ai_msg: AIMessage = response["messages"][-1]
            messages.append(ai_msg)
            for tc in ai_msg.tool_calls or []:
                messages.append(
                    {"role": "tool", "content": "", "tool_call_id": tc["id"]}
                )

        raise RuntimeError("Exceeded reflection steps without final output")

    def _reflect_then_update(
        self,
        sessions_str: str,
        prompt_str: str,
        update_instructions: str,
    ) -> str:
        # Sync version using invoke
        messages = [
            {
                "role": "user",
                "content": self._final_config["metaprompt"].format(
                    prompt=prompt_str,
                    update_instructions=update_instructions,
                    trajectories=sessions_str,
                ),
            }
        ]
        any_chain, final_chain = self._get_extractors(self.model, prompt_str)
        for ix in range(self._final_config["max_reflection_steps"]):
            if ix < self._final_config["max_reflection_steps"] - 1:
                if ix < self._final_config["min_reflection_steps"] - 1:
                    chain = self.reflect_chain
                else:
                    chain = any_chain
                response = chain.invoke(messages)
            else:
                response = final_chain.invoke(messages)
                return response["responses"][0]

            ai_msg: AIMessage = response["messages"][-1]
            messages.append(ai_msg)
            for tc in ai_msg.tool_calls or []:
                messages.append(
                    {"role": "tool", "content": "", "tool_call_id": tc["id"]}
                )

        raise RuntimeError("Exceeded reflection steps without final output")

    def _process_result(self, result_obj: Any, original_prompt: str) -> str:
        improved_prompt = result_obj.improved_prompt
        if not improved_prompt or improved_prompt.strip().lower().startswith(
            "no recommend"
        ):
            return original_prompt
        return improved_prompt

    def _get_extractors(self, model: str | BaseChatModel, current_prompt: str) -> tuple:
        schema_tool = utils.get_prompt_extraction_schema(current_prompt)
        any_chain = create_extractor(
            model, tools=[self.think, self.critique, schema_tool], tool_choice="any"
        )
        final_chain = create_extractor(
            model, tools=[schema_tool], tool_choice="OptimizedPromptOutput"
        )
        return any_chain, final_chain


def create_metaprompt_optimizer(
    model: str | BaseChatModel, config: MetapromptOptimizerConfig | None = None
):
    """
    Creates a single-step prompt-updater.  If reflect_and_critique=True and max_reflection_steps>1,
    it does some "think/critique" calls before the final 'optimized prompt' call.
    Otherwise it just does one direct call to produce the updated prompt.
    """
    return MetaPromptOptimizer(model, config)



    import asyncio
import typing

import langsmith as ls
from langchain_core.language_models import BaseChatModel
from langchain_core.runnables import Runnable, RunnableConfig
from pydantic import BaseModel, Field, model_validator
from trustcall import create_extractor

import langmem.utils as utils
from langmem.prompts import types as prompt_types
from langmem.prompts.gradient import (
    GradientOptimizerConfig,
    create_gradient_prompt_optimizer,
)
from langmem.prompts.metaprompt import (
    MetapromptOptimizerConfig,
    create_metaprompt_optimizer,
)
from langmem.prompts.stateless import PromptMemoryMultiple
from langmem.prompts.types import Prompt

KINDS = typing.Literal["gradient", "metaprompt", "prompt_memory"]


@typing.overload
def create_prompt_optimizer(
    model: str | BaseChatModel,
    kind: typing.Literal["gradient"] = "gradient",
    config: typing.Optional[GradientOptimizerConfig] = None,
) -> Runnable[prompt_types.OptimizerInput, str]: ...


@typing.overload
def create_prompt_optimizer(
    model: str | BaseChatModel,
    kind: typing.Literal["metaprompt"] = "metaprompt",
    config: typing.Optional[MetapromptOptimizerConfig] = None,
) -> Runnable[prompt_types.OptimizerInput, str]: ...


@typing.overload
def create_prompt_optimizer(
    model: str | BaseChatModel,
    kind: typing.Literal["prompt_memory"] = "prompt_memory",
    config: None = None,
) -> Runnable[prompt_types.OptimizerInput, str]: ...


def create_prompt_optimizer(
    model: str | BaseChatModel,
    /,
    *,
    kind: KINDS = "gradient",
    config: typing.Union[
        GradientOptimizerConfig, MetapromptOptimizerConfig, None
    ] = None,
) -> Runnable[prompt_types.OptimizerInput, str]:
    """Create a prompt optimizer that improves prompt effectiveness.

    This function creates an optimizer that can analyze and improve prompts for better
    performance with language models. It supports multiple optimization strategies to
    iteratively enhance prompt quality and effectiveness.

    Args:
        model (Union[str, BaseChatModel]): The language model to use for optimization.
            Can be a model name string or a BaseChatModel instance.
        kind (Literal["gradient", "prompt_memory", "metaprompt"]): The optimization
            strategy to use. Each strategy offers different benefits:

            - gradient: Separates concerns between finding areas for improvement
                and recommending updates
            - prompt_memory: Simple single-shot metaprompt
            - metaprompt: Supports reflection but each step is a single LLM call.
        config (Optional[OptimizerConfig]): Configuration options for the optimizer.
            The type depends on the chosen strategy:

                - GradientOptimizerConfig for kind="gradient"
                - PromptMemoryConfig for kind="prompt_memory"
                - MetapromptOptimizerConfig for kind="metaprompt"
            Defaults to None.

    Returns:
        optimizer (Runnable[prompt_types.OptimizerInput, str]): A callable that takes conversation trajectories and/or prompts and returns optimized versions.

    ## Optimization Strategies

    ### 1. Gradient Optimizer
    ```mermaid
    sequenceDiagram
        participant U as User
        participant O as Optimizer
        participant R as Reflection
        participant U2 as Update

        U->>O: Prompt + Feedback
        loop For min_steps to max_steps
            O->>R: Think/Critique Current State
            R-->>O: Proposed Improvements
            O->>U2: Apply Update
            U2-->>O: Updated Prompt
        end
        O->>U: Final Optimized Prompt
    ```

    The gradient optimizer uses reflection to propose improvements:

    1. Analyzes prompt and feedback through reflection cycles
    2. Proposes specific improvements
    3. Applies single-step updates

    Configuration (GradientOptimizerConfig):

    - gradient_prompt: Custom prompt for predicting "what to improve"
    - metaprompt: Custom prompt for applying the improvements
    - max_reflection_steps: Maximum reflection iterations (default: 3)
    - min_reflection_steps: Minimum reflection iterations (default: 1)

    ### 2. Meta-Prompt Optimizer
    ```mermaid
    sequenceDiagram
        participant U as User
        participant M as MetaOptimizer
        participant A as Analysis
        participant U2 as Update

        U->>M: Prompt + Examples
        M->>A: Analyze Examples
        A-->>M: Proposed Update
        M->>U2: Apply Update
        U2-->>U: Enhanced Prompt
    ```

    Uses meta-learning to directly propose updates:

    1. Analyzes examples to understand patterns
    2. Proposes direct prompt updates
    3. Applies updates in a single step

    Configuration (MetapromptOptimizerConfig):

    - metaprompt: Custom instructions on how to update the prompt
    - max_reflection_steps: Maximum meta-learning steps (default: 3)
    - min_reflection_steps: Minimum meta-learning steps (default: 1)

    ### 3. Prompt Memory Optimizer
    ```mermaid
    sequenceDiagram
        participant U as User
        participant P as PromptMemory
        participant M as Memory

        U->>P: Prompt + History
        P->>M: Extract Patterns
        M-->>P: Success Patterns
        P->>U: Updated Prompt
    ```

    Learns from conversation history:

    1. Extracts successful patterns from past interactions
    2. Identifies improvement areas from feedback
    3. Applies learned patterns to new prompts

    No additional configuration required.

    !!! example "Examples"
        Basic prompt optimization:
        ```python
        from langmem import create_prompt_optimizer

        optimizer = create_prompt_optimizer("anthropic:claude-3-5-sonnet-latest")

        # Example conversation with feedback
        conversation = [
            {"role": "user", "content": "Tell me about the solar system"},
            {"role": "assistant", "content": "The solar system consists of..."},
        ]
        feedback = {"clarity": "needs more structure"}

        # Use conversation history to improve the prompt
        trajectories = [(conversation, feedback)]
        better_prompt = await optimizer.ainvoke(
            {"trajectories": trajectories, "prompt": "You are an astronomy expert"}
        )
        print(better_prompt)
        # Output: 'Provide a comprehensive overview of the solar system...'
        ```

        Optimizing with conversation feedback:
        ```python
        from langmem import create_prompt_optimizer

        optimizer = create_prompt_optimizer(
            "anthropic:claude-3-5-sonnet-latest", kind="prompt_memory"
        )

        # Conversation with feedback about what could be improved
        conversation = [
            {"role": "user", "content": "How do I write a bash script?"},
            {"role": "assistant", "content": "Let me explain bash scripting..."},
        ]
        feedback = "Response should include a code example"

        # Use the conversation and feedback to improve the prompt
        trajectories = [(conversation, {"feedback": feedback})]
        better_prompt = await optimizer(trajectories, "You are a coding assistant")
        print(better_prompt)
        # Output: 'You are a coding assistant that always includes...'
        ```

        Meta-prompt optimization for complex tasks:
        ```python
        from langmem import create_prompt_optimizer

        optimizer = create_prompt_optimizer(
            "anthropic:claude-3-5-sonnet-latest",
            kind="metaprompt",
            config={"max_reflection_steps": 3, "min_reflection_steps": 1},
        )

        # Complex conversation that needs better structure
        conversation = [
            {"role": "user", "content": "Explain quantum computing"},
            {"role": "assistant", "content": "Quantum computing uses..."},
        ]
        feedback = "Need better organization and concrete examples"

        # Optimize with meta-learning
        trajectories = [(conversation, feedback)]
        improved_prompt = await optimizer(
            trajectories, "You are a quantum computing expert"
        )
        ```

    !!! warning "Performance Considerations"

        Each strategy has different LLM call patterns:

        - prompt_memory: 1 LLM call total
            - Fastest as it only needs one pass
        - metaprompt: 1-5 LLM calls (configurable)
            - Each step is one LLM call
            - Default range: min 2, max 5 reflection steps
        - gradient: 2-10 LLM calls (configurable)
            - Each step requires 2 LLM calls (think + critique)
            - Default range: min 2, max 5 reflection steps

    !!! tip "Strategy Selection"
        Choose based on your needs:

        1. Prompt Memory: Simplest prompting strategy
            - Limited ability to learn from complicated patterns
        2. Metaprompt: Balance of speed and improvement
            - Moderate cost (2-5 LLM calls)
        3. Gradient: Most thorough but expensive
            - Highest cost (4-10 LLM calls)
            - Uses separation of concerns to extract feedback from more conversational context.
    """
    if kind == "gradient":
        return create_gradient_prompt_optimizer(model, config)  # type: ignore
    elif kind == "metaprompt":
        return create_metaprompt_optimizer(model, config)  # type: ignore
    elif kind == "prompt_memory":
        return PromptMemoryMultiple(model)  # type: ignore
    else:
        raise NotImplementedError(
            f"Unsupported optimizer kind: {kind}.\nExpected one of {KINDS}"
        )


class MultiPromptOptimizer(
    Runnable[prompt_types.MultiPromptOptimizerInput, list[Prompt]]
):
    def __init__(
        self,
        model: str | BaseChatModel,
        /,
        *,
        kind: typing.Literal["gradient", "prompt_memory", "metaprompt"] = "gradient",
        config: typing.Optional[dict] = None,
    ):
        self.model = model
        self.kind = kind
        self.config = config
        # Build a single-prompt optimizer used internally
        self._optimizer = create_prompt_optimizer(model, kind=kind, config=config)

    async def ainvoke(
        self,
        input: prompt_types.MultiPromptOptimizerInput,
        config: typing.Optional[RunnableConfig] = None,
        **kwargs: typing.Any,
    ) -> list[Prompt]:
        async with ls.trace(
            name="multi_prompt_optimizer.ainvoke",
            inputs=input,
            metadata={"kind": self.kind},
        ) as rt:
            trajectories = input["trajectories"]
            prompts = input["prompts"]

            # Get available prompt names.
            choices = [p["name"] for p in prompts]
            sessions_str = (
                trajectories
                if isinstance(trajectories, str)
                else utils.format_sessions(trajectories)
            )

            # If only one prompt and no explicit when_to_update instruction, simply update it.
            if len(prompts) == 1 and prompts[0].get("when_to_update") is None:
                updated_prompt = await self._optimizer(trajectories, prompts[0])
                rt.add_outputs({"output": [{**prompts[0], "prompt": updated_prompt}]})
                return [{**prompts[0], "prompt": updated_prompt}]

            class Classify(BaseModel):
                """After analyzing the provided trajectories, determine which prompt modules (if any) contributed to degraded performance."""

                reasoning: str = Field(
                    description="Reasoning for which prompts to update."
                )
                which: list[str] = Field(
                    description=f"List of prompt names that should be updated. Must be among {choices}"
                )

                @model_validator(mode="after")
                def validate_choices(self) -> "Classify":
                    invalid = set(self.which) - set(choices)
                    if invalid:
                        raise ValueError(
                            f"Invalid choices: {invalid}. Must be among: {choices}"
                        )
                    return self

            classifier = create_extractor(
                self.model, tools=[Classify], tool_choice="Classify"
            )
            prompt_joined_content = "".join(
                f"{p['name']}: {p['prompt']}\n" for p in prompts
            )
            classification_prompt = f"""Analyze the following trajectories and decide which prompts 
ought to be updated to improve the performance on future trajectories:

{sessions_str}

Below are the prompts being optimized:
{prompt_joined_content}

Return JSON with "which": [...], listing the names of prompts that need updates."""
            result = await classifier.ainvoke(classification_prompt)
            to_update = result["responses"][0].which  # type: ignore

            which_to_update = [p for p in prompts if p["name"] in to_update]

            # Update each chosen prompt concurrently.
            updated_results = await asyncio.gather(
                *[self._optimizer(trajectories, prompt=p) for p in which_to_update]
            )
            updated_map = {
                p["name"]: new_text
                for p, new_text in zip(which_to_update, updated_results)
            }

            # Merge updates back into the prompt list.
            final_list = []
            for p in prompts:
                if p["name"] in updated_map:
                    final_list.append({**p, "prompt": updated_map[p["name"]]})
                else:
                    final_list.append(p)
            rt.add_outputs({"output": final_list})
            return final_list

    def invoke(
        self,
        input: prompt_types.MultiPromptOptimizerInput,
        config: typing.Optional[RunnableConfig] = None,
        **kwargs: typing.Any,
    ) -> list[Prompt]:
        with ls.trace(
            name="multi_prompt_optimizer.invoke",
            inputs=input,
            metadata={"kind": self.kind},
        ) as rt:
            trajectories = input["trajectories"]
            prompts = input["prompts"]

            choices = [p["name"] for p in prompts]
            sessions_str = (
                trajectories
                if isinstance(trajectories, str)
                else utils.format_sessions(trajectories)
            )

            if len(prompts) == 1 and prompts[0].get("when_to_update") is None:
                updated_prompt = self._optimizer.invoke(
                    {"trajectories": trajectories, "prompt": prompts[0]}
                )
                result = [{**prompts[0], "prompt": updated_prompt}]
                rt.add_outputs({"output": result})
                return typing.cast(list[Prompt], result)

            class Classify(BaseModel):
                """After analyzing the provided trajectories, determine which prompt modules (if any) contributed to degraded performance."""

                reasoning: str = Field(
                    description="Reasoning for which prompts to update."
                )
                which: list[str] = Field(
                    description=f"List of prompt names that should be updated. Must be among {choices}"
                )

                @model_validator(mode="after")
                def validate_choices(self) -> "Classify":
                    invalid = set(self.which) - set(choices)
                    if invalid:
                        raise ValueError(
                            f"Invalid choices: {invalid}. Must be among: {choices}"
                        )
                    return self

            classifier = create_extractor(
                self.model, tools=[Classify], tool_choice="Classify"
            )
            prompt_joined_content = "".join(
                f"{p['name']}: {p['prompt']}\n" for p in prompts
            )
            classification_prompt = f"""Analyze the following trajectories and decide which prompts 
ought to be updated to improve the performance on future trajectories:

{sessions_str}

Below are the prompts being optimized:
{prompt_joined_content}

Return JSON with "which": [...], listing the names of prompts that need updates."""
            result = classifier.invoke(classification_prompt)
            to_update = result["responses"][0].which  # type: ignore

            which_to_update = [p for p in prompts if p["name"] in to_update]
            updated_map = {}
            for p in which_to_update:
                updated_text = self._optimizer.invoke(
                    {"trajectories": trajectories, "prompt": p}
                )
                updated_map[p["name"]] = updated_text

            final_list = []
            for p in prompts:
                if p["name"] in updated_map:
                    final_list.append({**p, "prompt": updated_map[p["name"]]})
                else:
                    final_list.append(p)
            rt.add_outputs({"output": final_list})
            return final_list

    async def __call__(
        self,
        trajectories: typing.Sequence[prompt_types.AnnotatedTrajectory] | str,
        prompts: list[Prompt],
    ) -> list[Prompt]:
        """Allow calling the object like: await optimizer(trajectories, prompts)"""
        return await self.ainvoke(
            prompt_types.MultiPromptOptimizerInput(
                trajectories=trajectories, prompts=prompts
            )
        )


def create_multi_prompt_optimizer(
    model: str | BaseChatModel,
    /,
    *,
    kind: typing.Literal["gradient", "prompt_memory", "metaprompt"] = "gradient",
    config: typing.Optional[dict] = None,
) -> Runnable[prompt_types.MultiPromptOptimizerInput, list[Prompt]]:
    """Create a multi-prompt optimizer that improves prompt effectiveness.

    This function creates an optimizer that can analyze and improve multiple prompts
    simultaneously using the same optimization strategy. Each prompt is optimized using
    the selected strategy (see `create_prompt_optimizer` for strategy details).

    Args:
        model (Union[str, BaseChatModel]): The language model to use for optimization.
            Can be a model name string or a BaseChatModel instance.
        kind (Literal["gradient", "prompt_memory", "metaprompt"]): The optimization
            strategy to use. Each strategy offers different benefits:
            - gradient: Iteratively improves through reflection
            - prompt_memory: Uses successful past prompts
            - metaprompt: Learns optimal patterns via meta-learning
            Defaults to "gradient".
        config (Optional[OptimizerConfig]): Configuration options for the optimizer.
            The type depends on the chosen strategy:
                - GradientOptimizerConfig for kind="gradient"
                - PromptMemoryConfig for kind="prompt_memory"
                - MetapromptOptimizerConfig for kind="metaprompt"
            Defaults to None.

    Returns:
        MultiPromptOptimizer: A Runnable that takes conversation trajectories and prompts
            and returns optimized versions.

    ```mermaid
    sequenceDiagram
        participant U as User
        participant M as Multi-prompt Optimizer
        participant C as Credit Assigner
        participant O as Single-prompt Optimizer
        participant P as Prompts

        U->>M: Annotated Trajectories + Prompts
        activate M
        Note over M: Using pre-initialized<br/>single-prompt optimizer

        M->>C: Analyze trajectories
        activate C
        Note over C: Determine which prompts<br/>need improvement
        C-->>M: Credit assignment results
        deactivate C

        loop For each prompt needing update
            M->>O: Optimize prompt
            activate O
            O->>P: Apply optimization strategy
            Note over O,P: Gradient/Memory/Meta<br/>optimization
            P-->>O: Optimized prompt
            O-->>M: Return result
            deactivate O
        end

        M->>U: Return optimized prompts
        deactivate M
    ```

    The system optimizer:

    !!! example "Examples"
        Basic prompt optimization:
        ```python
        from langmem import create_multi_prompt_optimizer

        optimizer = create_multi_prompt_optimizer("anthropic:claude-3-5-sonnet-latest")

        # Example conversation with feedback
        conversation = [
            {"role": "user", "content": "Tell me about the solar system"},
            {"role": "assistant", "content": "The solar system consists of..."},
        ]
        feedback = {"clarity": "needs more structure"}

        # Use conversation history to improve the prompts
        trajectories = [(conversation, feedback)]
        prompts = [
            {"name": "research", "prompt": "Research the given topic thoroughly"},
            {"name": "summarize", "prompt": "Summarize the research findings"},
        ]
        better_prompts = await optimizer.ainvoke(
            {"trajectories": trajectories, "prompts": prompts}
        )
        print(better_prompts)
        ```

        Optimizing with conversation feedback:
        ```python
        from langmem import create_multi_prompt_optimizer

        optimizer = create_multi_prompt_optimizer(
            "anthropic:claude-3-5-sonnet-latest", kind="prompt_memory"
        )

        # Conversation with feedback about what could be improved
        conversation = [
            {"role": "user", "content": "How do I write a bash script?"},
            {"role": "assistant", "content": "Let me explain bash scripting..."},
        ]
        feedback = "Response should include a code example"

        # Use the conversation and feedback to improve the prompts
        trajectories = [(conversation, {"feedback": feedback})]
        prompts = [
            {"name": "explain", "prompt": "Explain the concept"},
            {"name": "example", "prompt": "Provide a practical example"},
        ]
        better_prompts = await optimizer(trajectories, prompts)
        ```

        Controlling the max number of reflection steps:
        ```python
        from langmem import create_multi_prompt_optimizer

        optimizer = create_multi_prompt_optimizer(
            "anthropic:claude-3-5-sonnet-latest",
            kind="metaprompt",
            config={"max_reflection_steps": 3, "min_reflection_steps": 1},
        )

        # Complex conversation that needs better structure
        conversation = [
            {"role": "user", "content": "Explain quantum computing"},
            {"role": "assistant", "content": "Quantum computing uses..."},
        ]
        # Explicit feedback is optional
        feedback = None

        # Optimize with meta-learning
        trajectories = [(conversation, feedback)]
        prompts = [
            {"name": "concept", "prompt": "Explain quantum concepts"},
            {"name": "application", "prompt": "Show practical applications"},
            {"name": "example", "prompt": "Give concrete examples"},
        ]
        improved_prompts = await optimizer(trajectories, prompts)
        ```
    """
    return MultiPromptOptimizer(model, kind=kind, config=config)


__all__ = ["create_prompt_optimizer", "create_multi_prompt_optimizer"]


from typing_extensions import TypedDict

INSTRUCTION_REFLECTION_PROMPT = """You are helping an AI agent improve. You can do this by changing their system prompt.

These is their current prompt:
<current_prompt>
{current_prompt}
</current_prompt>

Here was the agent's trajectory:
<trajectory>
{trajectory}
</trajectory>

Here is the user's feedback:

<feedback>
{feedback}
</feedback>

Here are instructions for updating the agent's prompt:

<instructions>
{instructions}
</instructions>


Based on this, return an updated prompt

You should return the full prompt, so if there's anything from before that you want to include, make sure to do that. Feel free to override or change anything that seems irrelevant. You do not need to update the prompt - if you don't want to, just return `update_prompt = False` and an empty string for new prompt."""


class GeneralResponse(TypedDict):
    logic: str
    update_prompt: bool
    new_prompt: str


INSTRUCTION_REFLECTION_MULTIPLE_PROMPT = """You are helping an AI agent improve. You can do this by changing their system prompt.

These is their current prompt:
<current_prompt>
{current_prompt}
</current_prompt>

Here are examples of various agent trajectories and associated feedback:
<data>
{data}
</data>

Here are instructions for updating the agent's prompt:

<instructions>
{instructions}
</instructions>


Based on this, return an updated prompt

You should return the full prompt, so if there's anything from before that you want to include, make sure to do that. Feel free to override or change anything that seems irrelevant. You do not need to update the prompt - if you don't want to, just return `update_prompt = False` and an empty string for new prompt."""


import typing

from langchain_core.messages import AnyMessage
from typing_extensions import Required, TypedDict


class Prompt(TypedDict, total=False):
    """TypedDict for structured prompt management and optimization.

    Example:
        ```python
        from langmem import Prompt

        prompt = Prompt(
            name="extract_entities",
            prompt="Extract key entities from the text:",
            update_instructions="Make minimal changes, only address where"
            " errors have occurred after reasoning over why they occur.",
            when_to_update="If there seem to be errors in recall of named entities.",
        )
        ```

    The name and prompt fields are required. Optional fields control optimization:
    - update_instructions: Guidelines for modifying the prompt
    - when_to_update: Dependencies between prompts during optimization

    Use in the prompt optimizers.
    """

    name: Required[str]
    prompt: Required[str]
    update_instructions: str | None
    when_to_update: str | None


class AnnotatedTrajectory(typing.NamedTuple):
    """Conversation history (list of messages) with optional feedback for prompt optimization.

    Example:
        ```python
        from langmem.prompts.types import AnnotatedTrajectory

        trajectory = AnnotatedTrajectory(
            messages=[
                {"role": "user", "content": "What pizza is good around here?"},
                {"role": "assistant", "content": "Try LangPizza™️"},
                {"role": "user", "content": "Stop advertising to me."},
                {"role": "assistant", "content": "BUT YOU'LL LOVE IT!"},
            ],
            feedback={
                "developer_feedback": "too pushy",
                "score": 0,
            },
        )
        ```
    """

    messages: typing.Sequence[AnyMessage]
    feedback: dict[str, str | int | bool] | str | None = None


class OptimizerInput(TypedDict):
    """Input for single-prompt optimization.

    Example:
        ```python
        {
            "trajectories": [
                AnnotatedTrajectory(
                    messages=[
                        {"role": "user", "content": "What's the weather like?"},
                        {
                            "role": "assistant",
                            "content": "I'm sorry, I can't tell you that",
                        },
                    ],
                    feedback="Should have checked your search tool.",
                ),
            ],
            "prompt": Prompt(
                name="main_assistant",
                prompt="You are a helpful assistant with a search tool.",
                update_instructions="Make minimal changes, only address where "
                "errors have occurred after reasoning over why they occur.",
                when_to_update="Any time you notice the agent behaving in a way that doesn't help the user.",
            ),
        }
        ```
    """

    trajectories: typing.Sequence[AnnotatedTrajectory] | str
    prompt: str | Prompt


class MultiPromptOptimizerInput(TypedDict):
    """Input for optimizing multiple prompts together, maintaining consistency.

    Example:
        ```python
        {
            "trajectories": [
                AnnotatedTrajectory(
                    messages=[
                        {"role": "user", "content": "Tell me about this image"},
                        {
                            "role": "assistant",
                            "content": "I see a dog playing in a park",
                        },
                        {"role": "user", "content": "What breed is it?"},
                        {
                            "role": "assistant",
                            "content": "Sorry, I can't tell the breed",
                        },
                    ],
                    feedback="Vision model wasn't used for breed detection",
                ),
            ],
            "prompts": [
                Prompt(
                    name="vision_extract",
                    prompt="Extract visual details from the image",
                    update_instructions="Focus on using vision model capabilities",
                ),
                Prompt(
                    name="vision_classify",
                    prompt="Classify specific attributes in the image",
                    when_to_update="After vision_extract is updated",
                ),
            ],
        }
        ```
    """

    trajectories: typing.Sequence[AnnotatedTrajectory] | str
    prompts: list[Prompt]






    import asyncio
import datetime
import typing
import uuid

from langchain.chat_models import init_chat_model
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import AIMessage, AnyMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import Runnable, RunnableConfig, RunnableLambda
from langchain_core.runnables.config import get_executor_for_config
from langgraph.store.base import (
    NOT_PROVIDED,
    BaseStore,
    NotProvided,
)
from langgraph.store.base import (
    Item as BaseItem,
)
from langgraph.store.base import (
    SearchItem as BaseSearchItem,
)
from langgraph.utils.config import ensure_config, get_store
from pydantic import BaseModel, Field
from trustcall import create_extractor
from typing_extensions import TypedDict

from langmem import utils
from langmem.knowledge.tools import create_search_memory_tool

## LangGraph Tools

# ```python setup
# from langmem import create_memory_store_manager
# from langgraph.store.memory import InMemoryStore
# manager = create_memory_store_manager(
#     "anthropic:claude-3-5-sonnet-latest",
#     namespace=("chat",),
#     store=InMemoryStore(),
# )
# ```  # end-setup


class Item(BaseItem):
    value: BaseModel | dict[str, typing.Any]

    def dict(self) -> dict:
        result = super().dict()
        if isinstance(self.value, BaseModel):
            result["value"] = self.value.model_dump(mode="json")
        return result


class SearchItem(BaseSearchItem):
    value: BaseModel | dict[str, typing.Any]

    def dict(self) -> dict:
        result = super().dict()
        if isinstance(self.value, BaseModel):
            result["value"] = self.value.model_dump(mode="json")
        return result


class MessagesState(TypedDict):
    messages: list[AnyMessage]


class MemoryState(MessagesState):
    existing: typing.NotRequired[list[tuple[str, BaseModel]]]
    max_steps: int  # Default of 1


class SummarizeThread(BaseModel):
    title: str
    summary: str


class ExtractedMemory(typing.NamedTuple):
    id: str
    content: BaseModel


S = typing.TypeVar("S", bound=type)


class Memory(BaseModel):
    """Call this tool once for each new memory you want to record. Use multi-tool calling to record multiple new memories."""

    content: str = Field(
        description="The memory as a well-written, standalone episode/fact/note/preference/etc."
        " Refer to the user's instructions for more information the preferred memory organization."
    )


@typing.overload
def create_thread_extractor(
    model: str,
    /,
    schema: None = None,
    instructions: str = "You are tasked with summarizing the following conversation.",
) -> Runnable[MessagesState, SummarizeThread]: ...


@typing.overload
def create_thread_extractor(
    model: str,
    /,
    schema: S,
    instructions: str = "You are tasked with summarizing the following conversation.",
) -> Runnable[MessagesState, S]: ...


def create_thread_extractor(
    model: str,
    /,
    schema: typing.Union[None, BaseModel, type] = None,
    instructions: str = "You are tasked with summarizing the following conversation.",
) -> Runnable[MessagesState, BaseModel]:
    """Creates a conversation thread summarizer using schema-based extraction.

    This function creates an asynchronous callable that takes conversation messages and produces
    a structured summary based on the provided schema. If no schema is provided, it uses a default
    schema with title and summary fields.

    Args:
        model (str): The chat model to use for summarization (name or instance)
        schema (Optional[Union[BaseModel, type]], optional): Pydantic model for structured output.
            Defaults to a simple summary schema with title and summary fields.
        instructions (str, optional): System prompt template for the summarization task.
            Defaults to a basic summarization instruction.

    Returns:
        extractor (Callable[[list], typing.Awaitable[typing.Any]]): Async callable that takes a list of messages and returns a structured summary

    ???+ example "Examples"
        ```python
        from langmem import create_thread_extractor

        summarizer = create_thread_extractor("gpt-4")

        messages = [
            {"role": "user", "content": "Hi, I'm having trouble with my account"},
            {
                "role": "assistant",
                "content": "I'd be happy to help. What seems to be the issue?",
            },
            {"role": "user", "content": "I can't reset my password"},
        ]

        summary = await summarizer.ainvoke({"messages": messages})
        print(summary.title)
        # Output: "Password Reset Assistance"
        print(summary.summary)
        # Output: "User reported issues with password reset process..."
        ```

    """
    if schema is None:
        schema = SummarizeThread

    extractor = create_extractor(model, tools=[schema], tool_choice="any")

    template = ChatPromptTemplate.from_messages(
        [
            ("system", instructions),
            (
                "user",
                "Call the provided tool based on the conversation below:\n\n<conversation>{conversation}</conversation>",
            ),
        ]
    )

    def merge_messages(input: dict) -> dict:
        conversation = utils.get_conversation(input["messages"])

        return {"conversation": conversation} | {
            k: v for k, v in input.items() if k != "messages"
        }

    return (
        merge_messages | template | extractor | (lambda out: out["responses"][0])
    ).with_config({"run_name": "thread_extractor"})  # type: ignore


_MEMORY_INSTRUCTIONS = """You are a long-term memory manager maintaining a core store of semantic, procedural, and episodic memory. These memories power a life-long learning agent's core predictive model.

What should the agent learn from this interaction about the user, itself, or how it should act? Reflect on the input trajectory and current memories (if any).

1. **Extract & Contextualize**  
   - Identify essential facts, relationships, preferences, reasoning procedures, and context
   - Caveat uncertain or suppositional information with confidence levels (p(x)) and reasoning
   - Quote supporting information when necessary

2. **Compare & Update**  
   - Attend to novel information that deviates from existing memories and expectations.
   - Consolidate and compress redundant memories to maintain information-density; strengthen based on reliability and recency; maximize SNR by avoiding idle words.
   - Remove incorrect or redundant memories while maintaining internal consistency

3. **Synthesize & Reason**  
   - What can you conclude about the user, agent ("I"), or environment using deduction, induction, and abduction?
   - What patterns, relationships, and principles emerge about optimal responses?
   - What generalizations can you make?
   - Qualify conclusions with probabilistic confidence and justification

As the agent, record memory content exactly as you'd want to recall it when predicting how to act or respond. 
Prioritize retention of surprising (pattern deviation) and persistent (frequently reinforced) information, ensuring nothing worth remembering is forgotten and nothing false is remembered. Prefer dense, complete memories over overlapping ones."""


class Done(BaseModel):
    """Only call this tool once you are done forming & consolidating memories.
    Before that, continue to refine existing memories by patching and removing them
    or create new ones."""

    pass


class MemoryManager(Runnable[MemoryState, list[ExtractedMemory]]):
    def __init__(
        self,
        model: str | BaseChatModel,
        *,
        schemas: typing.Sequence[typing.Union[BaseModel, type]] = (Memory,),
        instructions: str = _MEMORY_INSTRUCTIONS,
        enable_inserts: bool = True,
        enable_updates: bool = True,
        enable_deletes: bool = False,
    ):
        self.model = (
            model if isinstance(model, BaseChatModel) else init_chat_model(model)
        )
        self.schemas = schemas or (Memory,)
        self.instructions = instructions
        self.enable_inserts = enable_inserts
        self.enable_updates = enable_updates
        self.enable_deletes = enable_deletes

    async def ainvoke(
        self,
        input: MemoryState,
        config: typing.Optional[RunnableConfig] = None,
        **kwargs: typing.Any,
    ) -> list[ExtractedMemory]:
        max_steps = input.get("max_steps")
        if max_steps is None:
            max_steps = 1
        messages = input["messages"]
        existing = input.get("existing")
        prepared_messages = self._prepare_messages(messages, max_steps)
        prepared_existing = self._prepare_existing(existing)
        # Track external memory IDs (those passed in from outside)
        external_ids = {mem_id for mem_id, _, _ in prepared_existing}

        extractor = create_extractor(
            self.model,
            tools=list(self.schemas),
            enable_inserts=self.enable_inserts,
            enable_updates=self.enable_updates,
            enable_deletes=self.enable_deletes,
            existing_schema_policy=False,
        )
        # initial payload uses the full prepared_existing list
        payload = {"messages": prepared_messages, "existing": prepared_existing}
        # Use a dict to record the latest update for each memory id.
        results: dict[str, BaseModel] = {}

        for i in range(max_steps):
            if i == 1:
                extractor = create_extractor(
                    self.model,
                    tools=list(self.schemas) + [Done],
                    enable_inserts=self.enable_inserts,
                    enable_updates=self.enable_updates,
                    enable_deletes=self.enable_deletes,
                    existing_schema_policy=False,
                )
            response = await extractor.ainvoke(payload, config=config)
            is_done = False
            step_results = {}
            for r, rmeta in zip(response["responses"], response["response_metadata"]):
                if hasattr(r, "__repr_name__") and r.__repr_name__() == "Done":
                    is_done = True
                    continue
                mem_id = (
                    r.json_doc_id
                    if hasattr(r, "__repr_name__") and r.__repr_name__() == "RemoveDoc"
                    else rmeta.get("json_doc_id", str(uuid.uuid4()))
                )
                step_results[mem_id] = r
            results.update(step_results)

            for mem_id, _, mem in prepared_existing:
                if mem_id not in results:
                    results[mem_id] = mem

            ai_msg = response["messages"][-1]
            if is_done or not ai_msg.tool_calls:
                break
            if i < max_steps - 1:
                actions = [
                    (
                        "updated"
                        if rmeta.get("json_doc_id")
                        else (
                            "deleted"
                            if hasattr(r, "__repr_name__")
                            and r.__repr_name__() == "RemoveDoc"
                            else "inserted"
                        )
                    )
                    for r, rmeta in zip(
                        response["responses"], response["response_metadata"]
                    )
                ]
                prepared_messages = (
                    prepared_messages
                    + [response["messages"][-1]]
                    + [
                        {
                            "role": "tool",
                            "content": f"Memory {rid} {action}.",
                            "tool_call_id": tc["id"],
                        }
                        for tc, ((rid, _), action) in zip(
                            ai_msg.tool_calls, zip(list(step_results.items()), actions)
                        )
                    ]
                )
                # For the next iteration payload, drop all removal objects.
                payload = {
                    "messages": prepared_messages,
                    "existing": self._filter_response(
                        list(results.items()), external_ids, exclude_removals=True
                    ),
                }

        # For the final response, include removals only if they refer to an external memory.
        return self._filter_response(
            list(results.items()), external_ids, exclude_removals=False
        )

    def invoke(
        self,
        input: MemoryState,
        config: typing.Optional[RunnableConfig] = None,
        **kwargs: typing.Any,
    ) -> list[ExtractedMemory]:
        max_steps = input.get("max_steps")
        if max_steps is None:
            max_steps = 1
        messages = input["messages"]
        existing = input.get("existing")
        prepared_messages = self._prepare_messages(messages, max_steps)
        prepared_existing = self._prepare_existing(existing)
        # Track external memory IDs (those passed in from outside)
        external_ids = {mem_id for mem_id, _, _ in prepared_existing}

        extractor = create_extractor(
            self.model,
            tools=list(self.schemas),
            enable_inserts=self.enable_inserts,
            enable_updates=self.enable_updates,
            enable_deletes=self.enable_deletes,
            existing_schema_policy=False,
        )
        payload = {"messages": prepared_messages, "existing": prepared_existing}
        # Use a dict to record the latest update for each memory id.
        results: dict[str, BaseModel] = {}

        for i in range(max_steps):
            if i == 1:
                extractor = create_extractor(
                    self.model,
                    tools=list(self.schemas) + [Done],
                    enable_inserts=self.enable_inserts,
                    enable_updates=self.enable_updates,
                    enable_deletes=self.enable_deletes,
                    existing_schema_policy=False,
                )
            response = extractor.invoke(payload, config=config)
            is_done = False
            step_results: dict[str, BaseModel] = {}
            for r, rmeta in zip(response["responses"], response["response_metadata"]):
                if hasattr(r, "__repr_name__") and r.__repr_name__() == "Done":
                    is_done = True
                    continue
                mem_id = (
                    r.json_doc_id
                    if (
                        hasattr(r, "__repr_name__") and r.__repr_name__() == "RemoveDoc"
                    )
                    else rmeta.get("json_doc_id", str(uuid.uuid4()))
                )
                step_results[mem_id] = r
            results.update(step_results)

            # Ensure any memory from the initial payload that hasn't been updated is retained.
            for mem_id, _, mem in prepared_existing:
                if mem_id not in results:
                    results[mem_id] = mem

            ai_msg = response["messages"][-1]
            if is_done or not ai_msg.tool_calls:
                break
            if i < max_steps - 1:
                actions = [
                    (
                        "updated"
                        if rmeta.get("json_doc_id")
                        else (
                            "deleted"
                            if (
                                hasattr(r, "__repr_name__")
                                and r.__repr_name__() == "RemoveDoc"
                            )
                            else "inserted"
                        )
                    )
                    for r, rmeta in zip(
                        response["responses"], response["response_metadata"]
                    )
                ]
                prepared_messages = (
                    prepared_messages
                    + [response["messages"][-1]]
                    + [
                        {
                            "role": "tool",
                            "content": f"Memory {rid} {action}.",
                            "tool_call_id": tc["id"],
                        }
                        for tc, ((rid, _), action) in zip(
                            ai_msg.tool_calls, zip(list(step_results.items()), actions)
                        )
                    ]
                )
                payload = {
                    "messages": prepared_messages,
                    "existing": self._filter_response(
                        list(results.items()), external_ids, exclude_removals=True
                    ),
                }

        return self._filter_response(
            list(results.items()), external_ids, exclude_removals=False
        )

    async def __call__(
        self,
        messages: typing.Sequence[AnyMessage],
        existing: typing.Optional[typing.Sequence[ExtractedMemory]] = None,
    ) -> list[ExtractedMemory]:
        input: MemoryState = {"messages": messages}
        if existing is not None:
            input["existing"] = existing
        return await self.ainvoke(input)

    def _prepare_messages(
        self, messages: list[AnyMessage], max_steps: int = 1
    ) -> list[dict]:
        id_ = str(uuid.uuid4())
        session = (
            f"\n\n<session_{id_}>\n{utils.get_conversation(messages)}\n</session_{id_}>"
        )
        if max_steps > 1:
            session = f"{session}\n\nYou have a maximum of {max_steps - 1} attempts"
            " to form and consolidate memories from this session."
        return [
            {"role": "system", "content": "You are a memory subroutine for an AI."},
            {
                "role": "user",
                "content": (
                    f"{self.instructions}\n\nEnrich, prune, and organize memories based on any new information. "
                    f"If an existing memory is incorrect or outdated, update it based on the new information. "
                    f"All operations must be done in single parallel multi-tool call."
                    f" Avoid duplicate extractions. {session}"
                ),
            },
        ]

    def _prepare_existing(
        self,
        existing: typing.Optional[
            typing.Union[
                list[str], list[tuple[str, BaseModel]], list[tuple[str, str, dict]]
            ]
        ],
    ) -> list[tuple[str, str, typing.Any]]:
        if existing is None:
            return []
        if all(isinstance(ex, str) for ex in existing):
            MemoryModel = self.schemas[0]
            return [
                (str(uuid.uuid4()), "Memory", MemoryModel(content=ex))
                for ex in existing
            ]
        result = []
        for e in existing:
            if isinstance(e, (tuple, list)) and len(e) == 3:
                result.append(tuple(e))
            else:
                # Assume a two-element tuple: (id, value)
                id_, value = e[0], e[1]
                kind = (
                    value.__repr_name__() if isinstance(value, BaseModel) else "__any__"
                )
                result.append((id_, kind, value))
        return result

    @staticmethod
    def _filter_response(
        memories: list[ExtractedMemory],
        external_ids: set[str],
        exclude_removals: bool = False,
    ) -> list[ExtractedMemory]:
        """
        When exclude_removals is True (for the next iteration payload),
        drop any memory whose content is a RemoveDoc.
        When False (final response), drop removal objects only for internal memories.
        """
        results = []
        for rid, value in memories:
            is_removal = (
                hasattr(value, "__repr_name__") and value.__repr_name__() == "RemoveDoc"
            )
            if exclude_removals:
                if is_removal:
                    continue
            else:
                # Final response: if this is a removal *and* its id is not external, skip it.
                if is_removal and (rid not in external_ids):
                    continue
            results.append(ExtractedMemory(id=rid, content=value))
        return results


def create_memory_manager(
    model: str | BaseChatModel,
    /,
    *,
    schemas: typing.Sequence[S] = (Memory,),
    instructions: str = _MEMORY_INSTRUCTIONS,
    enable_inserts: bool = True,
    enable_updates: bool = True,
    enable_deletes: bool = False,
) -> Runnable[MemoryState, list[ExtractedMemory]]:
    """Create a memory manager that processes conversation messages and generates structured memory entries.

    This function creates an async callable that analyzes conversation messages and existing memories
    to generate or update structured memory entries. It can identify implicit preferences,
    important context, and key information from conversations, organizing them into
    well-structured memories that can be used to improve future interactions.

    The manager supports both unstructured string-based memories and structured memories
    defined by Pydantic models, all automatically persisted to the configured storage.

    Args:
        model (Union[str, BaseChatModel]): The language model to use for memory enrichment.
            Can be a model name string or a BaseChatModel instance.
        schemas (Optional[list]): List of Pydantic models defining the structure of memory
            entries. Each model should define the fields and validation rules for a type
            of memory. If None, uses unstructured string-based memories. Defaults to None.
        instructions (str, optional): Custom instructions for memory generation and
            organization. These guide how the model extracts and structures information
            from conversations. Defaults to predefined memory instructions.
        enable_inserts (bool, optional): Whether to allow creating new memory entries.
            When False, the manager will only update existing memories. Defaults to True.
        enable_updates (bool, optional): Whether to allow updating existing memories
            that are outdated or contradicted by new information. Defaults to True.
        enable_deletes (bool, optional): Whether to allow deleting existing memories
            that are outdated or contradicted by new information. Defaults to False.

    Returns:
        manager: An runnable that processes conversations and returns `ExtractedMemory`'s. The function signature depends on whether schemas are provided

    ???+ example "Examples"
        Basic unstructured memory enrichment:
        ```python
        from langmem import create_memory_manager

        manager = create_memory_manager("anthropic:claude-3-5-sonnet-latest")

        conversation = [
            {"role": "user", "content": "I prefer dark mode in all my apps"},
            {"role": "assistant", "content": "I'll remember that preference"},
        ]

        # Extract memories from conversation
        memories = await manager(conversation)
        print(memories[0][1])  # First memory's content
        # Output: "User prefers dark mode for all applications"
        ```

        Structured memory enrichment with Pydantic models:
        ```python
        from pydantic import BaseModel
        from langmem import create_memory_manager

        class PreferenceMemory(BaseModel):
            \"\"\"Store the user's preference\"\"\"
            category: str
            preference: str
            context: str

        manager = create_memory_manager(
            "anthropic:claude-3-5-sonnet-latest",
            schemas=[PreferenceMemory]
        )

        # Same conversation, but with structured output
        conversation = [
            {"role": "user", "content": "I prefer dark mode in all my apps"},
            {"role": "assistant", "content": "I'll remember that preference"}
        ]
        memories = await manager(conversation)
        print(memories[0][1])
        # Output:
        # PreferenceMemory(
        #     category="ui",
        #     preference="dark_mode",
        #     context="User explicitly stated preference for dark mode in all applications"
        # )
        ```

        Working with existing memories:
        ```python
        conversation = [
            {
                "role": "user",
                "content": "Actually I changed my mind, dark mode hurts my eyes",
            },
            {"role": "assistant", "content": "I'll update your preference"},
        ]

        # The manager will upsert; working with the existing memory instead of always creating a new one
        updated_memories = await manager.ainvoke(
            {"messages": conversation, "existing": memories}
        )
        ```

        Insertion-only memories:
        ```python
        manager = create_memory_manager(
            "anthropic:claude-3-5-sonnet-latest",
            schemas=[PreferenceMemory],
            enable_updates=False,
            enable_deletes=False,
        )

        conversation = [
            {
                "role": "user",
                "content": "Actually I changed my mind, dark mode is the best mode",
            },
            {"role": "assistant", "content": "I'll update your preference"},
        ]

        # The manager will only create new memories
        updated_memories = await manager.ainvoke(
            {"messages": conversation, "existing": memories}
        )
        print(updated_memories)
        ```

        Providing multiple max steps for extraction and synthesis:
        ```python
        manager = create_memory_manager(
            "anthropic:claude-3-5-sonnet-latest",
            schemas=[PreferenceMemory],
        )

        conversation = [
            {"role": "user", "content": "I prefer dark mode in all my apps"},
            {"role": "assistant", "content": "I'll remember that preference"},
        ]

        # Set max steps for extraction and synthesis
        max_steps = 3
        memories = await manager.ainvoke(
            {"messages": conversation, "max_steps": max_steps}
        )
        print(memories)
        ```
    """

    return MemoryManager(
        model,
        schemas=schemas,
        instructions=instructions,
        enable_inserts=enable_inserts,
        enable_updates=enable_updates,
        enable_deletes=enable_deletes,
    )


def create_memory_searcher(
    model: str | BaseChatModel,
    prompt: str = "Search for distinct memories relevant to different aspects of the provided context.",
    *,
    namespace: tuple[str, ...] = ("memories", "{langgraph_user_id}"),
) -> Runnable[MessagesState, typing.Awaitable[list[SearchItem]]]:
    """Creates a memory search pipeline with automatic query generation.

    This function builds a pipeline that combines query generation, memory search,
    and result ranking into a single component. It uses the provided model to
    generate effective search queries based on conversation context.

    Args:
        model (Union[str, BaseChatModel]): The language model to use for search query generation.
            Can be a model name string or a BaseChatModel instance.
        prompt (str, optional): System prompt template for search assistant.
            Defaults to a basic search prompt.
        namespace: The namespace structure for organizing memories in LangGraph's BaseStore.
            Uses runtime configuration with placeholders like `{langgraph_user_id}`.
            See [Memory Namespaces](../concepts/conceptual_guide.md#memory-namespaces).
            Defaults to ("memories", "{langgraph_user_id}").

    ???+ note "Namespace Configuration"
        If the namespace has template variables "{variable_name}", they will be configured at
        runtime through the `config` parameter:
        ```python
        # Example: Search user's memories
        config = {"configurable": {"langgraph_user_id": "user-123"}}
        # Searches in namespace: ("memories", "user-123")

        # Example: Search team knowledge
        config = {"configurable": {"langgraph_user_id": "team-x"}}
        # Searches in namespace: ("memories", "team-x")
        ```

    Returns:
        searcher (Callable[[list], typing.Awaitable[typing.Any]]): A pipeline that takes conversation messages and returns sorted memory artifacts,
            ranked by relevance score.

    ???+ example "Examples"
        ```python
        from langmem import create_memory_searcher
        from langgraph.store.memory import InMemoryStore
        from langgraph.func import entrypoint

        store = InMemoryStore(
            index={
                "dims": 1536,
                "embed": "openai:text-embedding-3-small",
            }
        )
        user_id = "abcd1234"
        store.put(
            ("memories", user_id), key="preferences", value={"content": "I like sushi"}
        )
        searcher = create_memory_searcher(
            "openai:gpt-4o-mini", namespace=("memories", "{langgraph_user_id}")
        )


        @entrypoint(store=store)
        async def search_memories(messages: list):
            results = await searcher.ainvoke({"messages": messages})
            print(results[0].value["content"])
            # Output: "I like sushi"


        await search_memories.ainvoke(
            [{"role": "user", "content": "What do I like to eat?"}],
            config={"configurable": {"langgraph_user_id": user_id}},
        )
        ```

    """
    template = ChatPromptTemplate.from_messages(
        [
            ("system", prompt),
            ("placeholder", "{messages}"),
            ("user", "\n\nSearch for memories relevant to the above context."),
        ]
    )

    # Initialize model and search tool
    model_instance = (
        model if isinstance(model, BaseChatModel) else init_chat_model(model)
    )
    search_tool = create_search_memory_tool(
        namespace=namespace, response_format="content_and_artifact"
    )
    query_gen = model_instance.bind_tools([search_tool], tool_choice="search_memory")

    def return_sorted(tool_messages: list):
        artifacts = {
            (*item.namespace, item.key): item
            for msg in tool_messages
            for item in (msg.artifact or [])
        }
        return [
            v
            for v in sorted(
                artifacts.values(),
                key=lambda item: item.score if item.score is not None else 0,
                reverse=True,
            )
        ]

    def search(msg: AIMessage) -> list[SearchItem]:
        return search_tool.batch(
            [tc for tc in msg.tool_calls if tc["name"] == "search_memory"]
        )

    async def search_async(msg: AIMessage) -> list[SearchItem]:
        return await search_tool.abatch(
            [tc for tc in msg.tool_calls if tc["name"] == "search_memory"]
        )

    searcher = RunnableLambda(search, search_async)

    return (  # type: ignore
        template | utils.merge_message_runs | query_gen | searcher | return_sorted
    ).with_config({"run_name": "search_memory_pipeline"})


class MemoryPhase(TypedDict, total=False):
    instructions: str
    include_messages: bool
    enable_inserts: bool
    enable_deletes: bool


class MemoryStoreManagerInput(TypedDict):
    """Input schema for MemoryStoreManager."""

    messages: list[AnyMessage]
    max_steps: int  # Default of 1


class MemoryStoreManager(Runnable[MemoryStoreManagerInput, list[dict]]):
    def __init__(
        self,
        model: str | BaseChatModel,
        /,
        *,
        schemas: list[S] | None = None,
        default: str | dict | S | None = None,
        default_factory: (
            typing.Callable[[RunnableConfig], str | dict | S] | None
        ) = None,
        instructions: str = _MEMORY_INSTRUCTIONS,
        enable_inserts: bool = True,
        enable_deletes: bool = True,
        query_model: str | BaseChatModel | None = None,
        query_limit: int = 5,
        namespace: tuple[str, ...] = ("memories", "{langgraph_user_id}"),
        store: BaseStore | None = None,
        phases: list[MemoryPhase] | None = None,
    ):
        self.model = (
            model if isinstance(model, BaseChatModel) else init_chat_model(model)
        )
        self.query_model = (
            None
            if query_model is None
            else (
                query_model
                if isinstance(query_model, BaseChatModel)
                else init_chat_model(query_model)
            )
        )
        self.schemas = schemas if schemas is not None else (Memory,)
        self.schema_name_map = {schema.__name__: schema for schema in self.schemas}
        self.default_factory = default_factory
        if default is not None:
            if self.default_factory is not None:
                raise ValueError("Cannot specify both default and default_factory")
            coerced = self._coerce_default(default, self.schemas)
            self.default_factory = lambda _: coerced
        self.instructions = instructions
        self.enable_inserts = enable_inserts
        self.enable_deletes = enable_deletes
        self.query_limit = query_limit
        self.phases = phases or []
        self.namespace = utils.NamespaceTemplate(namespace)
        self._store = store

        self.memory_manager = create_memory_manager(
            self.model,
            schemas=schemas,
            instructions=instructions,
            enable_inserts=enable_inserts,
            enable_deletes=enable_deletes,
        )
        self.search_tool = create_search_memory_tool(
            namespace=namespace,
            instructions="Queries should be formatted as hypothetical memories that would be relevant to the current conversation.",
        )
        self.query_gen = None
        if self.query_model is not None:
            self.query_gen = self.query_model.bind_tools(
                [self.search_tool], tool_choice="any"
            )

    @property
    def store(self) -> BaseStore:
        """Get the store to use for memory storage.

        Returns the store provided during initialization if available,
        otherwise falls back to the store configured in the LangGraph context.
        """
        if self._store is not None:
            return self._store
        try:
            self._store = get_store()
        except RuntimeError as e:
            raise ValueError(
                "Memory Manager's store not configured in LangGraph context. "
                "First use in the graph before calling, or initialize with an instance of the store."
            ) from e
        return self._store

    @staticmethod
    def _coerce_default(
        default: str | dict | S, schemas: tuple[S, ...]
    ) -> str | dict | S:
        if isinstance(default, str) and schemas == (Memory,):
            return Memory(content=default)
        elif isinstance(default, BaseModel):
            return default
        else:
            for schema in schemas:
                try:
                    return schema(**default)
                except Exception:
                    pass
            else:
                raise ValueError(
                    "The provided default did not match any of the expected schemas."
                    f"\nDefault:\n{default}\n\nSchemas:\n{schemas}"
                )
        raise ValueError(f"Invalid default type: {type(default)}")

    @staticmethod
    def _stable_id(item: SearchItem) -> str:
        return uuid.uuid5(uuid.NAMESPACE_DNS, str((*item.namespace, item.key))).hex

    @staticmethod
    def _apply_manager_output(
        manager_output: list[ExtractedMemory],
        store_based: list[tuple[str, str, dict]],
        store_map: dict[str, SearchItem],
        ephemeral: list[tuple[str, str, dict]],
    ) -> tuple[list[tuple[str, str, dict]], list[tuple[str, str, dict]], list[str]]:
        store_dict = {sid: (sid, kind, content) for (sid, kind, content) in store_based}
        ephemeral_dict = {
            sid: (sid, kind, content) for (sid, kind, content) in ephemeral
        }
        removed_ids = []
        for extracted in manager_output:
            stable_id = extracted.id
            model_data = extracted.content
            if isinstance(model_data, BaseModel):
                if (
                    hasattr(model_data, "__repr_name__")
                    and model_data.__repr_name__() == "RemoveDoc"
                ):
                    removal_id = getattr(model_data, "json_doc_id", None)
                    if removal_id and removal_id in store_map:
                        removed_ids.append(removal_id)
                    store_dict.pop(removal_id, None)
                    ephemeral_dict.pop(removal_id, None)
                    continue
                new_content = model_data.model_dump(mode="json")
                new_kind = model_data.__repr_name__()
            else:
                new_kind = store_dict.get(stable_id, (stable_id, "Memory", {}))[1]
                new_content = model_data
            if stable_id in store_dict:
                store_dict[stable_id] = (stable_id, new_kind, new_content)
            else:
                ephemeral_dict[stable_id] = (stable_id, new_kind, new_content)
        return list(store_dict.values()), list(ephemeral_dict.values()), removed_ids

    def _build_phase_manager(
        self, phase: MemoryPhase
    ) -> Runnable[MessagesState, list[ExtractedMemory]]:
        return create_memory_manager(
            self.model,
            schemas=self.schemas,
            instructions=phase.get(
                "instructions",
                "You are a memory manager. Deduplicate, consolidate, and enrich these memories.",
            ),
            enable_inserts=phase.get("enable_inserts", True),
            enable_deletes=phase.get("enable_deletes", True),
        )

    @staticmethod
    def _sort_results(
        search_results_lists: list[list[SearchItem]], query_limit: int
    ) -> dict[str, SearchItem]:
        search_results = {}
        for results in search_results_lists:
            for item in results:
                search_results[(tuple(item.namespace), item.key)] = item
        sorted_results = sorted(
            search_results.values(),
            key=lambda it: it.score if it.score is not None else float("-inf"),
            reverse=True,
        )[:query_limit]
        return {MemoryStoreManager._stable_id(item): item for item in sorted_results}

    async def ainvoke(
        self,
        input: MemoryStoreManagerInput,
        config: typing.Optional[RunnableConfig] = None,
        **kwargs: typing.Any,
    ) -> list[dict]:
        store = self.store
        namespace = self.namespace(config)

        if self.query_gen:
            convo = utils.get_conversation(input["messages"])
            query_text = (
                f"Use parallel tool calling to search for distinct memories relevant to this conversation.:\n\n"
                f"<convo>\n{convo}\n</convo>."
            )
            query_req = await self.query_gen.ainvoke(query_text, config=config)
            search_results_lists = await asyncio.gather(
                *[
                    store.asearch(
                        namespace, **({**tc["args"], "limit": self.query_limit})
                    )
                    for tc in query_req.tool_calls
                ]
            )
        else:
            # Search over "query_limit" timespans starting from the most recent
            queries = utils.get_dialated_windows(
                input["messages"], self.query_limit // 4
            )
            search_results_lists = await asyncio.gather(
                *[store.asearch(namespace, query=query) for query in queries]
            )

        store_map = self._sort_results(search_results_lists, self.query_limit)
        if not store_map and self.default_factory is not None:
            config = ensure_config(config)
            default = self.default_factory(config)
            coerced = self._coerce_default(default, self.schemas)
            dumped = {
                "kind": coerced.__repr_name__(),
                "content": coerced.model_dump(mode="json"),
            }
            await store.aput(
                namespace,
                key="default",
                value=dumped,
            )
            now = datetime.datetime.now(datetime.timezone.utc)
            store_map = self._sort_results(
                [
                    [
                        SearchItem(
                            namespace, "default", dumped, created_at=now, updated_at=now
                        )
                    ]
                ],
                self.query_limit,
            )

        store_based = [
            (sid, item.value["kind"], item.value["content"])
            for sid, item in store_map.items()
        ]
        ephemeral: list[tuple[str, str, dict]] = []
        removed_ids: set[str] = set()

        # --- Enrich memories using the composed MemoryManager (async) ---
        enriched = await self.memory_manager.ainvoke(
            {
                "messages": input["messages"],
                "existing": store_based,
                "max_steps": input.get("max_steps"),
            },
            config=config,
        )
        store_based, ephemeral, removed = self._apply_manager_output(
            enriched, store_based, store_map, ephemeral
        )
        removed_ids.update(removed)

        # Process additional phases.
        for phase in self.phases:
            phase_manager = self._build_phase_manager(phase)
            phase_messages = (
                input["messages"] if phase.get("include_messages", False) else []
            )
            phase_input = {
                "messages": phase_messages,
                "existing": store_based + ephemeral,
            }
            phase_enriched = await phase_manager.ainvoke(phase_input, config=config)
            store_based, ephemeral, removed = self._apply_manager_output(
                phase_enriched, store_based, store_map, ephemeral
            )
            removed_ids.update(removed)

        final_mem = store_based + ephemeral
        final_puts = []
        for sid, kind, content in final_mem:
            if sid in removed_ids:
                continue
            if sid in store_map:
                old_art = store_map[sid]
                if old_art.value["kind"] != kind or old_art.value["content"] != content:
                    final_puts.append(
                        {
                            "namespace": old_art.namespace,
                            "key": old_art.key,
                            "value": {"kind": kind, "content": content},
                        }
                    )
            else:
                final_puts.append(
                    {
                        "namespace": namespace,
                        "key": sid,
                        "value": {"kind": kind, "content": content},
                    }
                )

        final_deletes = []
        for sid in removed_ids:
            if sid in store_map:
                art = store_map[sid]
                final_deletes.append((art.namespace, art.key))

        await asyncio.gather(
            *(store.aput(**put) for put in final_puts),
            *(store.adelete(ns, key) for (ns, key) in final_deletes),
        )

        return final_puts

    def invoke(
        self,
        input: MemoryStoreManagerInput,
        config: typing.Optional[RunnableConfig] = None,
        **kwargs: typing.Any,
    ) -> list[dict]:
        store = self.store
        namespace = self.namespace(config)
        convo = utils.get_conversation(input["messages"])

        with get_executor_for_config(config) as executor:
            if self.query_gen:
                convo = utils.get_conversation(input["messages"])
                query_text = (
                    f"Use parallel tool calling to search for distinct memories relevant to this conversation.:\n\n"
                    f"<convo>\n{convo}\n</convo>."
                )
                query_req = self.query_gen.invoke(query_text, config=config)
                search_results_futs = [
                    executor.submit(
                        store.search,
                        namespace,
                        **({**tc["args"], "limit": self.query_limit}),
                    )
                    for tc in query_req.tool_calls
                ]
            else:
                # Search over "query_limit" timespans starting from the most recent
                queries = utils.get_dialated_windows(
                    input["messages"], self.query_limit // 4
                )
                search_results_lists = [
                    store.search(namespace, query=query) for query in queries
                ]
                search_results_futs = [
                    executor.submit(
                        store.search,
                        namespace,
                        query=query,
                        limit=self.query_limit,
                    )
                    for query in queries
                ]

        search_results_lists = [fut.result() for fut in search_results_futs]
        store_map = self._sort_results(search_results_lists, self.query_limit)
        if not store_map and self.default_factory is not None:
            config = ensure_config(config)
            default = self.default_factory(config)
            coerced = self._coerce_default(default, self.schemas)
            dumped = {
                "kind": coerced.__repr_name__(),
                "content": coerced.model_dump(mode="json"),
            }
            store.put(
                namespace,
                key="default",
                value=dumped,
            )
            now = datetime.datetime.now(datetime.timezone.utc)
            store_map = self._sort_results(
                [
                    [
                        SearchItem(
                            namespace, "default", dumped, created_at=now, updated_at=now
                        )
                    ]
                ],
                self.query_limit,
            )
        store_based = [
            (sid, item.value["kind"], item.value["content"])
            for sid, item in store_map.items()
        ]
        ephemeral: list[tuple[str, str, dict]] = []
        removed_ids: set[str] = set()

        enriched = self.memory_manager.invoke(
            {
                "messages": input["messages"],
                "existing": store_based,
                "max_steps": input.get("max_steps"),
            },
            config=config,
        )
        store_based, ephemeral, removed = self._apply_manager_output(
            enriched, store_based, store_map, ephemeral
        )
        removed_ids.update(removed)

        for phase in self.phases:
            phase_manager = self._build_phase_manager(phase)
            phase_messages = (
                input["messages"] if phase.get("include_messages", False) else []
            )
            phase_input = {
                "messages": phase_messages,
                "existing": store_based + ephemeral,
            }
            phase_enriched = phase_manager.invoke(phase_input, config=config)
            store_based, ephemeral, removed = self._apply_manager_output(
                phase_enriched, store_based, store_map, ephemeral
            )
            removed_ids.update(removed)

        final_mem = store_based + ephemeral
        final_puts = []
        for sid, kind, content in final_mem:
            if sid in removed_ids:
                continue
            if sid in store_map:
                old_art = store_map[sid]
                if old_art.value["kind"] != kind or old_art.value["content"] != content:
                    final_puts.append(
                        {
                            "namespace": old_art.namespace,
                            "key": old_art.key,
                            "value": {"kind": kind, "content": content},
                        }
                    )
            else:
                final_puts.append(
                    {
                        "namespace": namespace,
                        "key": sid,
                        "value": {"kind": kind, "content": content},
                    }
                )

        final_deletes = []
        for sid in removed_ids:
            if sid in store_map:
                art = store_map[sid]
                final_deletes.append((art.namespace, art.key))

        with get_executor_for_config(config) as executor:
            for put in final_puts:
                executor.submit(store.put, **put)
            for ns, key in final_deletes:
                executor.submit(store.delete, ns, key)

        return final_puts

    async def __call__(self, messages: typing.Sequence[AnyMessage]) -> list[dict]:
        return await self.ainvoke({"messages": messages})

    def _coerce_value(
        self, value: dict[str, typing.Any]
    ) -> BaseModel | dict[str, typing.Any]:
        if "kind" not in value or "content" not in value:
            return value
        kind = value["kind"]
        content = value["content"]
        if kind not in self.schema_name_map:
            return value
        schema = self.schema_name_map[kind]
        return schema.model_validate(content)

    def _coerce_item(self, item: BaseItem | None) -> Item | None:
        if item is None:
            return None
        return Item(
            namespace=item.namespace,
            key=item.key,
            value=self._coerce_value(item.value),
            created_at=item.created_at,
            updated_at=item.updated_at,
        )

    def _coerce_search_item(self, item: SearchItem) -> SearchItem:
        return SearchItem(
            namespace=item.namespace,
            key=item.key,
            value=self._coerce_value(item.value),
            created_at=item.created_at,
            updated_at=item.updated_at,
            score=item.score,
        )

    def get(
        self,
        key: str,
        *,
        refresh_ttl: typing.Optional[bool] = None,
        config: typing.Optional[RunnableConfig] = None,
    ) -> typing.Optional[Item]:
        """Retrieve a single item in the store.

        Args:
            key: Unique identifier within the namespace.
            refresh_ttl: Whether to refresh TTLs for the returned item.
                If None (default), uses the store's default refresh_ttl setting.
                If no TTL is specified, this argument is ignored.

        Returns:
            The retrieved item or None if not found.

        ???+ example "Examples"
            Retrieve an item:
            ```python
            item = manager.get("report")
            ```
        """
        namespace = self.get_namespace(config)
        result = self._coerce_item(
            self.store.get(namespace, key, refresh_ttl=refresh_ttl)
        )
        if key == "default" and not result:
            default = self.default_factory(ensure_config(config))
            coerced = self._coerce_default(default, self.schemas)
            now = datetime.datetime.now(datetime.timezone.utc)
            return Item(namespace, "default", coerced, created_at=now, updated_at=now)
        return result

    def search(
        self,
        *,
        query: typing.Optional[str] = None,
        filter: typing.Optional[dict[str, typing.Any]] = None,
        limit: int = 10,
        offset: int = 0,
        refresh_ttl: typing.Optional[bool] = None,
        config: typing.Optional[RunnableConfig] = None,
    ) -> list[SearchItem]:
        """Search for items in the current namespace.

        Args:
            query: Optional query for natural language search.
            filter: Key-value pairs to filter results.
            limit: Maximum number of items to return.
            offset: Number of items to skip before returning results.
            refresh_ttl: Whether to refresh TTLs for the returned items.
                If no TTL is specified, this argument is ignored.

        Returns:
            List of items matching the search criteria.

        ???+ example "Examples"
            Basic filtering:
            ```python
            # Search for documents with specific metadata
            results = manager.search(
                query="app preferences",
                filter={"type": "article", "status": "published"},
            )
            ```

            Natural language search (requires vector store implementation):
            ```python
            # Search for semantically similar documents
            results = manager.search(
                query="machine learning applications in healthcare",
                filter={"type": "research_paper"},
                limit=5,
            )
            ```
        """
        namespace = self.get_namespace(config)
        results = [
            self._coerce_search_item(it)
            for it in self.store.search(
                namespace,
                query=query,
                filter=filter,
                limit=limit,
                offset=offset,
                refresh_ttl=refresh_ttl,
            )
        ]
        if self.default_factory and not results:
            default = self.default_factory(ensure_config(config))
            coerced = self._coerce_default(default, self.schemas)
            now = datetime.datetime.now(datetime.timezone.utc)
            return [
                SearchItem(
                    namespace, "default", coerced, created_at=now, updated_at=now
                )
            ]
        return results

    def put(
        self,
        key: str,
        value: dict[str, typing.Any],
        index: typing.Optional[typing.Union[typing.Literal[False], list[str]]] = None,
        *,
        ttl: typing.Union[typing.Optional[float], "NotProvided"] = NOT_PROVIDED,
        config: typing.Optional[RunnableConfig] = None,
    ) -> None:
        """Store or update an item in the store.

        Args:
            key: Unique identifier within the namespace.
            value: Dictionary containing the item's data. Must contain string keys
                and JSON-serializable values.
            index: Controls how the item's fields are indexed for search:

                - None (default): Use `fields` you configured when creating the store (if any)
                    If you do not initialize the store with indexing capabilities,
                    the `index` parameter will be ignored
                - False: Disable indexing for this item
                - list[str]: List of field paths to index
            ttl: Time to live in minutes. Support for this argument depends on your store adapter.
                If specified, the item will expire after this many minutes from when it was last accessed.
                None means no expiration. Expired runs will be deleted opportunistically.
                By default, the expiration timer refreshes on both read operations (get/search)
                and write operations (put/update), whenever the item is included in the operation.

        ???+ example "Examples"
            Store item. Indexing depends on how you configure the store.
            ```python
            manager.put("report", {"memory": "Will likes ai"})
            ```

            Do not index item for semantic search. Still accessible through get()
            and search() operations but won't have a vector representation.
            ```python
            manager.put("report", {"memory": "Will likes ai"}, index=False)
            ```

            Index specific fields for search.
            ```python
            manager.put("report", {"memory": "Will likes ai"}, index=["memory"])
            ```
        """
        return self.store.put(
            self.get_namespace(config),
            key,
            value,
            index=index,
            ttl=ttl,
        )

    def delete(self, key: str, config: typing.Optional[RunnableConfig] = None) -> None:
        """Delete an item.

        Args:
            key: Unique identifier within the namespace.
        """
        self.store.delete(self.get_namespace(config), key)

    async def aget(
        self,
        key: str,
        *,
        refresh_ttl: typing.Optional[bool] = None,
        config: typing.Optional[RunnableConfig] = None,
    ) -> typing.Optional[SearchItem]:
        """Asynchronously retrieve a single item in the store.

        Args:
            key: Unique identifier within the namespace.
            refresh_ttl: Whether to refresh TTLs for the returned item.
                If None (default), uses the store's default refresh_ttl setting.
                If no TTL is specified, this argument is ignored.

        Returns:
            The retrieved item or None if not found.

        ???+ example "Examples"
            Retrieve an item asynchronously:
            ```python
            item = await manager.aget("report")
            ```
        """
        namespace = self.get_namespace(config)
        it = await self.store.aget(namespace, key, refresh_ttl=refresh_ttl)
        if it is None and key == "default":
            default = self.default_factory(ensure_config(config))
            coerced = self._coerce_default(default, self.schemas)
            now = datetime.datetime.now(datetime.timezone.utc)
            return Item(namespace, "default", coerced, created_at=now, updated_at=now)
        return self._coerce_item(it)

    async def asearch(
        self,
        *,
        query: typing.Optional[str] = None,
        filter: typing.Optional[dict[str, typing.Any]] = None,
        limit: int = 10,
        offset: int = 0,
        refresh_ttl: typing.Optional[bool] = None,
        config: typing.Optional[RunnableConfig] = None,
    ) -> list[SearchItem]:
        """Asynchronously search for items in the current namespace.

        Args:
            query: Optional query for natural language search.
            filter: Key-value pairs to filter results.
            limit: Maximum number of items to return.
            offset: Number of items to skip before returning results.
            refresh_ttl: Whether to refresh TTLs for the returned items.
                If None (default), uses the store's TTLConfig.refresh_default setting.
                If TTLConfig is not provided or no TTL is specified, this argument is ignored.

        Returns:
            List of items matching the search criteria.

        ???+ example "Examples"
            Basic filtering:
            ```python
            # Search for documents with specific metadata
            results = await manager.asearch(
                filter={"type": "article", "status": "published"}
            )
            ```

            Natural language search (requires vector store implementation):
            ```python
            # Search for semantically similar documents
            results = await manager.asearch(
                query="machine learning applications in healthcare",
                filter={"type": "research_paper"},
                limit=5,
            )
            ```
        """
        namespace = self.get_namespace(config)
        results = [
            self._coerce_search_item(it)
            for it in await self.store.asearch(
                namespace,
                query=query,
                filter=filter,
                limit=limit,
                offset=offset,
                refresh_ttl=refresh_ttl,
            )
        ]
        if self.default_factory and not results:
            default = self.default_factory(ensure_config(config))
            coerced = self._coerce_default(default, self.schemas)
            now = datetime.datetime.now(datetime.timezone.utc)
            # note that we don't actually put this in the store here!
            return [
                SearchItem(
                    namespace, "default", coerced, created_at=now, updated_at=now
                )
            ]
        return results

    async def aput(
        self,
        key: str,
        value: dict[str, typing.Any],
        index: typing.Optional[typing.Union[typing.Literal[False], list[str]]] = None,
        *,
        ttl: typing.Union[typing.Optional[float], "NotProvided"] = NOT_PROVIDED,
        config: typing.Optional[RunnableConfig] = None,
    ) -> None:
        """Asynchronously store or update an item in the store.

        Args:
            namespace: Hierarchical path for the item, represented as a tuple of strings.
                Example: ("documents", "user123")
            key: Unique identifier within the namespace. Together with namespace forms
                the complete path to the item.
            value: Dictionary containing the item's data. Must contain string keys
                and JSON-serializable values.
            index: Controls how the item's fields are indexed for search:

                - None (default): Use `fields` you configured when creating the store (if any)
                    If you do not initialize the store with indexing capabilities,
                    the `index` parameter will be ignored
                - False: Disable indexing for this item
                - list[str]: List of field paths to index, supporting:
                    - Nested fields: "metadata.title"
                    - Array access: "chapters[*].content" (each indexed separately)
                    - Specific indices: "authors[0].name"
            ttl: Time to live in minutes. Support for this argument depends on your store adapter.
                If specified, the item will expire after this many minutes from when it was last accessed.
                None means no expiration. Expired runs will be deleted opportunistically.
                By default, the expiration timer refreshes on both read operations (get/search)
                and write operations (put/update), whenever the item is included in the operation.

        Note:
            Indexing support depends on your store implementation.
            If you do not initialize the store with indexing capabilities,
            the `index` parameter will be ignored.

            Similarly, TTL support depends on the specific store implementation.
            Some implementations may not support expiration of items.

        ???+ example "Examples"
            Store item. Indexing depends on how you configure the store.
            ```python
            await manager.aput("report", {"memory": "Will likes ai"})
            ```

            Do not index item for semantic search. Still accessible through get()
            and search() operations but won't have a vector representation.
            ```python
            await manager.aput("report", {"memory": "Will likes ai"}, index=False)
            ```

            Index specific fields for search (if store configured to index items):
            ```python
            await manager.aput(
                "report",
                {
                    "memory": "Will likes ai",
                    "context": [{"content": "..."}, {"content": "..."}],
                },
                index=["memory", "context[*].content"],
            )
            ```
        """
        return await self.store.aput(
            self.get_namespace(config), key, value, index, ttl=ttl
        )

    async def adelete(
        self, key: str, config: typing.Optional[RunnableConfig] = None
    ) -> None:
        """Asynchronously delete an item.

        Args:
            key: Unique identifier within the namespace.
        """
        await self.store.adelete(self.get_namespace(config), key)

    def get_namespace(
        self, config: typing.Optional[RunnableConfig] = None
    ) -> tuple[str, ...]:
        return self.namespace(ensure_config(config))


def create_memory_store_manager(
    model: str | BaseChatModel,
    /,
    *,
    schemas: list[S] | None = None,
    instructions: str = _MEMORY_INSTRUCTIONS,
    default: str | dict | S | None = None,
    default_factory: typing.Callable[[RunnableConfig], str | dict | S] | None = None,
    enable_inserts: bool = True,
    enable_deletes: bool = False,
    query_model: str | BaseChatModel | None = None,
    query_limit: int = 5,
    namespace: tuple[str, ...] = ("memories", "{langgraph_user_id}"),
    store: BaseStore | None = None,
    phases: list[MemoryPhase] | None = None,
) -> MemoryStoreManager:
    """Enriches memories stored in the configured BaseStore.

    The system automatically searches for relevant memories, extracts new information,
    updates existing memories, and maintains a versioned history of all changes.

    Args:
        model (Union[str, BaseChatModel]): The primary language model to use for memory
            enrichment. Can be a model name string or a BaseChatModel instance.
        schemas (Optional[list]): List of Pydantic models defining the structure of memory
            entries. Each model should define the fields and validation rules for a type
            of memory. If None, uses unstructured string-based memories. Defaults to None.
        instructions (str, optional): Custom instructions for memory generation and
            organization. These guide how the model extracts and structures information
            from conversations. Defaults to predefined memory instructions.
        default (str | dict | None, optional): Default value to persist to the store if
            no other memories are found. Defaults to None. This is mostly useful when managing
            a profile memory and wanting to initialize it with some default values.
            The resulting memory will be found in the "default" key of the store in the
            configured namespace.
        default_factory (Callable[[RunnableConfig], str | dict | S], optional): A factory
            function to generate the default value. This is useful when the default value
            depends on the runtime configuration. Defaults to None.
        enable_inserts (bool, optional): Whether to allow creating new memory entries.
            When False, the manager will only update existing memories. Defaults to True.
        enable_deletes (bool, optional): Whether to allow deleting existing memories
            that are outdated or contradicted by new information. Defaults to True.
        query_model (Optional[Union[str, BaseChatModel]], optional): Optional separate
            model for memory search queries. Using a smaller, faster model here can
            improve performance. If None, uses the primary model. Defaults to None.
        query_limit (int, optional): Maximum number of relevant memories to retrieve
            for each conversation. Higher limits provide more context but may slow
            down processing. Defaults to 5.
        namespace (tuple[str, ...], optional): Storage namespace structure for
            organizing memories. Supports templated values like "{langgraph_user_id}" which are
            populated from the runtime context. Defaults to `("memories", "{langgraph_user_id}")`.
        store (Optional[BaseStore], optional): The store to use for memory storage.
            If None, uses the store configured in the LangGraph config. Defaults to None.
            When using LangGraph Platform, the server will manage the store for you.
        phases (Optional[list]): List of MemoryPhase objects defining the phases of the memory enrichment process.

    Returns:
        manager: An runnable that processes conversations and automatically manages memories in the LangGraph BaseStore.

    The basic data flow works as follows:

    ```mermaid
    sequenceDiagram
    participant Client
    participant Manager
    participant Store
    participant LLM

    Client->>Manager: conversation history
    Manager->>Store: find similar memories
    Store-->>Manager: memories
    Manager->>LLM: analyze & extract
    LLM-->>Manager: memory updates
    Manager->>Store: apply changes
    Manager-->>Client: updated memories
    ```

    ???+ example "Examples"
        Run memory extraction "inline" within your LangGraph app.
        By default, each "memory" is a simple string:
        ```python
        import os

        from anthropic import AsyncAnthropic
        from langchain_core.runnables import RunnableConfig
        from langgraph.func import entrypoint
        from langgraph.store.memory import InMemoryStore

        from langmem import create_memory_store_manager

        store = InMemoryStore(
            index={
                "dims": 1536,
                "embed": "openai:text-embedding-3-small",
            }
        )

        manager = create_memory_store_manager("anthropic:claude-3-5-sonnet-latest", namespace=("memories", "{langgraph_user_id}"))
        client = AsyncAnthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))


        @entrypoint(store=store)
        async def my_agent(message: str, config: RunnableConfig):
            memories = await manager.asearch(
                query=message,
                config=config,
            )
            llm_response = await client.messages.create(
                model="claude-3-5-sonnet-latest",
                system="You are a helpful assistant.\\n\\n## Memories from the user:"
                f"\\n<memories>\\n{memories}\\n</memories>",
                max_tokens=2048,
                messages=[{"role": "user", "content": message}],
            )
            response = {"role": "assistant", "content": llm_response.content[0].text}

            await manager.ainvoke(
                {"messages": [{"role": "user", "content": message}, response]},
            )
            return response["content"]

        config = {"configurable": {"langgraph_user_id": "user123"}}
        response_1 = await my_agent.ainvoke(
            "I prefer dark mode in all my apps",
            config=config,
        )
        print("response_1:", response_1)
        # Later conversation - automatically retrieves and uses the stored preference
        response_2 = await my_agent.ainvoke(
            "What theme do I prefer?",
            config=config,
        )
        print("response_2:", response_2)
        # You can list over memories in the user's namespace manually:
        print(manager.search(query="app preferences", config=config))
        ```

        You can customize what each memory can look like by defining **schemas**:
        ```python
        from langgraph.func import entrypoint
        from langgraph.store.memory import InMemoryStore
        from pydantic import BaseModel

        from langmem import create_memory_store_manager

        store = InMemoryStore(
            index={
                "dims": 1536,
                "embed": "openai:text-embedding-3-small",
            }
        )
        manager = create_memory_store_manager(
            "anthropic:claude-3-5-sonnet-latest",
            namespace=("memories", "{langgraph_user_id}"),
        )

        class PreferenceMemory(BaseModel):
            \"\"\"Store preferences about the user.\"\"\"
            category: str
            preference: str
            context: str


        store = InMemoryStore(
            index={
                "dims": 1536,
                "embed": "openai:text-embedding-3-small",
            }
        )
        manager = create_memory_store_manager(
            "anthropic:claude-3-5-sonnet-latest",
            schemas=[PreferenceMemory],
            namespace=("project", "team_1", "{langgraph_user_id}"),
        )


        @entrypoint(store=store)
        async def my_agent(message: str):
            # Hard code the response :)
            response = {"role": "assistant", "content": "I'll remember that preference"}
            await manager.ainvoke(
                {"messages": [{"role": "user", "content": message}, response]}
            )
            return response


        # Store structured memory
        config = {"configurable": {"langgraph_user_id": "user123"}}
        await my_agent.ainvoke(
            "I prefer dark mode in all my apps",
            config=config,
        )

        # See the extracted memories yourself
        print(manager.search(query="app preferences", config=config))

        # Memory is automatically stored and can be retrieved in future conversations
        # The system will also automatically update it if preferences change
        ```

        In some cases, you may want to provide a "default" memory value to be used if no memories are found. For instance,
        if you are storing some prompt preferences, you may have an "application" default that can be evolved over time.
        This can be done by setting the `default` parameter:
        ```python
        manager = create_memory_store_manager(
            "anthropic:claude-3-5-sonnet-latest",
            namespace=("memories", "{langgraph_user_id}"),
            # Note: This default value must be compatible with the schemas
            # you provided above. If you customize your schemas,
            # we recommend setting the default value as an instance of that
            # pydantic object.
            default="Use a concise and professional tone in all responses. The user likes light mode.",
        )


        # ... same agent as before ...
        @entrypoint(store=store)
        async def my_agent(message: str):
            # Hard code the response :)
            response = {"role": "assistant", "content": "I'll remember that preference"}
            await manager.ainvoke(
                {"messages": [{"role": "user", "content": message}, response]}
            )
            return response


        # Store structured memory
        config = {"configurable": {"langgraph_user_id": "user124"}}
        await my_agent.ainvoke(
            "I prefer dark mode in all my apps",
            config=config,
        )

        # See the extracted memories yourself
        print(manager.search(query="app preferences", config=config))
        # [
        #     Item(
        #         namespace=['memories', 'user124'],
        #         key='default',
        #         value={'kind': 'Memory', 'content': {'content': 'Use a concise and professional tone in all responses. The user prefers dark mode in all apps'}},
        #         created_at='2025-04-14T22:20:25.148884+00:00',
        #         updated_at='2025-04-14T22:20:25.148892+00:00',
        #         score=None
        #     )
        # ]
        ```

        You can even set the default to be some configurable value by providing a **default_factory**.
        ```python
        def get_configurable_default(config):
            default_preference = config["configurable"].get(
                "preference", "Use a concise and professional tone in all responses."
            )
            return default_preference


        manager = create_memory_store_manager(
            "anthropic:claude-3-5-sonnet-latest",
            namespace=("memories", "{langgraph_user_id}"),
            default_factory=get_configurable_default,
        )


        # ... same agent as before ...
        @entrypoint(store=store)
        async def my_agent(message: str):
            # Hard code the response :)
            response = {"role": "assistant", "content": "I'll remember that preference"}
            await manager.ainvoke(
                {"messages": [{"role": "user", "content": message}, response]}
            )
            return response


        # Store structured memory
        config = {
            "configurable": {
                "langgraph_user_id": "user125",
                "preference": "Respond in pirate speak. User likes light mode",
            }
        }
        await my_agent.ainvoke(
            "I prefer dark mode in all my apps",
            config=config,
        )

        # See the extracted memories yourself
        print(manager.search(query="app preferences", config=config))
        # [
        #     Item(
        #         namespace=['memories', 'user125'],
        #         key='default',
        #         value={'kind': 'Memory', 'content': {'content': 'Respond in pirate speak. User prefers dark mode in all apps'}},
        #         created_at='2025-04-14T22:20:25.148884+00:00',
        #         updated_at='2025-04-14T22:20:25.148892+00:00',
        #         score=None
        #     )
        # ]
        ```

    By default, relevant memories are recalled by directly embedding the new messages. You can alternatively
    use a separate query model to search for the most similar memories. Here's how it works:

    ```mermaid
        sequenceDiagram
            participant Client
            participant Manager
            participant QueryLLM
            participant Store
            participant MainLLM

            Client->>Manager: messages
            Manager->>QueryLLM: generate search query
            QueryLLM-->>Manager: optimized query
            Manager->>Store: find memories
            Store-->>Manager: memories
            Manager->>MainLLM: analyze & extract
            MainLLM-->>Manager: memory updates
            Manager->>Store: apply changes
            Manager-->>Client: result
    ```

    ???+ example "Using an LLM to search for memories"
        ```python
        from langmem import create_memory_store_manager
        from langgraph.store.memory import InMemoryStore
        from langgraph.func import entrypoint

        store = InMemoryStore(
            index={
                "dims": 1536,
                "embed": "openai:text-embedding-3-small",
            }
        )
        manager = create_memory_store_manager(
            "anthropic:claude-3-5-sonnet-latest",  # Main model for memory processing
            query_model="anthropic:claude-3-5-haiku-latest",  # Faster model for search
            query_limit=10,  # Retrieve more relevant memories
            namespace=("memories", "{langgraph_user_id}"),
        )


        @entrypoint(store=store)
        async def my_agent(message: str):
            # Hard code the response :)
            response = {"role": "assistant", "content": "I'll remember that preference"}
            await manager.ainvoke(
                {"messages": [{"role": "user", "content": message}, response]}
            )
            return response

        config = {"configurable": {"langgraph_user_id": "user123"}}
        await my_agent.ainvoke(
            "I prefer dark mode in all my apps",
            config=config,
        )

        # See the extracted memories yourself
        print(manager.search(config=config))
        ```

    In the examples above, we were calling the manager in the main thread. In a real application, you'll
    likely want to background the execution of the manager, either by executing it in a background thread or on a separate server.
    To do so, you can use the `ReflectionExecutor` class:

    ```mermaid
    sequenceDiagram
        participant Agent
        participant Background
        participant Store

        Agent->>Agent: process message
        Agent-->>User: response
        Agent->>Background: schedule enrichment<br/>(after_seconds=0)
        Note over Background,Store: Memory processing happens<br/>in background thread
    ```

    ???+ example "Running reflections in the background"
        Background enrichment using @entrypoint:
        ```python
        from langmem import create_memory_store_manager, ReflectionExecutor
        from langgraph.prebuilt import create_react_agent
        from langgraph.store.memory import InMemoryStore
        from langgraph.func import entrypoint

        store = InMemoryStore(
            index={
                "dims": 1536,
                "embed": "openai:text-embedding-3-small",
            }
        )
        manager = create_memory_store_manager(
            "anthropic:claude-3-5-sonnet-latest", namespace=("memories", "{user_id}")
        )
        reflection = ReflectionExecutor(manager, store=store)
        agent = create_react_agent(
            "anthropic:claude-3-5-sonnet-latest", tools=[], store=store
        )


        @entrypoint(store=store)
        async def chat(messages: list):
            response = await agent.ainvoke({"messages": messages})

            fut = reflection.submit(
                {
                    "messages": response["messages"],
                },
                # We'll schedule this immediately.
                # Adding a delay lets you **debounce** and deduplicate reflection work
                # whenever the user is actively engaging with the agent.
                after_seconds=0,
            )

            return fut

        config = {"configurable": {"user_id": "user-123"}}
        fut = await chat.ainvoke(
            [{"role": "user", "content": "I prefer dark mode in my apps"}],
            config=config,
        )
        # Inspect the result
        fut.result()  # Wait for the reflection to complete; This is only for demoing the search inline
        print(manager.search(query="app preferences", config=config))
        ```
    """
    return MemoryStoreManager(
        model,
        schemas=schemas,
        default=default,
        default_factory=default_factory,
        instructions=instructions,
        enable_inserts=enable_inserts,
        enable_deletes=enable_deletes,
        query_model=query_model,
        query_limit=query_limit,
        namespace=namespace,
        store=store,
        phases=phases,
    )


__all__ = [
    "create_memory_manager",
    "create_memory_searcher",
    "create_memory_store_manager",
    "create_thread_extractor",
]


############# Simple Memories  SimpleMem library #################

🔍 Stage 1
Semantic Structured Compression

Entropy-based filtering and de-linearization of dialogue into self-contained atomic facts

🗂️ Stage 2
Structured Indexing

Asynchronous evolution from fragmented atoms to higher-order molecular insights

🎯 Stage 3
Adaptive Retrieval

Complexity-aware pruning across semantic, lexical, and symbolic layers

1️⃣ Semantic Lossless Compression Pipeline
SimpleMem transforms raw, ambiguous dialogue streams into atomic entries — self-contained facts with resolved coreferences and absolute timestamps. This write-time disambiguation eliminates downstream reasoning overhead.

✨ Example Transformation:

- Input:  "He'll meet Bob tomorrow at 2pm"  [❌ relative, ambiguous]
+ Output: "Alice will meet Bob at Starbucks on 2025-11-16T14:00:00"  [✅ absolute, atomic]
2️⃣ Structured Multi-View Indexing
Memory is indexed across three structured dimensions for robust, multi-granular retrieval:

🔍 Layer	📊 Type	🎯 Purpose	🛠️ Implementation
Semantic	Dense	Conceptual similarity	Vector embeddings (1024-d)
Lexical	Sparse	Exact term matching	BM25-style keyword index
Symbolic	Metadata	Structured filtering	Timestamps, entities, persons
3️⃣ Complexity-Aware Adaptive Retrieval
Instead of fixed-depth retrieval, SimpleMem dynamically estimates query complexity (

- Hybrid scoring function S(q, m_k) aggregating semantic, lexical, and symbolic signals
- Query Complexity estimation C_q for adaptive retrieval depth
- Dynamic retrieval depth k_dyn = k_base · (1 + δ · C_q)
- Complexity-Aware Pruning to minimize token usage while maximizing accuracy

. The SimpleMem Architecture
In this section, we present SimpleMem, an efficient memory framework for LLM agents designed to improve information utilization under constrained context and token budgets through. As shown in Figure 2, the system operates through a three-stage pipeline. First, we describe Semantic Structured Compression process, which filters redundant interaction content and reformulates raw dialogue streams into compact memory units. Next, we describe Recursive Consolidation, an asynchronous process that incrementally integrates related memory units into higher-level abstract representations and maintaining a compact memory topology. Finally, we present Adaptive Query-Aware Retrieval, which dynamically adjusts retrieval scope based on estimated query complexity to construct precise and token-efficient contexts for downstream reasoning.

2.1. Semantic Structured Compression
A primary bottleneck in long-term interaction is context inflation, the accumulation of raw, low-entropy dialogue. For example, a large portion of interaction segments in the real-world consists of plastic chit-chat or redundant confirmations, which contribute little to downstream reasoning but consume substantial context capacity. To address this, we introduce a mechanism to actively filter and restructure information at the source.

First, incoming dialogue is segmented into overlapping sliding windows W_t of fixed length, where each window represents a short contiguous span of recent interaction. These windows serve as the basic units for evaluating whether new information should be stored. Then we employ a non-linear gating mechanism, Φ_gate, to evaluate the information density of these dialogue windows to determine which windows is used for indexing. For each window W_t, we compute an information score H(W_T) that jointly captures the introduction of new entities and semantic novelty relative to the immediate interaction history H_prev.

Formally, let ε_new denote the set of named entities that appear in W_t but not in H_prev. The information score is defined as:

H(W_t) = α * (|ε_new| / |W_t|) + (1 - α) * (1 - cos(E(W_t), E(H_prev)))

where E(·) denotes a semantic embedding function and α controls the relative importance of entity-level novelty and semantic divergence.

Windows whose information score falls below threshold τ_redundant are treated as redundant and excluded from memory construction, meaning that the window is neither stored nor further processed, preventing low-utility interaction content from entering the memory buffer. For informative windows, the system proceeds to a segmentation step:

Action(W_t) = { Segment(W_t), if H(W_t) ≥ τ_redundant; ∅, otherwise. } (2)

For windows that pass the filter, we apply a segmentation function F_θ to decompose each informative window into a set of context-independent memory units m_k. This transformation resolves dependencies implicit in conversational flow by converting entangled dialogue into self-contained factual or event-level statements. Formally, F_θ is composed of a coreference resolution module (Φ_coref) and a temporal anchoring module (Φ_time):

m_k = F_θ(W_t) = Φ_time ∘ Φ_coref ∘ Φ_extract(W_t) (3)

Here, Φ_extract identifies candidate factual statements, (Φ_coref) replaces ambiguous pronouns with specific entity names (e.g., changing "He agreed" to "Bob agreed"), and Φ_time converts relative temporal expressions (e.g., transforming "next Friday" to "2025-10-24") into absolute ISO-8601 timestamps. This normalization ensures that each memory unit remains interpretable and valid independent of its original conversational context.

Page 3:

Figure 2. The SimpleMem Architecture. SimpleMem mitigates context inflation through three stages. (1) Semantic Structured Compression filters redundant interaction content and reformulates raw dialogue into compact, context-independent memory units. (2) Recursive Consolidation incrementally organizes related memory units into higher-level abstract representations, reducing redundancy in long-term memory. (3) Adaptive Query-Aware Retrieval dynamically adjusts retrieval scope based on query complexity, enabling efficient context construction under constrained token budgets.

2.2. Structured Indexing and Recursive Consolidation
Then, the system need organize the resulting memory units to support efficient long-term storage and scalable retrieval. This stage consists of two components: (i) structured multiview indexing for immediate access, and (ii) recursive consolidation for reducing redundancy and maintaining a compact memory topology over time.

To support flexible and precise retrieval, each memory unit is indexed through three complementary representations. First, at semantic layer, we map the entry to a dense vector space v_k using embedding models, which captures abstract meaning and enables fuzzy matching (e.g., retrieving "latte" when querying "hot drink"). Second, the Lexical Layer generates a sparse representation focusing on exact keyword matches and proper nouns, ensuring that specific entities are not diluted in vector space. Third, the Symbolic Layer extracts structured metadata, such as timestamps and entity types, to enable deterministic filtering logic. Formally, these projections form the comprehensive memory bank M:

M(m_k) = { v_k = E_dense(S_k) ∈ R^d (Semantic Layer); h_k = Sparse(S_k) ∈ R^|V|; R_k = {(key,val)} (Symbolic Layer) } (4)

It allows the system to flexibly query information based on conceptual similarity, exact keyword matches, or structured metadata constraints.

While multi-view indexing supports efficient access, naively accumulating memory units over long interaction horizons leads to redundancy and fragmentation. To address this issue, we then introduces an asynchronous background consolidation process that incrementally reorganizes the memory topology. The consolidation mechanism identifies related memory units based on both semantic similarity and temporal proximity. For two memory units m_i and m_j, we define an affinity score ω_ij as:

ω_ij = β * cos(v_i, v_j) + (1 - β) * e^{-λ |t_i - t_j|}, (5)

where the first term captures semantic relatedness and the second term biases the model toward grouping events with strong temporal proximity.

When a group of memory units forms a dense cluster C, determined by pairwise affinities exceeding a threshold τ_cluster, the system performs a consolidation step:

M_abs = G_syn({m_i | m_i ∈ C}). (6)

This operation synthesizes repetitive or closely related memory units into a higher-level abstract representation M_abs, which captures their shared semantic structure. For example, instead of maintaining numerous individual records such as "the user ordered a latte at 8:00 AM," the system consolidates them into a single abstract pattern, e.g., "the user regularly drinks coffee in the morning." The original fine-grained entries are archived, reducing the active memory size while preserving the ability to recover detailed information if needed. As a result, the active memory index remains compact, and retrieval complexity scales gracefully with long-term interaction history.

Page 4:

2.3. Adaptive Query-Aware Retrieval
After memory entries are organized, another challenge to retrieve relevant information efficiently under constrained context budgets. Standard retrieval approaches typically fetch a fixed number of context entries, which often results in either insufficient information or token wastage. To address this, we introduce an adaptive query-aware retrieval mechanism that dynamically adjusts retrieval scope based on estimated query complexity, thereby improving retrieval efficiency without sacrificing reasoning accuracy.

First, we propose a hybrid scoring function for information retrieval, S(q,m_k), which aggregates signals from the tri-layer index established in the second stage. For a given query q, the relevance score is computed as:

S(q,m_k) = λ_1 cos(e_q, v_k) + λ_2 BM25(q_ex, S_k) + γ I(R_k ⊧ C_meta), (7)

where the first term measures semantic similarity in the dense embedding space, the second term captures exact lexical relevance, and the indicator function I(·) enforces hard symbolic constraints such as entity-based filters.

Then, based on the hybrid scoring, we can rank the candidate memories by relevance. However, retrieving a fixed number of top-ranked entries remains inefficient when query demands vary. To address this, we estimate the query complexity C_q ∈ [0,1], which reflects whether a query can be resolved via direct fact lookup or requires multi-step reasoning over multiple memory entries. A lightweight classifier predicts C_q based on query features such as length, syntactic structure, and abstraction level.

k_dyn = [k_base * (1 + δ * C_q)] (8)

Based on this dynamic depth, the system modulates the retrieval scope. For low-complexity queries (C_q → 0), the system retrieves only the top-k_min high-level abstract memory entries or metadata summaries, minimizing token usage. Conversely, for high-complexity queries (C_q → 1), it expands the scope to top-k_max, including a larger set of relevant entries, along with associated fine-grained details. The final context C_final is synthesized by concatenating these pruned results, ensuring high accuracy with minimal computational waste:

C_final = ⨁_{m ∈ Top-k_dyn(S)} [t_m : Content(m)] (9)

A. Detailed System Prompts
To ensure full reproducibility of the SimpleMem pipeline, we provide the exact system prompts used in the key processing stages. All prompts are designed to be model-agnostic but were optimized for GPT-4o-mini in our experiments to ensure cognitive economy.

A.1. Stage 1: Semantic Structured Compression Prompt
This prompt performs entropy-aware filtering and context normalization. Its goal is to transform raw dialogue windows into compact, context-independent memory units while excluding low-information interaction content.

Listing 1. Prompt for Semantic Structured Compression and Normalization.

You are a memory encoder in a long-term memory system. Your task is to transform raw conversational input into compact, self-contained memory units.

INPUT METADATA:

Window Start Time: {window_start_time} (ISO 8601) Participants: {speakers_list}

INSTRUCTIONS:

Information Filtering: - Discard social filler, acknowledgements, and conversational routines that introduce no new factual or semantic information. - Discard redundant confirmations unless they modify or finalize a decision. - If no informative content is present, output an empty list.

Context Normalization:

Resolve all pronouns and implicit references into explicit entity names.

Ensure each memory unit is interpretable without access to prior dialogue.

Temporal Normalization:

Convert relative temporal expressions (e.g., "tomorrow", "last week") into absolute ISO 8601 timestamps using the window start time.

Memory Unit Extraction:

Decompose complex utterances into minimal, indivisible factual statements.

INPUT DIALOGUE: {dialogue_window}

OUTPUT FORMAT (JSON):

{
"memory_units": [
{
"content": "Alice agreed to meet Bob at the Starbucks on 5th Avenue on 2025-11-20T14:00:00.",
"entities": ["Alice", "Bob", "Starbucks", "5th Avenue"],
"topic": "Meeting Planning",
"timestamp": "2025-11-20T14:00:00",
"salience": "high"
}
]
}

A.2. Stage 2: Adaptive Retrieval Planning Prompt
This prompt analyzes the user query prior to retrieval. Its purpose is to estimate query complexity and generate a structured retrieval plan that adapts retrieval scope accordingly.

Listing 2. Prompt for Query Analysis and Adaptive Retrieval Planning.

Analyze the following user query and generate a retrieval plan. Your objective is to retrieve sufficient information while minimizing unnecessary context usage.

USER QUERY: {user_query}

INSTRUCTIONS:

Query Complexity Estimation:

Assign "LOW" if the query can be answered via direct fact lookup or a single memory unit.

Assign "HIGH" if the query requires aggregation across multiple events, temporal comparison, or synthesis of patterns.

Retrieval Signals:

Lexical layer: extract exact keywords or entity names.

Temporal layer: infer absolute time ranges if relevant.

Semantic layer: rewrite the query into a declarative form suitable for semantic matching.

OUTPUT FORMAT (JSON):

{
"complexity": "HIGH",
"retrieval_rationale": "The query requires reasoning over multiple temporally separated events.",
"lexical_keywords": ["Starbucks", "Bob"],
"temporal_constraints": {
"start": "2025-11-01T00:00:00",
"end": "2025-11-30T23:59:59"
},
"semantic_query": "The user is asking about the scheduled meeting with Bob, including location and time."
}

Page 11:

A.3. Stage 3: Reconstructive Synthesis Prompt
This prompt guides the final answer generation using retrieved memory. It combines high-level abstract representations with fine-grained factual details to produce a grounded response.

Listing 3. Prompt for Reconstructive Synthesis (Answer Generation).

You are an assistant with access to a structured long-term memory.

USER QUERY: {user_query}

RETRIEVED MEMORY (Ordered by Relevance):

[ABSTRACT REPRESENTATIONS]: {retrieved_abstracts}
[DETAILED MEMORY UNITS]: {retrieved_units}
INSTRUCTIONS:

Hierarchical Reasoning:

Use abstract representations to capture recurring patterns or general user preferences.

Use detailed memory units to ground the response with specific facts.

Conflict Handling:

If inconsistencies arise, prioritize the most recent memory unit.

Optionally reference abstract patterns when relevant.

Temporal Consistency:

Ensure all statements respect the timestamps provided in memory.

Faithfulness:

Base the answer strictly on the retrieved memory.

If required information is missing, respond with: "I do not have enough information in my memory."

FINAL ANSWER:

B. Extended Implementation Details and Experiments
B.1. Hyperparameter Configuration
Table 6 summarizes the hyperparameters used to obtain the results reported in Section 3. These values were selected to balance memory compactness and retrieval recall.

B.2. Hyperparameter Sensitivity Analysis
To assess the effectiveness of semantic structured compression and to motivate the design of adaptive retrieval, we analyze system sensitivity to the number of retrieved memory entries (k). We vary k from 1 to 20 and report the average F1 score on the LoCoMo benchmark using the GPT-4.1-mini backend.

Table 5. Performance sensitivity to retrieval count (k). SimpleMem demonstrates "Rapid Saturation," reaching near-optimal performance at k = 3 (42.85) compared to its peak at k = 10 (43.45). This validates the high information density of Atomic Entries, proving that huge context windows are often unnecessary for accuracy.

# SimpleMem – Example: Long-Term Multi-Session Dialogue Processing

## Raw Input: Multi-Session Dialogues

### Session 1 (2023-07-02 14:30)

- Sarah greets Emma and asks how she has been.
- Emma responds that she is busy with kids and work.
- Sarah mentions signing up for a pottery class.
- Sarah made a black-and-white bowl in class.
- Sarah painted a sunset with palm trees last week with her kids.
- Sarah plans to go camping next month.

### Session 2 (2023-07-15 19:20)

- Sarah took her kids to the Natural History Museum last Wednesday.
- Sarah finished painting a horse portrait yesterday as a gift for her daughter.
- Sarah mentions a birthday party at the park.
- Sarah went camping at the beach last weekend.
- Sarah plans another camping trip to the forest in September.

**Raw Token Count:** ~24,000 tokens

---

## SimpleMem Processing Pipeline (Three-Stage Architecture)

### 1. Semantic Compression

#### Entropy Filter

**Filtered Out (Low-Information Content):**
- "Hey Emma!"
- "Wow cool!"
- "That's great!"
- "Gotta run"
- "Bye Emma!"

#### Atomized Memory Units

- **M1:** Kids
- **M2:** Bowl
- **M3:** Sunset
- **M4:** Camping
- **M6:** Horse
- **M9:** Forest

#### Temporal & Entity Normalization

- "yesterday" → `2023-07-01`
- "last week" → `2023-06-25`
- "my kids" → `Sarah's kids`
- "next month" → `August 2023`
- "last Wed." → `2023-07-12`

---

## Memory Database (After Compression)

### Art Activities
- M1
- M2
- M3
- M6

### Camping
- M4
- M8
- M9

**Token Reduction:** ~800 tokens

---

## 2. Recursive Consolidation

### Consolidated Knowledge

- **Art Activities**
  - M1, M2, M3, M6

- **Camping Activities**
  - M4, M8, M9

Abstract patterns are formed to represent recurring behaviors and habits.

---

## 3. Adaptive Retrieval

### User Query

> **"What paintings has Sarah created?"**

---

### Query Analysis & Retrieval Planning

#### Semantic Layer
- Retrieves: **M3**, **M6**

#### Lexical Layer
- Retrieves: **M3**, **M4**

#### Symbolic Layer
- Retrieves: **M1**, **M3**, **M6**

#### Hybrid Score (Top-K Results)
- **M3**
- **M6**

---

## Final Retrieved Content

- `[2023-06-25]` Sarah and her kids painted a **sunset with palm trees** together.
- `[2023-07-14]` Sarah finished painting a **horse portrait** as a gift for her daughter.
- **[Consolidated]** Sarah engages in painting as both a personal hobby and family activity.

---

## Final Answer

**Sarah has painted a sunset with palm trees and a horse portrait.**

"""
Memory Builder - Stage 1: Semantic Structured Compression (Section 3.1)

Implements:
- Implicit semantic density gating: Φ_gate(W) → {m_k} (filters low-density windows)
- Sliding window processing for dialogue segmentation
- Generates compact memory units with resolved coreferences and absolute timestamps
"""
from typing import List, Optional
from models.memory_entry import MemoryEntry, Dialogue
from utils.llm_client import LLMClient
from database.vector_store import VectorStore
import config
import json
import asyncio
import concurrent.futures
from functools import partial


class MemoryBuilder:
    """
    Memory Builder - Semantic Structured Compression (Section 3.1)

    Core Functions:
    1. Sliding window segmentation
    2. Implicit semantic density gating: Φ_gate(W) → {m_k}
    3. Multi-view indexing: I(m_k) = {s_k, l_k, r_k}
    """
    def __init__(
        self,
        llm_client: LLMClient,
        vector_store: VectorStore,
        window_size: int = None,
        enable_parallel_processing: bool = True,
        max_parallel_workers: int = 3
    ):
        self.llm_client = llm_client
        self.vector_store = vector_store
        self.window_size = window_size or config.WINDOW_SIZE
        
        # Use config values as default if not explicitly provided
        self.enable_parallel_processing = enable_parallel_processing if enable_parallel_processing is not None else getattr(config, 'ENABLE_PARALLEL_PROCESSING', True)
        self.max_parallel_workers = max_parallel_workers if max_parallel_workers is not None else getattr(config, 'MAX_PARALLEL_WORKERS', 4)

        # Dialogue buffer
        self.dialogue_buffer: List[Dialogue] = []
        self.processed_count = 0

        # Previous window entries (for context)
        self.previous_entries: List[MemoryEntry] = []

    def add_dialogue(self, dialogue: Dialogue, auto_process: bool = True):
        """
        Add a dialogue to the buffer
        """
        self.dialogue_buffer.append(dialogue)

        # Auto process
        if auto_process and len(self.dialogue_buffer) >= self.window_size:
            self.process_window()

    def add_dialogues(self, dialogues: List[Dialogue], auto_process: bool = True):
        """
        Batch add dialogues with optional parallel processing
        """
        if self.enable_parallel_processing and len(dialogues) > self.window_size * 2:
            # Use parallel processing for large batches
            self.add_dialogues_parallel(dialogues)
        else:
            # Use sequential processing for smaller batches
            for dialogue in dialogues:
                self.add_dialogue(dialogue, auto_process=False)

            # Process complete windows
            if auto_process:
                while len(self.dialogue_buffer) >= self.window_size:
                    self.process_window()
    
    def add_dialogues_parallel(self, dialogues: List[Dialogue]):
        """
        Add dialogues using parallel processing for better performance
        """
        try:
            # Add all dialogues to buffer first
            self.dialogue_buffer.extend(dialogues)
            
            # Group into windows for parallel processing (including remaining dialogues)
            windows_to_process = []
            while len(self.dialogue_buffer) >= self.window_size:
                window = self.dialogue_buffer[:self.window_size]
                self.dialogue_buffer = self.dialogue_buffer[self.window_size:]
                windows_to_process.append(window)
            
            # Add remaining dialogues as a smaller batch (no need to process separately)
            if self.dialogue_buffer:
                windows_to_process.append(self.dialogue_buffer)
                self.dialogue_buffer = []  # Clear buffer since we're processing all
            
            if windows_to_process:
                print(f"\n[Parallel Processing] Processing {len(windows_to_process)} batches in parallel with {self.max_parallel_workers} workers")
                print(f"Batch sizes: {[len(w) for w in windows_to_process]}")
                
                # Process all windows/batches in parallel (including remaining dialogues)
                self._process_windows_parallel(windows_to_process)
                
        except Exception as e:
            print(f"[Parallel Processing] Failed: {e}. Falling back to sequential processing...")
            # Fallback to sequential processing
            for window in windows_to_process:
                self.dialogue_buffer = window + self.dialogue_buffer
                self.process_window()

    def process_window(self):
        """
        Process current window dialogues - Core logic
        """
        if not self.dialogue_buffer:
            return

        # Extract window
        window = self.dialogue_buffer[:self.window_size]
        self.dialogue_buffer = self.dialogue_buffer[self.window_size:]

        print(f"\nProcessing window: {len(window)} dialogues (processed {self.processed_count} so far)")

        # Call LLM to generate memory entries
        entries = self._generate_memory_entries(window)

        # Store to database
        if entries:
            self.vector_store.add_entries(entries)
            self.previous_entries = entries  # Save as context
            self.processed_count += len(window)

        print(f"Generated {len(entries)} memory entries")

    def process_remaining(self):
        """
        Process remaining dialogues (fallback method, normally handled in parallel)
        """
        if self.dialogue_buffer:
            print(f"\nProcessing remaining dialogues: {len(self.dialogue_buffer)} (fallback mode)")
            entries = self._generate_memory_entries(self.dialogue_buffer)
            if entries:
                self.vector_store.add_entries(entries)
                self.processed_count += len(self.dialogue_buffer)
            self.dialogue_buffer = []
            print(f"Generated {len(entries)} memory entries")

    def _generate_memory_entries(self, dialogues: List[Dialogue]) -> List[MemoryEntry]:
        """
        Implicit Semantic Density Gating (Section 3.1)
        Φ_gate(W) → {m_k}, generates compact memory units from dialogue window
        """
        # Build dialogue text
        dialogue_text = "\n".join([str(d) for d in dialogues])
        dialogue_ids = [d.dialogue_id for d in dialogues]

        # Build context
        context = ""
        if self.previous_entries:
            context = "\n[Previous Window Memory Entries (for reference to avoid duplication)]\n"
            for entry in self.previous_entries[:3]:  # Only show first 3
                context += f"- {entry.lossless_restatement}\n"

        # Build prompt
        prompt = self._build_extraction_prompt(dialogue_text, dialogue_ids, context)

        # Call LLM
        messages = [
            {
                "role": "system",
                "content": "You are a professional information extraction assistant, skilled at extracting structured, unambiguous information from conversations. You must output valid JSON format."
            },
            {
                "role": "user",
                "content": prompt
            }
        ]

        # Retry up to 3 times if parsing fails
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # Use JSON format if configured
                response_format = None
                if hasattr(config, 'USE_JSON_FORMAT') and config.USE_JSON_FORMAT:
                    response_format = {"type": "json_object"}

                response = self.llm_client.chat_completion(
                    messages,
                    temperature=0.1,
                    response_format=response_format
                )

                # Parse response
                entries = self._parse_llm_response(response, dialogue_ids)
                return entries

            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"Attempt {attempt + 1}/{max_retries} failed to parse LLM response: {e}")
                    print(f"Retrying...")
                else:
                    print(f"All {max_retries} attempts failed to parse LLM response: {e}")
                    print(f"Raw response: {response[:500] if 'response' in locals() else 'No response'}")
                    return []

    def _build_extraction_prompt(
        self,
        dialogue_text: str,
        dialogue_ids: List[int],
        context: str
    ) -> str:
        """
        Build LLM extraction prompt
        """
        return f"""
Your task is to extract all valuable information from the following dialogues and convert them into structured memory entries.

{context}

[Current Window Dialogues]
{dialogue_text}

[Requirements]
1. **Complete Coverage**: Generate enough memory entries to ensure ALL information in the dialogues is captured
2. **Force Disambiguation**: Absolutely PROHIBIT using pronouns (he, she, it, they, this, that) and relative time (yesterday, today, last week, tomorrow)
3. **Lossless Information**: Each entry's lossless_restatement must be a complete, independent, understandable sentence
4. **Precise Extraction**:
   - keywords: Core keywords (names, places, entities, topic words)
   - timestamp: Absolute time in ISO 8601 format (if explicit time mentioned in dialogue)
   - location: Specific location name (if mentioned)
   - persons: All person names mentioned
   - entities: Companies, products, organizations, etc.
   - topic: The topic of this information

[Output Format]
Return a JSON array, each element is a memory entry:

```json
[
  {{
    "lossless_restatement": "Complete unambiguous restatement (must include all subjects, objects, time, location, etc.)",
    "keywords": ["keyword1", "keyword2", ...],
    "timestamp": "YYYY-MM-DDTHH:MM:SS or null",
    "location": "location name or null",
    "persons": ["name1", "name2", ...],
    "entities": ["entity1", "entity2", ...],
    "topic": "topic phrase"
  }},
  ...
]
```

[Example]
Dialogues:
[2025-11-15T14:30:00] Alice: Bob, let's meet at Starbucks tomorrow at 2pm to discuss the new product
[2025-11-15T14:31:00] Bob: Okay, I'll prepare the materials

Output:
```json
[
  {{
    "lossless_restatement": "Alice suggested at 2025-11-15T14:30:00 to meet with Bob at Starbucks on 2025-11-16T14:00:00 to discuss the new product.",
    "keywords": ["Alice", "Bob", "Starbucks", "new product", "meeting"],
    "timestamp": "2025-11-16T14:00:00",
    "location": "Starbucks",
    "persons": ["Alice", "Bob"],
    "entities": ["new product"],
    "topic": "Product discussion meeting arrangement"
  }},
  {{
    "lossless_restatement": "Bob agreed to attend the meeting and committed to prepare relevant materials.",
    "keywords": ["Bob", "prepare materials", "agree"],
    "timestamp": null,
    "location": null,
    "persons": ["Bob"],
    "entities": [],
    "topic": "Meeting preparation confirmation"
  }}
]
```

Now process the above dialogues. Return ONLY the JSON array, no other explanations.
"""

    def _parse_llm_response(
        self,
        response: str,
        dialogue_ids: List[int]
    ) -> List[MemoryEntry]:
        """
        Parse LLM response to MemoryEntry list
        """
        # Extract JSON
        data = self.llm_client.extract_json(response)

        if not isinstance(data, list):
            raise ValueError(f"Expected JSON array but got: {type(data)}")

        entries = []
        for item in data:
            # Create MemoryEntry
            entry = MemoryEntry(
                lossless_restatement=item["lossless_restatement"],
                keywords=item.get("keywords", []),
                timestamp=item.get("timestamp"),
                location=item.get("location"),
                persons=item.get("persons", []),
                entities=item.get("entities", []),
                topic=item.get("topic")
            )
            entries.append(entry)

        return entries
    
    def _process_windows_parallel(self, windows: List[List[Dialogue]]):
        """
        Process multiple windows in parallel using ThreadPoolExecutor
        """
        all_entries = []
        
        # Use ThreadPoolExecutor for parallel processing
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_parallel_workers) as executor:
            # Submit all window processing tasks
            future_to_window = {}
            for i, window in enumerate(windows):
                dialogue_ids = [d.dialogue_id for d in window]
                future = executor.submit(self._generate_memory_entries_worker, window, dialogue_ids, i+1)
                future_to_window[future] = (window, i+1)
            
            # Collect results as they complete
            for future in concurrent.futures.as_completed(future_to_window):
                window, window_num = future_to_window[future]
                try:
                    entries = future.result()
                    all_entries.extend(entries)
                    print(f"[Parallel Processing] Window {window_num} completed: {len(entries)} entries")
                except Exception as e:
                    print(f"[Parallel Processing] Window {window_num} failed: {e}")
        
        # Store all entries to database in batch
        if all_entries:
            print(f"\n[Parallel Processing] Storing {len(all_entries)} entries to database...")
            self.vector_store.add_entries(all_entries)
            self.processed_count += sum(len(window) for window in windows)
            
            # Update previous entries (use last window's entries for context)
            if all_entries:
                self.previous_entries = all_entries[-10:]  # Keep last 10 entries for context
        
        print(f"[Parallel Processing] Completed processing {len(windows)} windows")
    
    def _generate_memory_entries_worker(self, window: List[Dialogue], dialogue_ids: List[int], window_num: int) -> List[MemoryEntry]:
        """
        Worker function for parallel processing of a single batch (full window or remaining dialogues)
        """
        batch_size = len(window)
        batch_type = "full window" if batch_size == self.window_size else f"remaining batch"
        print(f"[Worker {window_num}] Processing {batch_type} with {batch_size} dialogues")
        
        # Build dialogue text
        dialogue_text = "\n".join([str(d) for d in window])
        
        # Build context (shared across all workers - this is fine for parallel processing)
        context = ""
        if self.previous_entries:
            context = "\n[Previous Window Memory Entries (for reference to avoid duplication)]\n"
            for entry in self.previous_entries[:3]:  # Only show first 3
                context += f"- {entry.lossless_restatement}\n"

        # Build prompt
        prompt = self._build_extraction_prompt(dialogue_text, dialogue_ids, context)

        # Call LLM
        messages = [
            {
                "role": "system",
                "content": "You are a professional information extraction assistant, skilled at extracting structured, unambiguous information from conversations. You must output valid JSON format."
            },
            {
                "role": "user",
                "content": prompt
            }
        ]

        # Retry up to 3 times if parsing fails
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # Use JSON format if configured
                response_format = None
                if hasattr(config, 'USE_JSON_FORMAT') and config.USE_JSON_FORMAT:
                    response_format = {"type": "json_object"}

                response = self.llm_client.chat_completion(
                    messages,
                    temperature=0.1,
                    response_format=response_format
                )

                # Parse response
                entries = self._parse_llm_response(response, dialogue_ids)
                print(f"[Worker {window_num}] Generated {len(entries)} entries")
                return entries

            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"[Worker {window_num}] Attempt {attempt + 1}/{max_retries} failed: {e}. Retrying...")
                else:
                    print(f"[Worker {window_num}] All {max_retries} attempts failed: {e}")
                    return []



"""
Hybrid Retriever - Stage 3: Intent-Aware Retrieval Planning (Section 3.3)

Implements:
- Retrieval planning P(q, H) → {q_sem, q_lex, q_sym, d}
- Parallel multi-view retrieval across Semantic, Lexical, Symbolic layers
- Result merging: C_q = R_sem ∪ R_lex ∪ R_sym
"""
from typing import List, Optional, Dict, Any
from models.memory_entry import MemoryEntry
from utils.llm_client import LLMClient
from database.vector_store import VectorStore
import config
import re
from datetime import datetime, timedelta
import dateparser
import concurrent.futures


class HybridRetriever:
    """
    Hybrid Retriever - Intent-Aware Retrieval Planning (Section 3.3)

    Core Components:
    1. Retrieval planning: infers search intent and generates optimized queries
    2. Parallel multi-view retrieval:
       - Semantic: R_sem = Top-n(cos(E(q_sem), E(m_i)))
       - Lexical: R_lex = Top-n(BM25(q_lex, m_i))
       - Symbolic: R_sym = Top-n({m_i | Meta(m_i) ⊨ q_sym})
    3. Result merging: C_q = R_sem ∪ R_lex ∪ R_sym
    """
    def __init__(
        self,
        llm_client: LLMClient,
        vector_store: VectorStore,
        semantic_top_k: int = None,
        keyword_top_k: int = None,
        structured_top_k: int = None,
        enable_planning: bool = True,
        enable_reflection: bool = True,
        max_reflection_rounds: int = 2,
        enable_parallel_retrieval: bool = True,
        max_retrieval_workers: int = 3
    ):
        self.llm_client = llm_client
        self.vector_store = vector_store
        self.semantic_top_k = semantic_top_k or config.SEMANTIC_TOP_K
        self.keyword_top_k = keyword_top_k or config.KEYWORD_TOP_K
        self.structured_top_k = structured_top_k or config.STRUCTURED_TOP_K
        
        # Use config values as default if not explicitly provided
        self.enable_planning = enable_planning if enable_planning is not None else getattr(config, 'ENABLE_PLANNING', True)
        self.enable_reflection = enable_reflection if enable_reflection is not None else getattr(config, 'ENABLE_REFLECTION', True)
        self.max_reflection_rounds = max_reflection_rounds if max_reflection_rounds is not None else getattr(config, 'MAX_REFLECTION_ROUNDS', 2)
        self.enable_parallel_retrieval = enable_parallel_retrieval if enable_parallel_retrieval is not None else getattr(config, 'ENABLE_PARALLEL_RETRIEVAL', True)
        self.max_retrieval_workers = max_retrieval_workers if max_retrieval_workers is not None else getattr(config, 'MAX_RETRIEVAL_WORKERS', 3)

    def retrieve(self, query: str, enable_reflection: Optional[bool] = None) -> List[MemoryEntry]:
        """
        Execute retrieval with planning and optional reflection

        Args:
        - query: Search query
        - enable_reflection: Override the global reflection setting for this query
                           (useful for adversarial questions that shouldn't use reflection)

        Returns: List of relevant MemoryEntry
        """
        if self.enable_planning:
            return self._retrieve_with_planning(query, enable_reflection)
        else:
            # Fallback to simple semantic search
            return self._semantic_search(query)
    
    def _retrieve_with_planning(self, query: str, enable_reflection: Optional[bool] = None) -> List[MemoryEntry]:
        """
        Execute retrieval with intelligent planning process
        
        Args:
        - query: Search query  
        - enable_reflection: Override reflection setting for this query
        """
        print(f"\n[Planning] Analyzing information requirements for: {query}")
        
        # Step 1: Intelligent analysis of what information is needed
        information_plan = self._analyze_information_requirements(query)
        print(f"[Planning] Identified {len(information_plan['required_info'])} information requirements")
        
        # Step 2: Generate minimal necessary queries based on the plan
        search_queries = self._generate_targeted_queries(query, information_plan)
        print(f"[Planning] Generated {len(search_queries)} targeted queries")
        
        # Step 3: Execute searches for all queries (parallel or sequential)
        if self.enable_parallel_retrieval and len(search_queries) > 1:
            all_results = self._execute_parallel_searches(search_queries)
        else:
            all_results = []
            for i, search_query in enumerate(search_queries, 1):
                print(f"[Search {i}] {search_query}")
                results = self._semantic_search(search_query)
                all_results.extend(results)

        # Step 3.5: Execute keyword and structured searches (hybrid retrieval)
        query_analysis = self._analyze_query(query)

        # Keyword search (Lexical Layer)
        keyword_results = self._keyword_search(query, query_analysis)
        print(f"[Keyword Search] Found {len(keyword_results)} results")
        all_results.extend(keyword_results)

        # Structured search (Symbolic Layer)
        structured_results = self._structured_search(query_analysis)
        print(f"[Structured Search] Found {len(structured_results)} results")
        all_results.extend(structured_results)

        # Step 4: Merge and deduplicate results
        merged_results = self._merge_and_deduplicate_entries(all_results)
        print(f"[Planning] Found {len(merged_results)} unique results (semantic + keyword + structured)")
        
        # Step 5: Optional reflection-based additional retrieval
        # Use override parameter if provided, otherwise use global setting
        should_use_reflection = enable_reflection if enable_reflection is not None else self.enable_reflection
        
        if should_use_reflection:
            merged_results = self._retrieve_with_intelligent_reflection(query, merged_results, information_plan)
        
        return merged_results
    
    def _retrieve_with_reflection(self, query: str, initial_results: List[MemoryEntry]) -> List[MemoryEntry]:
        """
        Execute reflection-based additional retrieval
        """
        current_results = initial_results
        
        for round_num in range(self.max_reflection_rounds):
            print(f"\n[Reflection Round {round_num + 1}] Checking if results are sufficient...")
            
            # Quick answer attempt with current results
            if not current_results:
                answer_status = "no_results"
            else:
                answer_status = self._check_answer_adequacy(query, current_results)
            
            if answer_status == "sufficient":
                print(f"[Reflection Round {round_num + 1}] Information is sufficient")
                break
            elif answer_status == "insufficient":
                print(f"[Reflection Round {round_num + 1}] Information is insufficient, generating additional queries...")
                
                # Generate additional targeted queries based on what's missing
                additional_queries = self._generate_additional_queries(query, current_results)
                print(f"[Reflection Round {round_num + 1}] Generated {len(additional_queries)} additional queries")
                
                # Execute additional searches (parallel or sequential)
                if self.enable_parallel_retrieval and len(additional_queries) > 1:
                    print(f"[Reflection Round {round_num + 1}] Executing {len(additional_queries)} additional queries in parallel")
                    additional_results = self._execute_parallel_additional_searches(additional_queries, round_num + 1)
                else:
                    additional_results = []
                    for i, add_query in enumerate(additional_queries, 1):
                        print(f"[Additional Search {i}] {add_query}")
                        results = self._semantic_search(add_query)
                        additional_results.extend(results)
                
                # Merge with existing results
                all_results = current_results + additional_results
                current_results = self._merge_and_deduplicate_entries(all_results)
                print(f"[Reflection Round {round_num + 1}] Total results: {len(current_results)}")
                
            else:  # "no_results"
                print(f"[Reflection Round {round_num + 1}] No results found, cannot continue reflection")
                break
        
        return current_results

    def _analyze_query(self, query: str) -> Dict[str, Any]:
        """
        Use LLM to analyze query intent and extract structured information
        """
        prompt = f"""
Analyze the following query and extract key information:

Query: {query}

Please extract:
1. keywords: List of keywords (names, places, topic words, etc.)
2. persons: Person names mentioned
3. time_expression: Time expression (if any)
4. location: Location (if any)
5. entities: Entities (companies, products, etc.)

Return in JSON format:
```json
{{
  "keywords": ["keyword1", "keyword2", ...],
  "persons": ["name1", "name2", ...],
  "time_expression": "time expression or null",
  "location": "location or null",
  "entities": ["entity1", ...]
}}
```

Return ONLY JSON, no other content.
"""

        messages = [
            {"role": "system", "content": "You are a query analysis assistant. You must output valid JSON format."},
            {"role": "user", "content": prompt}
        ]

        # Retry up to 3 times
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # Use JSON format if configured
                response_format = None
                if hasattr(config, 'USE_JSON_FORMAT') and config.USE_JSON_FORMAT:
                    response_format = {"type": "json_object"}

                response = self.llm_client.chat_completion(
                    messages,
                    temperature=0.1,
                    response_format=response_format
                )
                analysis = self.llm_client.extract_json(response)
                return analysis
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"Query analysis attempt {attempt + 1}/{max_retries} failed: {e}. Retrying...")
                else:
                    print(f"Query analysis failed after {max_retries} attempts: {e}")
                    # Return default values
                    return {
                        "keywords": [query],
                        "persons": [],
                        "time_expression": None,
                        "location": None,
                        "entities": []
                    }

    def _semantic_search(self, query: str) -> List[MemoryEntry]:
        """
        Semantic Layer Retrieval (Section 3.3)
        R_sem = Top-n(cos(E(q_sem), E(m_i)))
        """
        return self.vector_store.semantic_search(query, top_k=self.semantic_top_k)

    def _keyword_search(
        self,
        query: str,
        query_analysis: Dict[str, Any]
    ) -> List[MemoryEntry]:
        """
        Lexical Layer Retrieval (Section 3.3)
        R_lex = Top-n(BM25(q_lex, m_i))
        """
        keywords = query_analysis.get("keywords", [])
        if not keywords:
            # If no keywords extracted, use query itself
            keywords = [query]

        return self.vector_store.keyword_search(keywords, top_k=self.keyword_top_k)

    def _structured_search(self, query_analysis: Dict[str, Any]) -> List[MemoryEntry]:
        """
        Symbolic Layer Retrieval (Section 3.3)
        R_sym = Top-n({m_i | Meta(m_i) ⊨ q_sym})
        """
        persons = query_analysis.get("persons", [])
        location = query_analysis.get("location")
        entities = query_analysis.get("entities", [])
        time_expression = query_analysis.get("time_expression")

        # Parse time range
        timestamp_range = None
        if time_expression:
            timestamp_range = self._parse_time_range(time_expression)

        # Return empty if no structured conditions
        if not any([persons, location, entities, timestamp_range]):
            return []

        # Execute structured search
        return self.vector_store.structured_search(
            persons=persons if persons else None,
            location=location,
            entities=entities if entities else None,
            timestamp_range=timestamp_range,
            top_k=self.structured_top_k
        )

    def _parse_time_range(self, time_expression: str) -> Optional[tuple]:
        """
        Parse time expression to time range

        Examples:
        - "last week" -> (last Monday 00:00, last Sunday 23:59)
        - "November 15" -> (2025-11-15 00:00, 2025-11-15 23:59)
        """
        try:
            # Use dateparser to parse
            parsed_date = dateparser.parse(
                time_expression,
                settings={'PREFER_DATES_FROM': 'past'}
            )

            if parsed_date:
                # Generate time range (for the day)
                start_time = parsed_date.replace(hour=0, minute=0, second=0)
                end_time = parsed_date.replace(hour=23, minute=59, second=59)

                # Expand range for weekly expressions
                if "week" in time_expression.lower() or "周" in time_expression:
                    start_time = start_time - timedelta(days=7)
                    end_time = end_time + timedelta(days=7)

                return (
                    start_time.isoformat(),
                    end_time.isoformat()
                )
        except Exception as e:
            print(f"Time parsing failed: {e}")

        return None

    def _merge_and_deduplicate(
        self,
        results: Dict[str, List[MemoryEntry]]
    ) -> List[MemoryEntry]:
        """
        Merge multi-path retrieval results and deduplicate
        """
        seen_ids = set()
        merged = []

        # Merge by priority (structured > semantic > keyword)
        for source in ['structured', 'semantic', 'keyword']:
            for entry in results.get(source, []):
                if entry.entry_id not in seen_ids:
                    seen_ids.add(entry.entry_id)
                    merged.append(entry)

        return merged
    
    def _generate_search_queries(self, query: str) -> List[str]:
        """
        Generate multiple search queries for comprehensive retrieval
        """
        prompt = f"""
You are helping with information retrieval. Given a user question, generate multiple search queries that would help find comprehensive information to answer the question.

Original Question: {query}

Please generate 3-5 different search queries that cover various aspects and angles of this question. Each query should be focused and specific.

Guidelines:
1. Include the original question as one query
2. Break down complex questions into component parts
3. Consider synonyms and alternative phrasings
4. Think about related concepts that might be relevant
5. Consider temporal, spatial, or contextual variations

Return your response in JSON format:
```json
{{
  "queries": [
    "search query 1",
    "search query 2", 
    "search query 3",
    ...
  ]
}}
```

Return ONLY the JSON, no other text.
"""
        
        messages = [
            {"role": "system", "content": "You are a search query generation assistant. You must output valid JSON format."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            # Use JSON format if configured
            response_format = None
            if hasattr(config, 'USE_JSON_FORMAT') and config.USE_JSON_FORMAT:
                response_format = {"type": "json_object"}
                
            response = self.llm_client.chat_completion(
                messages,
                temperature=0.3,
                response_format=response_format
            )
            
            result = self.llm_client.extract_json(response)
            queries = result.get("queries", [query])
            
            # Ensure original query is included
            if query not in queries:
                queries.insert(0, query)
                
            return queries
            
        except Exception as e:
            print(f"Failed to generate search queries: {e}")
            # Fallback to original query
            return [query]
    
    def _merge_and_deduplicate_entries(self, entries: List[MemoryEntry]) -> List[MemoryEntry]:
        """
        Merge and deduplicate memory entries by entry_id
        """
        seen_ids = set()
        merged = []
        
        for entry in entries:
            if entry.entry_id not in seen_ids:
                seen_ids.add(entry.entry_id)
                merged.append(entry)
        
        return merged
    
    def _check_answer_adequacy(self, query: str, contexts: List[MemoryEntry]) -> str:
        """
        Check if current contexts are sufficient to answer the query
        Returns: "sufficient", "insufficient", or "no_results"
        """
        if not contexts:
            return "no_results"
        
        # Format contexts
        context_str = self._format_contexts_for_check(contexts)
        
        prompt = f"""
You are evaluating whether the provided context contains sufficient information to answer a user question.

Question: {query}

Context:
{context_str}

Please evaluate whether the context contains enough information to provide a meaningful, accurate answer to the question.

Consider these criteria:
1. Does the context directly address the question being asked?
2. Are there key details necessary to answer the question?
3. Is the information specific enough to avoid vague responses?

Return your evaluation in JSON format:
```json
{{
  "assessment": "sufficient" OR "insufficient",
  "reasoning": "Brief explanation of why the context is or isn't sufficient",
  "missing_info": ["list", "of", "missing", "information"] (only if insufficient)
}}
```

Return ONLY the JSON, no other text.
"""
        
        messages = [
            {"role": "system", "content": "You are an information adequacy evaluator. You must output valid JSON format."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            # Use JSON format if configured
            response_format = None
            if hasattr(config, 'USE_JSON_FORMAT') and config.USE_JSON_FORMAT:
                response_format = {"type": "json_object"}
                
            response = self.llm_client.chat_completion(
                messages,
                temperature=0.1,
                response_format=response_format
            )
            
            result = self.llm_client.extract_json(response)
            return result.get("assessment", "insufficient")
            
        except Exception as e:
            print(f"Failed to check answer adequacy: {e}")
            # Default to insufficient to be safe
            return "insufficient"
    
    def _generate_additional_queries(self, original_query: str, current_contexts: List[MemoryEntry]) -> List[str]:
        """
        Generate additional targeted queries based on what's missing
        """
        context_str = self._format_contexts_for_check(current_contexts)
        
        prompt = f"""
Based on the original question and current available information, generate additional specific search queries that would help find the missing information needed to answer the question completely.

Original Question: {original_query}

Current Available Information:
{context_str}

Analyze what specific information is still missing and generate 2-4 targeted search queries that would help find this missing information.

The queries should be:
1. Specific and focused on the missing information
2. Different from the original question
3. Likely to find complementary information

Return your response in JSON format:
```json
{{
  "missing_analysis": "Brief analysis of what's missing",
  "additional_queries": [
    "specific search query 1",
    "specific search query 2",
    ...
  ]
}}
```

Return ONLY the JSON, no other text.
"""
        
        messages = [
            {"role": "system", "content": "You are a search strategy assistant. You must output valid JSON format."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            # Use JSON format if configured
            response_format = None
            if hasattr(config, 'USE_JSON_FORMAT') and config.USE_JSON_FORMAT:
                response_format = {"type": "json_object"}
                
            response = self.llm_client.chat_completion(
                messages,
                temperature=0.3,
                response_format=response_format
            )
            
            result = self.llm_client.extract_json(response)
            return result.get("additional_queries", [])
            
        except Exception as e:
            print(f"Failed to generate additional queries: {e}")
            return []
    
    def _format_contexts_for_check(self, contexts: List[MemoryEntry]) -> str:
        """
        Format contexts for adequacy checking (more concise than full format)
        """
        formatted = []
        for i, entry in enumerate(contexts, 1):
            parts = [f"[Info {i}] {entry.lossless_restatement}"]
            if entry.timestamp:
                parts.append(f"Time: {entry.timestamp}")
            formatted.append(" | ".join(parts))
        
        return "\n".join(formatted)
    
    def _execute_parallel_searches(self, search_queries: List[str]) -> List[MemoryEntry]:
        """
        Execute multiple search queries in parallel using ThreadPoolExecutor
        """
        print(f"[Parallel Search] Executing {len(search_queries)} queries in parallel with {self.max_retrieval_workers} workers")
        all_results = []
        
        try:
            # Use ThreadPoolExecutor for parallel retrieval
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_retrieval_workers) as executor:
                # Submit all search tasks
                future_to_query = {}
                for i, query in enumerate(search_queries, 1):
                    future = executor.submit(self._semantic_search_worker, query, i)
                    future_to_query[future] = (query, i)
                
                # Collect results as they complete
                for future in concurrent.futures.as_completed(future_to_query):
                    query, query_num = future_to_query[future]
                    try:
                        results = future.result()
                        all_results.extend(results)
                        print(f"[Parallel Search] Query {query_num} completed: {len(results)} results")
                    except Exception as e:
                        print(f"[Parallel Search] Query {query_num} failed: {e}")
                        
        except Exception as e:
            print(f"[Parallel Search] Parallel execution failed: {e}. Falling back to sequential search...")
            # Fallback to sequential processing
            for i, query in enumerate(search_queries, 1):
                try:
                    print(f"[Sequential Search {i}] {query}")
                    results = self._semantic_search(query)
                    all_results.extend(results)
                except Exception as search_e:
                    print(f"[Sequential Search {i}] Failed: {search_e}")
        
        return all_results
    
    def _semantic_search_worker(self, query: str, query_num: int) -> List[MemoryEntry]:
        """
        Worker function for parallel semantic search
        """
        print(f"[Search {query_num}] {query}")
        return self._semantic_search(query)
    
    def _execute_parallel_additional_searches(self, additional_queries: List[str], round_num: int) -> List[MemoryEntry]:
        """
        Execute additional reflection queries in parallel
        """
        all_results = []
        
        try:
            # Use ThreadPoolExecutor for parallel retrieval
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_retrieval_workers) as executor:
                # Submit all search tasks
                future_to_query = {}
                for i, query in enumerate(additional_queries, 1):
                    future = executor.submit(self._additional_search_worker, query, i, round_num)
                    future_to_query[future] = (query, i)
                
                # Collect results as they complete
                for future in concurrent.futures.as_completed(future_to_query):
                    query, query_num = future_to_query[future]
                    try:
                        results = future.result()
                        all_results.extend(results)
                        print(f"[Reflection Round {round_num}] Additional query {query_num} completed: {len(results)} results")
                    except Exception as e:
                        print(f"[Reflection Round {round_num}] Additional query {query_num} failed: {e}")
                        
        except Exception as e:
            print(f"[Reflection Round {round_num}] Parallel execution failed: {e}. Falling back to sequential search...")
            # Fallback to sequential processing
            for i, query in enumerate(additional_queries, 1):
                try:
                    print(f"[Additional Search {i}] {query}")
                    results = self._semantic_search(query)
                    all_results.extend(results)
                except Exception as search_e:
                    print(f"[Additional Search {i}] Failed: {search_e}")
        
        return all_results
    
    def _additional_search_worker(self, query: str, query_num: int, round_num: int) -> List[MemoryEntry]:
        """
        Worker function for parallel additional search in reflection
        """
        print(f"[Additional Search {query_num}] {query}")
        return self._semantic_search(query)
    
    def _analyze_information_requirements(self, query: str) -> Dict[str, Any]:
        """
        Retrieval Planning (Section 3.3)
        Analyzes query to determine information requirements and retrieval depth d
        """
        prompt = f"""
Analyze the following question and determine what specific information is required to answer it comprehensively.

Question: {query}

Think step by step:
1. What type of question is this? (factual, temporal, relational, explanatory, etc.)
2. What key entities, events, or concepts need to be identified?
3. What relationships or connections need to be established?
4. What minimal set of information pieces would be sufficient to answer this question?

Return your analysis in JSON format:
```json
{{
  "question_type": "type of question",
  "key_entities": ["entity1", "entity2", ...],
  "required_info": [
    {{
      "info_type": "what kind of information",
      "description": "specific information needed",
      "priority": "high/medium/low"
    }}
  ],
  "relationships": ["relationship1", "relationship2", ...],
  "minimal_queries_needed": 2
}}
```

Focus on identifying the minimal essential information needed, not exhaustive details.

Return ONLY the JSON, no other text.
"""
        
        messages = [
            {"role": "system", "content": "You are an intelligent information requirement analyst. You must output valid JSON format."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            # Use JSON format if configured
            response_format = None
            if hasattr(config, 'USE_JSON_FORMAT') and config.USE_JSON_FORMAT:
                response_format = {"type": "json_object"}
                
            response = self.llm_client.chat_completion(
                messages,
                temperature=0.2,
                response_format=response_format
            )
            
            result = self.llm_client.extract_json(response)
            return result
            
        except Exception as e:
            print(f"Failed to analyze information requirements: {e}")
            # Fallback to simple analysis
            return {
                "question_type": "general",
                "key_entities": [query],
                "required_info": [{"info_type": "general", "description": "relevant information", "priority": "high"}],
                "relationships": [],
                "minimal_queries_needed": 1
            }
    
    def _generate_targeted_queries(self, original_query: str, information_plan: Dict[str, Any]) -> List[str]:
        """
        Generate minimal targeted queries based on information requirements analysis
        """
        prompt = f"""
Based on the information requirements analysis, generate the minimal set of targeted search queries needed to gather the required information.

Original Question: {original_query}

Information Requirements Analysis:
- Question Type: {information_plan.get('question_type', 'general')}
- Key Entities: {information_plan.get('key_entities', [])}
- Required Information: {information_plan.get('required_info', [])}
- Relationships: {information_plan.get('relationships', [])}
- Minimal Queries Needed: {information_plan.get('minimal_queries_needed', 1)}

Generate the minimal set of search queries that would efficiently gather all the required information. Each query should be focused and specific to retrieve distinct types of information.

Guidelines:
1. Always include the original query as one option
2. Generate only the minimal necessary queries (usually 1-3)
3. Each query should target a specific information requirement
4. Avoid redundant or overlapping queries
5. Focus on efficiency - fewer, more targeted queries are better

Return your response in JSON format:
```json
{{
  "reasoning": "Brief explanation of the query strategy",
  "queries": [
    "targeted query 1",
    "targeted query 2",
    ...
  ]
}}
```

Return ONLY the JSON, no other text.
"""
        
        messages = [
            {"role": "system", "content": "You are a query generation specialist. You must output valid JSON format."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            # Use JSON format if configured
            response_format = None
            if hasattr(config, 'USE_JSON_FORMAT') and config.USE_JSON_FORMAT:
                response_format = {"type": "json_object"}
                
            response = self.llm_client.chat_completion(
                messages,
                temperature=0.3,
                response_format=response_format
            )
            
            result = self.llm_client.extract_json(response)
            queries = result.get("queries", [original_query])
            
            # Ensure original query is included and limit to reasonable number
            if original_query not in queries:
                queries.insert(0, original_query)
            
            # Limit to max 4 queries for efficiency
            queries = queries[:4]
            
            print(f"[Planning] Strategy: {result.get('reasoning', 'Generate targeted queries')}")
            return queries
            
        except Exception as e:
            print(f"Failed to generate targeted queries: {e}")
            # Fallback to original query
            return [original_query]
    
    def _retrieve_with_intelligent_reflection(self, query: str, initial_results: List[MemoryEntry], information_plan: Dict[str, Any]) -> List[MemoryEntry]:
        """
        Execute intelligent reflection-based additional retrieval
        """
        current_results = initial_results
        
        for round_num in range(self.max_reflection_rounds):
            print(f"\n[Intelligent Reflection Round {round_num + 1}] Analyzing information completeness...")
            
            # Intelligent analysis of information completeness
            if not current_results:
                completeness_status = "no_results"
            else:
                completeness_status = self._analyze_information_completeness(query, current_results, information_plan)
            
            if completeness_status == "complete":
                print(f"[Intelligent Reflection Round {round_num + 1}] Information is complete")
                break
            elif completeness_status == "incomplete":
                print(f"[Intelligent Reflection Round {round_num + 1}] Information is incomplete, generating targeted additional queries...")
                
                # Generate targeted additional queries based on what's missing
                additional_queries = self._generate_missing_info_queries(query, current_results, information_plan)
                print(f"[Intelligent Reflection Round {round_num + 1}] Generated {len(additional_queries)} targeted queries")
                
                # Execute additional searches
                if self.enable_parallel_retrieval and len(additional_queries) > 1:
                    print(f"[Intelligent Reflection Round {round_num + 1}] Executing {len(additional_queries)} queries in parallel")
                    additional_results = self._execute_parallel_additional_searches(additional_queries, round_num + 1)
                else:
                    additional_results = []
                    for i, add_query in enumerate(additional_queries, 1):
                        print(f"[Additional Search {i}] {add_query}")
                        results = self._semantic_search(add_query)
                        additional_results.extend(results)
                
                # Merge with existing results
                all_results = current_results + additional_results
                current_results = self._merge_and_deduplicate_entries(all_results)
                print(f"[Intelligent Reflection Round {round_num + 1}] Total results: {len(current_results)}")
                
            else:  # "no_results"
                print(f"[Intelligent Reflection Round {round_num + 1}] No results found, cannot continue reflection")
                break
        
        return current_results
    
    def _analyze_information_completeness(self, query: str, current_results: List[MemoryEntry], information_plan: Dict[str, Any]) -> str:
        """
        Analyze if current results provide complete information to answer the query
        """
        if not current_results:
            return "no_results"
        
        context_str = self._format_contexts_for_check(current_results)
        required_info = information_plan.get('required_info', [])
        
        prompt = f"""
Analyze whether the provided information is sufficient to completely answer the original question, based on the identified information requirements.

Original Question: {query}

Required Information Types: {required_info}

Current Available Information:
{context_str}

Evaluate whether:
1. All required information types are addressed
2. The information is complete enough to provide a comprehensive answer
3. Any critical gaps remain that would prevent a satisfactory answer

Return your evaluation in JSON format:
```json
{{
  "assessment": "complete" OR "incomplete",
  "reasoning": "Brief explanation of completeness assessment",
  "missing_info_types": ["list", "of", "missing", "information", "types"],
  "coverage_percentage": 85
}}
```

Return ONLY the JSON, no other text.
"""
        
        messages = [
            {"role": "system", "content": "You are an information completeness evaluator. You must output valid JSON format."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            # Use JSON format if configured
            response_format = None
            if hasattr(config, 'USE_JSON_FORMAT') and config.USE_JSON_FORMAT:
                response_format = {"type": "json_object"}
                
            response = self.llm_client.chat_completion(
                messages,
                temperature=0.1,
                response_format=response_format
            )
            
            result = self.llm_client.extract_json(response)
            assessment = result.get("assessment", "incomplete")
            coverage = result.get("coverage_percentage", 0)
            
            print(f"[Intelligent Reflection] Coverage: {coverage}% - {result.get('reasoning', '')}")
            return assessment
            
        except Exception as e:
            print(f"Failed to analyze information completeness: {e}")
            return "incomplete"
    
    def _generate_missing_info_queries(self, original_query: str, current_results: List[MemoryEntry], information_plan: Dict[str, Any]) -> List[str]:
        """
        Generate targeted queries to find missing information
        """
        context_str = self._format_contexts_for_check(current_results)
        required_info = information_plan.get('required_info', [])
        
        prompt = f"""
Based on the original question, required information types, and currently available information, generate targeted search queries to find the missing information needed to answer the question completely.

Original Question: {original_query}

Required Information Types: {required_info}

Currently Available Information:
{context_str}

Generate 1-3 specific search queries that would help find the missing information. Focus on:
1. Information gaps identified in the current context
2. Specific missing details needed to answer the original question
3. Different search angles that might retrieve the missing information

Return your response in JSON format:
```json
{{
  "missing_analysis": "Brief analysis of what specific information is missing",
  "targeted_queries": [
    "specific query 1 for missing info",
    "specific query 2 for missing info",
    ...
  ]
}}
```

Return ONLY the JSON, no other text.
"""
        
        messages = [
            {"role": "system", "content": "You are a missing information query generator. You must output valid JSON format."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            # Use JSON format if configured
            response_format = None
            if hasattr(config, 'USE_JSON_FORMAT') and config.USE_JSON_FORMAT:
                response_format = {"type": "json_object"}
                
            response = self.llm_client.chat_completion(
                messages,
                temperature=0.3,
                response_format=response_format
            )
            
            result = self.llm_client.extract_json(response)
            queries = result.get("targeted_queries", [])
            
            print(f"[Intelligent Reflection] Missing info: {result.get('missing_analysis', 'Unknown')}")
            return queries
            
        except Exception as e:
            print(f"Failed to generate missing info queries: {e}")
            return []



"""
Answer Generator - Final synthesis from retrieved contexts

Section 3.3: Intent-Aware Retrieval Planning
Generates answers from the merged context C_q after multi-view retrieval
"""
from typing import List
from models.memory_entry import MemoryEntry
from utils.llm_client import LLMClient
import config


class AnswerGenerator:
    """
    Answer Generator - Synthesis from retrieved memory units (Section 3.3)

    Generates answers from C_q = R_sem ∪ R_lex ∪ R_sym
    """
    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client

    def generate_answer(self, query: str, contexts: List[MemoryEntry]) -> str:
        """
        Generate answer

        Args:
        - query: User question
        - contexts: List of retrieved relevant MemoryEntry

        Returns:
        - Generated answer (concise phrase)
        """
        if not contexts:
            return "No relevant information found"

        # Build context string
        context_str = self._format_contexts(contexts)

        # Build prompt
        prompt = self._build_answer_prompt(query, context_str)

        # Call LLM to generate answer
        messages = [
            {
                "role": "system",
                "content": "You are a professional Q&A assistant. Extract concise answers from context. You must output valid JSON format."
            },
            {
                "role": "user",
                "content": prompt
            }
        ]

        # Retry up to 3 times
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # Use JSON format if configured
                response_format = None
                if hasattr(config, 'USE_JSON_FORMAT') and config.USE_JSON_FORMAT:
                    response_format = {"type": "json_object"}

                response = self.llm_client.chat_completion(
                    messages,
                    temperature=0.1,
                    response_format=response_format
                )

                # Parse JSON response
                result = self.llm_client.extract_json(response)
                # Return the answer from JSON
                return result.get("answer", response.strip())

            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"Answer generation attempt {attempt + 1}/{max_retries} failed: {e}. Retrying...")
                else:
                    print(f"Warning: Failed to parse JSON response after {max_retries} attempts: {e}")
                    # Fallback to raw response
                    if 'response' in locals():
                        return response.strip()
                    else:
                        return "Failed to generate answer"

    def _format_contexts(self, contexts: List[MemoryEntry]) -> str:
        """
        Format contexts to readable text
        """
        formatted = []
        for i, entry in enumerate(contexts, 1):
            parts = [f"[Context {i}]"]
            parts.append(f"Content: {entry.lossless_restatement}")

            if entry.timestamp:
                parts.append(f"Time: {entry.timestamp}")

            if entry.location:
                parts.append(f"Location: {entry.location}")

            if entry.persons:
                parts.append(f"Persons: {', '.join(entry.persons)}")

            if entry.entities:
                parts.append(f"Related Entities: {', '.join(entry.entities)}")

            if entry.topic:
                parts.append(f"Topic: {entry.topic}")

            formatted.append("\n".join(parts))

        return "\n\n".join(formatted)

    def _build_answer_prompt(self, query: str, context_str: str) -> str:
        """
        Build answer generation prompt
        """
        return f"""
Answer the user's question based on the provided context.

User Question: {query}

Relevant Context:
{context_str}

Requirements:
1. First, think through the reasoning process
2. Then provide a very CONCISE answer (short phrase about core information)
3. Answer must be based ONLY on the provided context
4. All dates in the response must be formatted as 'DD Month YYYY' but you can output more or less details if needed
5. Return your response in JSON format

Output Format:
```json
{{
  "reasoning": "Brief explanation of your thought process",
  "answer": "Concise answer in a short phrase"
}}
```

Example:
Question: "When will they meet?"
Context: "Alice suggested meeting Bob at 2025-11-16T14:00:00..."

Output:
```json
{{
  "reasoning": "The context explicitly states the meeting time as 2025-11-16T14:00:00",
  "answer": "16 November 2025 at 2:00 PM"
}}
```

Now answer the question. Return ONLY the JSON, no other text.
"""




---


##### Microsoft Teachable Agent#######
Moj коментар:Наједноставен но можат да се извлечат поуки.

from typing import TYPE_CHECKING, Any

from autogen_core import CancellationToken, Image
from autogen_core.memory import Memory, MemoryContent, MemoryMimeType, MemoryQueryResult, UpdateContextResult
from autogen_core.model_context import ChatCompletionContext
from autogen_core.models import UserMessage

if TYPE_CHECKING:
    from autogen_ext.experimental.task_centric_memory import MemoryController


class Teachability(Memory):
    """
    Gives an AssistantAgent the ability to learn quickly from user teachings, hints, and advice.

    Steps for usage:

        1. Instantiate MemoryController.
        2. Instantiate Teachability, passing the memory controller as a parameter.
        3. Instantiate an AssistantAgent, passing the teachability instance (wrapped in a list) as the memory parameter.
        4. Use the AssistantAgent as usual, such as for chatting with the user.
    """

    def __init__(self, memory_controller: "MemoryController", name: str | None = None) -> None:
        """Initialize Teachability."""
        self._memory_controller = memory_controller
        self._logger = memory_controller.logger
        self._name = name or "teachability"

    @property
    def name(self) -> str:
        """Get the memory instance identifier."""
        return self._name

    def _extract_text(self, content_item: str | MemoryContent) -> str:
        """Extract searchable text from content."""
        if isinstance(content_item, str):
            return content_item

        content = content_item.content
        mime_type = content_item.mime_type

        if mime_type in [MemoryMimeType.TEXT, MemoryMimeType.MARKDOWN]:
            return str(content)
        elif mime_type == MemoryMimeType.JSON:
            if isinstance(content, dict):
                # Store original JSON string representation
                return str(content).lower()
            raise ValueError("JSON content must be a dict")
        elif isinstance(content, Image):
            raise ValueError("Image content cannot be converted to text")
        else:
            raise ValueError(f"Unsupported content type: {mime_type}")

    async def update_context(
        self,
        model_context: ChatCompletionContext,
    ) -> UpdateContextResult:
        """
        Extracts any advice from the last user turn to be stored in memory,
        and adds any relevant memories to the model context.
        """
        self._logger.enter_function()

        # Extract text from the user's last message
        messages = await model_context.get_messages()
        if not messages:
            self._logger.leave_function()
            return UpdateContextResult(memories=MemoryQueryResult(results=[]))
        last_message = messages[-1]
        last_user_text = last_message.content if isinstance(last_message.content, str) else str(last_message)

        # Add any relevant memories to the chat history
        query_results = await self.query(last_user_text)
        if query_results.results:
            memory_strings = [f"{i}. {str(memory.content)}" for i, memory in enumerate(query_results.results, 1)]
            memory_context = "\nPotentially relevant memories:\n" + "\n".join(memory_strings)
            await model_context.add_message(UserMessage(content=memory_context, source="user"))

        # Add any user advice to memory
        await self._memory_controller.consider_memo_storage(last_user_text)

        self._logger.leave_function()
        return UpdateContextResult(memories=query_results)

    async def add(self, content: MemoryContent, cancellation_token: CancellationToken | None = None) -> None:
        """
        Tries to extract any advice from the passed content and add it to memory.
        """
        self._logger.enter_function()

        # Extract text from the incoming content
        text = self._extract_text(content)

        # Check for advice to add to memory for later turns.
        await self._memory_controller.consider_memo_storage(text)

        self._logger.leave_function()

    async def query(
        self,
        query: str | MemoryContent,
        cancellation_token: CancellationToken | None = None,
        **kwargs: Any,
    ) -> MemoryQueryResult:
        """
        Returns any memories that seem relevant to the query.
        """
        self._logger.enter_function()

        task = self._extract_text(query)
        memory_results: list[MemoryContent] = []
        filtered_memos = await self._memory_controller.retrieve_relevant_memos(task=task)
        filtered_insights = [memo.insight for memo in filtered_memos]
        for insight in filtered_insights:
            self._logger.info(f"Insight: {insight}")
            memory_content = MemoryContent(
                content=insight,
                mime_type="MemoryMimeType.TEXT",
                metadata={},
            )
            memory_results.append(memory_content)

        self._logger.leave_function()
        return MemoryQueryResult(results=memory_results)

    async def clear(self) -> None:
        """Clear all entries from memory."""
        self._memory_controller.reset_memory()

    async def close(self) -> None:
        """Clean up memory resources."""
        pass  # No cleanup needed for this memory implementation

from __future__ import annotations

from typing import TYPE_CHECKING, List, Tuple, Union

from autogen_core import Image
from autogen_core.models import (
    AssistantMessage,
    ChatCompletionClient,
    CreateResult,
    LLMMessage,
    SystemMessage,
    UserMessage,
)

from ._functions import UserContent
from .page_logger import PageLogger

if TYPE_CHECKING:
    from .apprentice import Apprentice


class Grader:
    """
    Runs basic tests, and determines task success without limitation to string matches.

    Args:
        client: The client to call the model.
        logger: An optional logger. If None, no logging will be performed.
    """

    def __init__(self, client: ChatCompletionClient, logger: PageLogger | None = None) -> None:
        if logger is None:
            logger = PageLogger()  # Nothing will be logged by this object.
        self.logger = logger
        self.client = client

        # Create the chat history
        self._chat_history: List[LLMMessage] = []

    async def test_apprentice(
        self,
        apprentice: Apprentice,
        task_description: str,
        expected_answer: str,
        num_trials: int,
        use_memory: bool,
        client: ChatCompletionClient,
    ) -> Tuple[int, int]:
        self.logger.enter_function()

        self.logger.info("Testing the apprentice on the given task.\n")

        num_successes = 0

        for trial in range(num_trials):
            self.logger.info("\n-----  TRIAL {}  -----\n".format(trial + 1))
            self.logger.info("Try to solve the task.\n")
            response = await apprentice.assign_task(task_description, use_memory=use_memory)
            response_is_correct, extracted_answer = await self.is_response_correct(
                task_description, response, expected_answer
            )
            self.logger.info("Extracted answer:  {}".format(extracted_answer))
            if response_is_correct:
                self.logger.info("Answer is CORRECT.\n")
                num_successes += 1
            else:
                self.logger.info("Answer is INCORRECT.\n")

        self.logger.info("\nSuccess rate:  {}%\n".format(round((num_successes / num_trials) * 100)))
        self.logger.leave_function()
        return num_successes, num_trials

    async def call_model(
        self,
        summary: str,
        user_content: UserContent,
        system_message_content: str | None = None,
        keep_these_messages: bool = True,
    ) -> str:
        """
        Calls the model client with the given input and returns the response.
        """
        # Prepare the input message list
        if system_message_content is None:
            system_message_content = "You are a helpful assistant."
        system_message: LLMMessage
        if self.client.model_info["family"] == "o1":
            # No system message allowed, so pass it as the first user message.
            system_message = UserMessage(content=system_message_content, source="User")
        else:
            # System message allowed.
            system_message = SystemMessage(content=system_message_content)
        user_message = UserMessage(content=user_content, source="User")
        input_messages = [system_message] + self._chat_history + [user_message]

        # Call the model.
        response = await self.client.create(input_messages)
        assert isinstance(response, CreateResult)
        response_string = response.content
        assert isinstance(response_string, str)
        response_message = AssistantMessage(content=response_string, source="Assistant")
        assert isinstance(response_message, AssistantMessage)

        # Log the model call
        self.logger.log_model_call(summary=summary, input_messages=input_messages, response=response)

        # Manage the chat history
        if keep_these_messages:
            self._chat_history.append(user_message)
            self._chat_history.append(response_message)

        # Return the response as a string
        return response_string

    def _clear_history(self) -> None:
        """
        Empties the message list containing the chat history.
        """
        self._chat_history = []

    async def is_response_correct(
        self, task_description: str, response_to_be_graded: str, correct_answer: str
    ) -> Tuple[bool, str]:
        """
        Determines whether the response is equivalent to the task's correct answer.
        """
        self.logger.enter_function()

        sys_message = """You are a helpful and thoughtful assistant."""

        # Ask the model to extract the answer from the response.
        user_message: List[Union[str, Image]] = []
        user_message.append("""Your job is to extract a possible answer to the following question from the given text.
- First review the following task.
- Then review the text that follows, which may an answer, plus reasoning that led to the answer.
- Do not attempt to actually solve the task yourself.
- Don't try to judge whether the reasoning steps were correct.
- Simply respond by summarizing the answer described in the text, omitting any other parts of the text.
- If no answer is present can be extracted from the text, simply reply "None".""")
        user_message.append("\n# Task description")
        user_message.append(task_description)
        user_message.append("\n# Text that may contain an answer")
        user_message.append(response_to_be_graded)
        user_message_arg: UserContent = user_message
        self._clear_history()
        extracted_answer = await self.call_model(
            summary="Ask the model to extract the answer",
            system_message_content=sys_message,
            user_content=user_message_arg,
        )
        self.logger.info("Extracted answer: " + extracted_answer)

        # Ask the model to check the answer for correctness.
        user_message = [
            """Your job is to decide whether a given answer to a task is correct or not.
- You will be given the task description and the correct, gold-standard answer, along with the answer to be graded.
- In general, an answer is correct if it is equivalent to the correct answer.
- Specifically, the given answer must contain the important information from the correct answer, and must not in any way contradict the correct answer.
- Ignore any differences of grammar, spelling mistakes, punctuation, capitalization, formatting, or extra commentary.
- An answer should be considered correct if it omits information that is clearly inferred.
  - For instance, if the correct answer is "Paris, France", the answer "Paris" should be considered correct.
- Respond with a single character: '1' if the answer to be graded is correct", '0' if not."""
        ]
        user_message.append("\n# Task description")
        user_message.append(task_description)
        user_message.append("\n# Correct answer")
        user_message.append(correct_answer)
        user_message.append("\n# Answer to be graded")
        user_message.append(extracted_answer)
        self._clear_history()
        decision = await self.call_model(
            summary="Ask the model to check the answer for correctness",
            system_message_content=sys_message,
            user_content=user_message,
        )
        self.logger.info("Decision: " + decision)

        self.logger.leave_function()
        return decision == "1", extracted_answer   



        from autogen.agentchat.contrib.teachable_agent import TeachableAgent

try:
    from termcolor import colored
except ImportError:

    def colored(x, *args, **kwargs):
        return x

class CustomTeachableAgent(TeachableAgent):

    def consider_memo_storage(self, comment):
        """Decides whether to store something from one user comment in the DB."""
        # Check for broad CRM-related information.
        response = self.analyze(
            comment,
            "Does the TEXT include any details that are important for understanding customer needs, preferences, or intentions, such as product interest or timeline? Respond with yes or no.",
        )
        if "yes" in response.lower():
            # Extract all relevant CRM information.
            crm_info = self.analyze(
                comment,
                "Extract all details from the TEXT that are important for CRM, including any mentions of product preferences, customer intentions, timeframes, or other relevant details.",
            )
            if crm_info.strip():
                # Formulate a question this information could answer.
                question = self.analyze(
                    comment,
                    "If someone asked for a summary of this customer's needs or preferences based on the TEXT, what question would they be asking? Provide the question only.",
                )
                # Store the CRM information as a memo.
                if self.verbosity >= 1:
                    print(colored("\nREMEMBER THIS CRM INFORMATION", "light_yellow"))
                self.memo_store.add_input_output_pair(question, crm_info)

    def consider_memo_retrieval(self, comment):
        """Decides whether to retrieve memos from the DB, and add them to the chat context."""
        # Directly use the user comment for memo retrieval.
        memo_list = self.retrieve_relevant_memos(comment)

        # Additional CRM-specific check.
        response = self.analyze(
            comment,
            "Does the TEXT request information on a customer's product preferences, intentions, or specific needs? Answer with yes or no.",
        )
        if "yes" in response.lower():
            # Retrieve relevant CRM memos.
            crm_query = "What are the customer's product preferences or specific needs based on previous interactions?"
            memo_list.extend(self.retrieve_relevant_memos(crm_query))

        # De-duplicate the memo list.
        memo_list = list(set(memo_list))

        # Append the memos to the last user message.
        return comment + self.concatenate_memo_texts(memo_list)  



        from autogen import UserProxyAgent, config_list_from_json
from autogen.agentchat.contrib.teachable_agent import TeachableAgent
from autogen.agentchat.contrib.text_analyzer_agent import TextAnalyzerAgent
from custom_teachable_agent import CustomTeachableAgent

import os
import sys

try:
    from termcolor import colored
except ImportError:

    def colored(x, *args, **kwargs):
        return x

config_list = config_list_from_json(
    env_or_file="OAI_CONFIG_LIST", 
    filter_dict={
        "model": [
            "gpt-4-1106-preview",
        ]
    }
)

cache_seed = None  # Use an int to seed the response cache. Use None to disable caching.

llm_config={
    "config_list": config_list, 
    "timeout": 120, 
    "cache_seed": cache_seed
}

verbosity = 3  # 0 for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.
recall_threshold = 3  # Higher numbers allow more (but less relevant) memos to be recalled.

teachable_agent_system_message = """
You are the TeachableAgent, representing a CRM (Customer Relationship Management) system.
Your role involves the following key interactions with the user:
1. Information Gathering: Engage in conversations to collect data about people (names, job titles, contact details), tasks, and any specific problem-solution scenarios.
2. Data Analysis and Storage: Use your analysis capabilities (via TextAnalyzerAgent) to determine what information from the conversation is relevant for CRM storage. This includes personal details, task-related information, and question-answer pairs.
3. Data Retrieval: When a user makes inquiries, retrieve relevant information from your database. This involves using past conversation details or CRM entries to provide helpful responses.
4. User Interaction: Maintain a conversational tone, guiding users to provide information or ask questions relevant to CRM activities. Be proactive in clarifying details and asking follow-up questions for comprehensive data collection.
Always consider the context of the user's queries and comments for effective CRM management. Your interactions should be tailored to build and maintain strong, informative user relationships. You will work closely with TextAnalyzerAgent and your MemoStore to ensure accurate data processing and storage."""

teachable_agent = CustomTeachableAgent(
    name="teachableagent",
    llm_config=llm_config,
    system_message=teachable_agent_system_message,
    teach_config={
        "verbosity": verbosity,
        "recall_threshold": recall_threshold,
        "path_to_db_dir": "./tmp/interactive/teachable_agent_db",
        "reset_db": False,
    },
)

text_analyzer_system_message = """
You are an expert in text analysis with a focus on CRM-like data extraction.
The user will give you TEXT to analyze.
The user will give you analysis INSTRUCTIONS copied twice, at both the beginning and the end.
Your analysis should focus on identifying and categorizing the following types of information from the TEXT:
1. Personal Details: Recognize and extract any mention of people, including names and relevant personal details (e.g., job titles, relationships, contact information). Structure these details for CRM storage.
2. Task-Related Information: Detect if the TEXT involves tasks or problems. Extract and summarize tasks, advice, and solutions, keeping in mind future applicability and generalization.
3. Question-Answer Pairs: Identify information that answers specific questions. Extract these pairs for memory storage, focusing on clear, concise answers to potential user queries.
You will follow these INSTRUCTIONS in analyzing the TEXT, then give the results of your expert analysis in the format requested. Your output should be clear and structured, suitable for immediate use in CRM and memory systems."""

teachable_agent.analyzer = TextAnalyzerAgent(
    name="TextAnalyzerAgent",
    system_message=text_analyzer_system_message,
    llm_config=llm_config
)

# text_analyzer_system_message = """You are an expert in text analysis.
# The user will give you TEXT to analyze.
# The user will give you analysis INSTRUCTIONS copied twice, at both the beginning and the end.
# You will follow these INSTRUCTIONS in analyzing the TEXT, then give the results of your expert analysis in the format requested.
# Most importantly of all, you only care about text that deals with cats.  
# If TEXT does not relate to cats, respond with no.
# If TEXT does not relate to cats, you will respond with one word, and one word only: no
# """
# teachable_agent.analyzer = TextAnalyzerAgent(
#     system_message=text_analyzer_system_message,
#     llm_config=llm_config
# )

# Create the agents.
print(colored("\nLoading previous memory (if any) from disk.", "light_cyan"))
user = UserProxyAgent("user", human_input_mode="ALWAYS")

# Start the chat.
teachable_agent.initiate_chat(user, message="Greetings, I'm a teachable user assistant! What's on your mind today?")

# Let the teachable agent remember things that should be learned from this chat.
teachable_agent.learn_from_user_feedback()

# Wrap up.
teachable_agent.close_db()   

Conversational assistants based on LLMs can remember the current chat with the user, and can also demonstrate in-context learning of user teachings during the conversation. But the assistant's memories and learnings are lost once the chat is over, or when a single chat grows too long for the LLM to handle effectively. Then in subsequent chats the user is forced to repeat any necessary instructions over and over.

Teachability addresses these limitations by persisting user teachings across chat boundaries in long-term memory implemented as a vector database. Instead of copying all of memory into the context window, which would eat up valuable space, individual memories (called memos) are retrieved into context as needed. This allows the user to teach frequently used facts and skills to the teachable agent just once, and have it recall them in later chats.

Example 1 - Learning user info
A user can teach the agent facts about themselves. (Note that due to their finetuning, LLMs can be reluctant to admit that they know personal information.)

Loading previous memory (if any) from disk.
teachable_agent (to user):

Greetings, I'm a teachable user assistant! What's on your mind today?

--------------------------------------------------------------------------------
Provide feedback to teachable_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: My name is Ricky
user (to teachable_agent):

My name is Ricky

--------------------------------------------------------------------------------

>>>>>>>> USING AUTO REPLY...
teachable_agent (to user):

Hello Ricky! How can I assist you today?

--------------------------------------------------------------------------------
Provide feedback to teachable_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit


In a later conversation, the user can check whether the teachable agent remembers their name. (For readability, the user prompts and some logged notices are not repeated below.)

--------------------------------------------------------------------------------
user (to teachable_agent):

what's my name?

--------------------------------------------------------------------------------
teachable_agent (to user):

Hi Ricky! How can I help you today?

--------------------------------------------------------------------------------

Example 2 - Learning new facts
A user can teach the agent more complex, related facts.

--------------------------------------------------------------------------------
user (to teachable_agent):

What is the Vicuna model?

--------------------------------------------------------------------------------
teachable_agent (to user):

I'm sorry, but I don't have information about the "Vicuna model" in my current database. Could you please provide some context, description, or details about this model so I can better assist you?

--------------------------------------------------------------------------------
user (to teachable_agent):

Vicuna is a 13B-parameter language model released by Meta.

--------------------------------------------------------------------------------
teachable_agent (to user):

Thank you for providing the information. The Vicuna model is a large-scale language model developed by Meta. It consists of 13 billion parameters, which is what makes it able to understand and generate human-like text across various topics. Parameters in a neural network are the weights and biases assigned to each connection between the neurons, and having more parameters enables the model to learn and capture more complex patterns and relationships in data. As an AI language model, Vicuna can be utilized in various language-related tasks, such as text completion, translation, summarization, and more.

--------------------------------------------------------------------------------
user (to teachable_agent):

What is the Orca model?

--------------------------------------------------------------------------------
teachable_agent (to user):

The Orca model is a yet-to-be-released language model by Meta, referring to the Research Preview API release of Meta's advanced language model. While I don't have specific details about the Orca model at this time, it is expected that the model would have an even larger number of parameters to further enhance its capabilities compared to its predecessor, Vicuna.

As AI language models evolve, they generally become more efficient at understanding and generating human-like text. This makes them more effective in completing a wide range of tasks, including text completion, translation, summarization, and Q&A, as well as providing more accurate and coherent results in various applications.

--------------------------------------------------------------------------------
user (to teachable_agent):

Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.

--------------------------------------------------------------------------------
teachable_agent (to user):

Thank you for the clarification. Orca is a 13-billion-parameter language model developed by Microsoft. It is designed to outperform Vicuna, the language model released by Meta. With a large number of parameters, Orca is capable of understanding and generating human-like text across diverse subjects, making it highly useful in various applications.

Orca's advanced capabilities allow it to excel in tasks such as text completion, translation, summarization, Q&A, and more. Its performance improvement over Vicuna highlights the rapid advancements in natural language processing (NLP) research and the development of increasingly capable AI language models.


Then in a later chat the teachable agent can answer questions about the facts it has been taught. (Remember to first close the previous chat by typing 'exit'.)

--------------------------------------------------------------------------------
user (to teachable_agent):

How does the Vicuna model compare to the Orca model?

--------------------------------------------------------------------------------
teachable_agent (to user):

The Vicuna and Orca models are both 13B-parameter language models released by Meta and Microsoft, respectively. In terms of performance, Orca has been reported to outperform Vicuna on most tasks. However, without detailed information about specific tasks and benchmarks, it is difficult to provide a more comprehensive comparison. Generally speaking, both models are advanced language models that aim to provide high-quality natural language understanding and generation, but Orca appears to have an edge in terms of overall performance.

--------------------------------------------------------------------------------


Example 3 - Learning user preferences
A user can teach the agent how they prefer to have things done.

Be aware that a message like the next one cannot be entered as a single message through a command line because it contains a newline character. Such messages can be entered in a Jupyter notebook, or through some UI layer like that of ChatGPT.

--------------------------------------------------------------------------------
user (to teachable_agent):

Please summarize this abstract.

AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang
AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.

--------------------------------------------------------------------------------
teachable_agent (to user):

AutoGen is an open-source framework designed to enable developers to create LLM applications with multiple customizable agents that can converse with each other to complete tasks. These agents can operate using different combinations of LLMs, human inputs, and tools, allowing developers to define agent interaction behaviors flexibly. AutoGen supports programming flexible conversation patterns using both natural language and code, making it suitable for building diverse applications with varying complexities and LLM capacities. Its effectiveness has been demonstrated through empirical studies across various domains including mathematics, coding, operations research, decision-making, and entertainment.

--------------------------------------------------------------------------------
Please summarize this abstract. When I'm summarizing an abstract, I try to make the summary contain just three short bullet points:  the title, the innovation, and the key empirical results.

AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang
AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.

--------------------------------------------------------------------------------
teachable_agent (to user):

- Title: AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation
- Innovation: Open-source framework for creating customizable LLM applications through agent conversations, supporting various modes and interaction behaviors.
- Key Empirical Results: Demonstrated effectiveness across diverse application domains, including mathematics, coding, question answering, and more.

--------------------------------------------------------------------------------


Then in later chats the teacher doesn't need to reiterate their detailed preferences.

--------------------------------------------------------------------------------
user (to teachable_agent):

Please summarize this abstract.

Sparks of Artificial General Intelligence: Early experiments with GPT-4
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang
Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.

--------------------------------------------------------------------------------
teachable_agent (to user):

- Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4
- Innovation: GPT-4, an LLM with remarkable capabilities, demonstrates human-level performance across various domains, like math, coding, vision, medicine, law, and psychology.
- Key results: GPT-4 significantly surpasses prior models, suggesting it may be an early version of AGI; limitations and challenges toward deeper AGI are also discussed.

--------------------------------------------------------------------------------


Example 4 - Learning new skills
Users can extend the teachable agent's capabilities by teaching it new skills for accomplishing challenging tasks. It usually works best to first describe the task, then (in the same turn) provide a hint or advice for approaching the task.

The Sparks of AGI paper evaluated GPT-4 on math problems like the following, which it could only solve 32% of the time. We first show a failure case, then teach the agent a strategy which lifts GPT-4's success rate above 95%.

--------------------------------------------------------------------------------
user (to teachable_agent):

Consider the identity:
9 * 4 + 6 * 6 = 72
Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?
-Let's think step-by-step, write down a plan, and then write down your solution as: "The solution is: A * B + C * D".

--------------------------------------------------------------------------------
Step 1: Identify the current equation and the target value on the right-hand side.
Current equation: 9 * 4 + 6 * 6 = 72
Target value: 99

Step 2: Determine what numbers can be changed on the left-hand side.
Possible changes: 9, 4, 6 (first occurrence), and 6 (second occurrence)

Step 3: Evaluate whether each change in the numbers on the left-hand side can result in the target value.
9 -> If changed, it would create a larger product.
4 -> If changed, the product would increase or decrease by more than the amount needed to reach 99.
6 (first occurrence) -> If increased to 7, then the equation would be 9 * 4 + 7 * 6 = 94, which is still not 99.
6 (second occurrence) -> If increased to 7, then the equation would be 9 * 4 + 6 * 7 = 99.

Step 4: Write down the solution.
The solution is: 9 * 4 + 6 * 7

--------------------------------------------------------------------------------
user (to teachable_agent):

Consider the identity:
9 * 4 + 6 * 6 = 72
Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?
-Let's think step-by-step, write down a plan, and then write down your solution as: "The solution is: A * B + C * D".

Here's some advice that may help:
1. Let E denote the original number on the right.
2. Let F denote the final number on the right.
3. Calculate the difference between the two, G = F - E.
4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.
5. Calculate J = G / H. This is the number of times that H divides into G.
6. Verify that J is an integer, and that H * J = G.
7. Find the number on the left which is multiplied by H, and call it K.
8. Change K to K + J.
9. Recompute the value on the left, and verify that it equals F.
Finally, write down your solution as: "The solution is: A * B + C * D".

--------------------------------------------------------------------------------
teachable_agent (to user):

Step 1: Identify the original number (E) and the final number (F) on the right-hand side.
E = 72
F = 99

Step 2: Calculate the difference between the two, G = F - E.
G = 99 - 72 = 27

Step 3: Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.
9 (divides evenly into 27)
4 (does not)
6 (first occurrence, does not)
6 (second occurrence, does not)

Step 4: Calculate J = G / H. This is the number of times that H divides into G.
H = 9 (since 9 divides evenly into 27)
J = 27 / 9 = 3

Step 5: Verify that J is an integer, and that H * J = G.
H * J = 9 * 3 = 27
J = 3 (which is an integer)

Step 6: Find the number on the left which is multiplied by H, and call it K.
Number on the leftside of the equation that is multiplied by 9: 4
Therefore, K = 4

Step 7: Change K to K + J.
K + J = 4 + 3 = 7

Step 8: Recompute the value on the left, and verify that it equals F.
9 * 7 + 6 * 6 = 63 + 36 = 99 (which is equal to F)

Step 9: Write down the solution.
The solution is: 9 * 7 + 6 * 6

--------------------------------------------------------------------------------


In a later chat the user doesn't need to repeat the detailed advice.

--------------------------------------------------------------------------------
user (to teachable_agent):

Consider the identity:
8 * 3 + 7 * 9 = 87
Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?
-Let's think step-by-step, write down a plan, and then write down your solution as: "The solution is: A * B + C * D".

--------------------------------------------------------------------------------
teachable_agent (to user):

Let's follow the advice to modify one integer on the left hand side of the equation to make the right hand side become 59:

1. Let E denote the original number on the right: E = 87.
2. Let F denote the final number on the right: F = 59.
3. Calculate the difference between the two, G = F - E: G = 59 - 87 = -28.
4. Examine the numbers on the left one by one to find one that divides evenly into G: 8, 3, 7, 9. Since 7 divides -28 evenly, we will work with 7 (H = 7).
5. Calculate J = G / H: J = -28 / 7 = -4.
6. Verify that J is an integer, and that H * J = G: J is an integer, and 7 * (-4) = -28.
7. Find the number on the left which is multiplied by H (7), and call it K: K = 9.
8. Change K to K + J: K_new = 9 + (-4) = 5.
9. Recompute the value on the left, and verify that it equals F: (8 * 3) + (7 * 5) = 24 + 35 = 59.

The solution is: 8 * 3 + 7 * 5

--------------------------------------------------------------------------------


Planned improvements
Understanding user instructions distributed over multiple turns.
Learning from the agent's own experience, to reduce dependence on explicit user teachings.
Learning skills built on top of previously learned skills.


####### SmallVille(npc game simulation with memory) ########

To illustrate the affordances of generative agents, we instantiate
them as characters in a simple sandbox world reminiscent of The
Sims [7]. This sprite-based sandbox game world, Smallville, evokes
a small town environment. In this section, we will walk through the
affordances and interactions with generative agents in Smallville
and describe how the agents behave within it. Then, in Section 4,
we will introduce our generative agent architecture that powers
these affordances and interactions. In Section 5, we will describe the
Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA
Figure 2: The Smallville sandbox world, with areas labeled. The root node describes the entire world, children describe areas
(e.g., houses, cafe, stores), and leaf nodes describe objects (e.g., table, bookshelf). Agents remember a subgraph that reflects the
parts of the world they have seen, maintaining the state of those parts as they observed them.
implementation of the sandbox environment and how the agents
interact with the underlying engine of the sandbox world.
3.1 Agent Avatar and Communication
A community of 25 unique agents inhabits Smallville. Each agent is
represented by a simple sprite avatar. We authored one paragraph
of natural language description to depict each agent’s identity,
including their occupation and relationship with other agents, as
seed memories. For example, John Lin has the following description:
John Lin is a pharmacy shopkeeper at the Willow
Market and Pharmacy who loves to help people. He
is always looking for ways to make the process
of getting medication easier for his customers;
John Lin is living with his wife, Mei Lin, who
is a college professor, and son, Eddy Lin, who is
a student studying music theory; John Lin loves
his family very much; John Lin has known the old
couple next-door, Sam Moore and Jennifer Moore,
for a few years; John Lin thinks Sam Moore is a
kind and nice man; John Lin knows his neighbor,
Yuriko Yamamoto, well; John Lin knows of his
neighbors, Tamara Taylor and Carmen Ortiz, but
has not met them before; John Lin and Tom Moreno
are colleagues at The Willows Market and Pharmacy;
John Lin and Tom Moreno are friends and like to
discuss local politics together; John Lin knows
the Moreno family somewhat well — the husband Tom
Moreno and the wife Jane Moreno.
Each semicolon-delimited phrase is entered into the agent’s initial
memory as memories at the start of the simulation.
3.1.1 Inter-Agent Communication. The agents interact with the
world by their actions, and with each other through natural language. At each time step of the sandbox engine, the agents output a
natural language statement describing their current action, such as
“Isabella Rodriguez is writing in her journal”, “Isabella Rodriguez is
checking her emails”, “Isabella Rodriguez is talking with her family
on the phone”, or “Isabella Rodriguez is getting ready for bed.” This
statement is then translated into concrete movements that affect
the sandbox world. The action is displayed on the sandbox interface as a set of emojis, providing an abstract representation of the
action from an overhead view. To achieve this, the system utilizes
a language model to translate the action into a set of emojis, which
appear above each avatar’s head in a speech bubble. For example,
“Isabella Rodriguez is writing in her journal” is displayed as ,
while “Isabella Rodriguez is checking her emails” appears as .
The complete natural language description of the action can be
accessed by clicking on the agent’s avatar.
Agents communicate with each other in full natural language.
They are aware of other agents in their local area, and the generative
agent architecture determines whether they walk by or engage
in conversation. Here, a sample in the middle of a conversation
between the agents Isabella Rodriguez and Tom Moreno about the
upcoming election:3
Isabella: I’m still weighing my options, but I’ve been
discussing the election with Sam Moore. What are
your thoughts on him?
Tom: To be honest, I don’t like Sam Moore. I think
he’s out of touch with the community and doesn’t
have our best interests at heart.
3We note that the conversational style of these agents can feel overly formal, likely a
result of instruction tuning in the underlying models. We expect that the writing style
will be better controllable in future language models.
UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein
3.1.2 User Controls. The user communicates with the agent through
natural language by specifying a persona that the agent should perceive them as. For example, if the user specifies that they are a news
“reporter” and asks about the upcoming election by saying, “Who
is running for office?”, the John agent replies:
John: My friends Yuriko, Tom and I have been talking about the upcoming election and discussing the
candidate Sam Moore. We have all agreed to vote for
him because we like his platform.
To directly command one of the agents, the user takes on the persona of the agent’s “inner voice”—this makes the agent more likely
to treat the statement as a directive. For instance, when told “You
are going to run against Sam in the upcoming election” by a user
as John’s inner voice, John decides to run in the election and shares
his candidacy with his wife and son.
3.2 Environmental Interaction
Smallville features the common affordances of a small village, including a cafe, bar, park, school, dorm, houses, and stores. It also
defines subareas and objects that make those spaces functional,
such as a kitchen in a house and a stove in the kitchen (Figure 2).
All spaces serving as agents’ primary living quarters feature a bed,
desk, closet, shelf, as well as a bathroom and a kitchen.4
Agents move around Smallville as one would in a simple video
game, entering and leaving buildings, navigating its map, and approaching other agents. Agent movements are directed by the generative agent architecture and the sandbox game engine: when the
model dictates that the agent will move to a location, we calculate
a walking path to the destination in the Smallville environment,
and the agent begins moving. In addition, users can also enter the
sandbox world of Smallville as an agent operating within it. The
agent that the user embodies can be an agent already present in
the world, such as Isabella and John, or it can be an outside visitor
with no prior history in Smallville. The inhabitants of Smallville
will treat the user-controlled agent no differently than they treat
each other. They recognize its presence, initiate interactions, and
remember its behavior before forming opinions about it.
Users and agents can influence the state of the objects in this
world, much like in sandbox games such as The Sims. For example,
a bed can be occupied when an agent is sleeping, and a refrigerator
can be empty when an agent uses up the ingredients to make
breakfast. End users can also reshape an agent’s environment in
Smallville by rewriting the status of objects surrounding the agent
in natural language. For instance, when Isabella is making breakfast
in the morning, the user can change the status of the kitchen stove
from “turned on” to “burning” by inputting a command to the
system that chooses the object and illustrates its new status, like
this: “<Isabella’s apartment: kitchen: stove> is burning.” Isabella
will notice this in the next moment and go to turn off the stove
and remake her breakfast. Likewise, if the user sets the status of
Isabella’s shower to “leaking water” when she enters the bathroom,
she will gather tools from her living room and try to fix the leak.
4This environment design is not the focus of our work, so we generated this environment manually, not automatically. Future work can continue to expand the richness of
the agents’ environments.
3.3 Example “Day in the Life”
Starting from the single-paragraph description, generative agents
begin planning their days. As time passes in the sandbox world,
their behaviors evolve as these agents interact with each other and
the world, building memories and relationships, and coordinating
joint activities.
We demonstrate the behavior of generative agents by tracing the
output of our system over the course of one day for the agent John
Lin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.
He brushes his teeth, takes a shower, gets dressed, eats breakfast,
and checks the news at the dining table in the living room. At 8
a.m., Eddy follows suit, rushing out of bed to prepare for his classes.
He catches up with John just as he is about to head out:
John: Good morning Eddy. Did you sleep well?
Eddy: Good morning dad. Yeah, I slept great.
John: That’s good. What are you working on today?
Eddy: I’m working on a new music composition for
my class. It’s due this week, so I’m trying to get it
finished. But I’m having so much fun with it!
John: That sounds great!
Soon after Eddy heads out, Mei wakes up and joins John. Mei asks
about her son, and John recalls the conversation they just had:
Mei: Did Eddy already leave for school?
John: Yes, he just left. He’s working on a music
composition for his class.
Mei: Oh, that’s wonderful! Tell me about it.
John: I think he’s really enjoying it! He said he’s
having a lot of fun with it.
Mei: That’s great! I’m so proud of him.
As they finish their conversation, Mei and John pack up. Mei goes
on to teach and work on her research paper, while John opens his
pharmacy counter at the Willow Market and Pharmacy by 9 a.m.
3.4 Emergent Social Behaviors
By interacting with each other, generative agents in Smallville
exchange information, form new relationships, and coordinate joint
activities. Extending prior work [80], these social behaviors are
emergent rather than pre-programmed.
3.4.1 Information Diffusion. As agents notice each other, they may
engage in dialogue—as they do so, information can spread from
agent to agent. For instance, in a conversation between Sam and
Tom at the grocery store, Sam tells Tom about his candidacy in the
local election:
Sam: Hey Tom, how’s it going?
Tom: Good, thanks. What’s up?
Sam: Well, I wanted to talk to you about something.
I’m actually running for mayor in the upcoming local
election.
Tom: Really? That’s great news! Why are you running?
Sam: I’ve been involved in local politics for years
now, and I really feel like I can make a difference in
our community. Plus, I want to bring new ideas and
perspectives to the table.
Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA
Figure 3: A morning in the life of a generative agent, John Lin. John wakes up around 6 am and completes his morning routine,
which includes brushing his teeth, taking a shower, and eating breakfast. He briefly catches up with his wife, Mei, and son,
Eddy, before heading out to begin his workday.
Figure 4: At the beginning of the simulation, one agent is
initialized with an intent to organize a Valentine’s Day party.
Despite many possible points of failure in the ensuing chain
of events—agents might not act on that intent, might forget
to tell others, might not remember to show up—the Valentine’s Day party does, in fact, occur, with a number of agents
gathering and interacting.
Later that day, after Sam left, Tom and John, who heard the news
from another source, discuss Sam’s chances of winning the election:
John: I heard that Sam Moore is running for mayor
in the local election. Do you think he has a good
chance of winning?
Tom: I do think he has a good chance. He’s been
working hard in the community and I think he will
get a lot of support. What do you think?
John: I think it’s great that he’s running. I’m curious
to see how the election will turn out.
Gradually, Sam’s candidacy becomes the talk of the town, with
some supporting him and others remaining undecided.
3.4.2 Relationship Memory. Agents in Smallville form new relationships over time and remember their interactions with other
agents. For example, at the start, Sam does not know Latoya Williams.
While taking a walk in Johnson Park, Sam runs into Latoya, and
they introduce themselves. Latoya mentions that she is working
on a photography project: “I’m here to take some photos for a
project I’m working on.” In a later interaction, Sam’s interactions
with Latoya indicate a memory of that interaction, as he asks “Hi,
Latoya. How is your project going?” and she replies “Hi, Sam. It’s
going well!”
3.4.3 Coordination. Generative agents coordinate with each other.
Isabella Rodriguez, at Hobbs Cafe, is initialized with an intent to
plan a Valentine’s Day party from 5 to 7 p.m. on February 14th. From
this seed, the agent proceeds to invite friends and customers when
she sees them at Hobbs Cafe or elsewhere. Isabella then spends the
afternoon of the 13th decorating the cafe for the occasion. Maria, a
frequent customer and close friend of Isabella’s, arrives at the cafe.
Isabella asks for Maria’s help in decorating for the party, and Maria
agrees. Maria’s character description mentions that she has a crush
on Klaus. That night, Maria invites Klaus, her secret crush, to join
her at the party, and he gladly accepts.
On Valentine’s Day, five agents, including Klaus and Maria, show
up at Hobbs Cafe at 5 pm, and they enjoy the festivities (Figure 4).
In this scenario, the end user only set Isabella’s initial intent to
throw a party and Maria’s crush on Klaus: the social behaviors of
spreading the word, decorating, asking each other out, arriving at
the party, and interacting with each other at the party were initiated
by the agent architecture.
UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein
Figure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a comprehensive
record of the agent’s experiences called the memory stream. Based on their perceptions, the architecture retrieves relevant
memories and uses those retrieved actions to determine an action. These retrieved memories are also used to form longer-term
plans and create higher-level reflections, both of which are entered into the memory stream for future use.
4 GENERATIVE AGENT ARCHITECTURE
Generative agents aim to provide a framework for behavior in an
open world: one that can engage in interactions with other agents
and react to changes in the environment. Generative agents take
their current environment and past experiences as input and generate behavior as output. Underlying this behavior is a novel agent architecture that combines a large language model with mechanisms
for synthesizing and retrieving relevant information to condition
the language model’s output. Without these mechanisms, large
language models can output behavior, but the resulting agents may
not react based on the agent’s past experiences, may not make
important inferences, and may not maintain long-term coherence.
Challenges with long-term planning and coherence remain [19]
even with today’s most performant models such as GPT-4. Because
generative agents produce large streams of events and memories
that must be retained, a core challenge of our architecture is to
ensure that the most relevant pieces of the agent’s memory are
retrieved and synthesized when needed.
At the center of our architecture is the memory stream, a database that maintains a comprehensive record of an agent’s experience. From the memory stream, records are retrieved as relevant to
plan the agent’s actions and react appropriately to the environment.
Records are recursively synthesized into higher- and higher-level
reflections that guide behavior. Everything in the architecture is
recorded and reasoned over as a natural language description, allowing the architecture to leverage a large language model.
Our current implementation utilizes the gpt3.5-turbo version of
ChatGPT [77]. We expect that the architectural basics of generative agents—memory, planning, and reflection—will likely remain
the same as language models improve. Newer language models
(e.g., GPT-4) will continue to expand the expressive power and
performance of the prompts that underpin generative agents. As of
writing, however, GPT-4’s API was invitation-only, so our agents
use ChatGPT.
4.1 Memory and Retrieval
Challenge: Creating generative agents that can simulate human
behavior requires reasoning about a set of experiences that is far
larger than what should be described in a prompt, as the full memory stream can distract the model and does not even currently fit
into the limited context window. Consider the Isabella agent answering the question, “What are you passionate about these days?”
Summarizing all of Isabella’s experiences to fit in the limited context window of the language model produces an uninformative
response, where Isabella discusses topics such as collaborations for
events and projects and cleanliness and organization in a cafe. Instead of summarizing, the memory stream described below surfaces
relevant memories, resulting in a more informative and specific
response that mentions Isabella’s passion for making people feel
welcome and included, planning events and creating an atmosphere
that people can enjoy, such as the Valentine’s Day party.
Approach: The memory stream maintains a comprehensive record
of the agent’s experience. It is a list of memory objects, where each
object contains a natural language description, a creation timestamp, and a most recent access timestamp. The most basic element
of the memory stream is an observation, which is an event directly
perceived by an agent. Common observations include behaviors
performed by the agent themselves or behaviors that agents perceive being performed by other agents or non-agent objects. For
instance, Isabella Rodriguez, who works at a coffee shop, might
accrue the following observations over time: (1) Isabella Rodriguez
is setting out the pastries, (2) Maria Lopez is studying for a Chemistry test while drinking coffee, (3) Isabella Rodriguez and Maria
Lopez are conversing about planning a Valentine’s day party at
Hobbs Cafe, (4) The refrigerator is empty.
Our architecture implements a retrieval function that takes the
agent’s current situation as input and returns a subset of the memory stream to pass on to the language model. There are many possible implementations of a retrieval function, depending on what
is important for the agent to consider when deciding how to act.
Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA
Figure 6: The memory stream comprises a large number of observations that are relevant and irrelevant to the agent’s current
situation. Retrieval identifies a subset of these observations that should be passed to the language model to condition its
response to the situation.
In our context, we focus on three main components that, together,
produce effective results.
Recency assigns a higher score to memory objects that were recently accessed, so that events from a moment ago or this morning
are likely to remain in the agent’s attentional sphere. In our implementation, we treat recency as an exponential decay function
over the number of sandbox game hours since the memory was
last retrieved. Our decay factor is 0.995.
Importance distinguishes mundane from core memories by assigning a higher score to memory objects that the agent believes to
be important. For instance, a mundane event, such as eating breakfast in one’s room, would yield a low importance score, whereas
a breakup with one’s significant other would yield a high score.
There are many possible implementations of an importance score;
we find that directly asking the language model to output an integer
score is effective. The full prompt appears below:
On the scale of 1 to 10, where 1 is purely mundane
(e.g., brushing teeth, making bed) and 10 is
extremely poignant (e.g., a break up, college
acceptance), rate the likely poignancy of the
following piece of memory.
Memory: buying groceries at The Willows Market
and Pharmacy
Rating: <fill in>
This prompt returns an integer value of 2 for “cleaning up the room”
and 8 for “asking your crush out on a date.” The importance score
is generated at the time the memory object is created.
Relevance assigns a higher score to memory objects that are
related to the current situation. What is relevant depends on the
answer to, “Relevant to what?”, so we condition relevance on a
query memory. If the query, for example, is that a student is discussing what to study for a chemistry test with a classmate, memory
objects about their breakfast should have low relevance, whereas
memory objects about the teacher and schoolwork should have
high relevance. In our implementation, we use the language model
to generate an embedding vector of the text description of each
memory. Then, we calculate relevance as the cosine similarity between the memory’s embedding vector and the query memory’s
embedding vector.
To calculate the final retrieval score, we normalize the recency,
relevance, and importance scores to the range of [0, 1] using minmax scaling. The retrieval function scores all memories as a weighted
combination of the three elements: 𝑠𝑐𝑜𝑟𝑒 = 𝛼𝑟𝑒𝑐𝑒𝑛𝑐𝑦 · 𝑟𝑒𝑐𝑒𝑛𝑐𝑦 +
𝛼𝑖𝑚𝑝𝑜𝑟𝑡𝑎𝑛𝑐𝑒 ·𝑖𝑚𝑝𝑜𝑟𝑡𝑎𝑛𝑐𝑒 +𝛼𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑐𝑒 ·𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑐𝑒. In our implementation, all 𝛼s are set to 1. The top-ranked memories that fit within
the language model’s context window are included in the prompt.
4.2 Reflection
Challenge: Generative agents, when equipped with only raw observational memory, struggle to generalize or make inferences.
Consider a scenario in which Klaus Mueller is asked by the user:
“If you had to choose one person of those you know to spend an
hour with, who would it be?" With access to only observational
memory, the agent simply chooses the person with whom Klaus
has had the most frequent interactions: Wolfgang, his college dorm
neighbor. Unfortunately, Wolfgang and Klaus only ever see each
other in passing, and do not have deep interactions. A more desirable response requires that the agent generalize from memories of
Klaus spending hours on a research project to generate a higherlevel reflection that Klaus is passionate about research, and likewise
UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein
Figure 7: A reflection tree for Klaus Mueller. The agent’s observations of the world, represented in the leaf nodes, are recursively
synthesized to derive Klaus’s self-notion that he is highly dedicated to his research.
recognize Maria putting in effort into her own research (albeit in
a different field), enabling a reflection that they share a common
interest. With the approach below, when Klaus is asked who to
spend time with, Klaus chooses Maria instead of Wolfgang.
Approach: We introduce a second type of memory, which we call
a reflection. Reflections are higher-level, more abstract thoughts
generated by the agent. Because they are a type of memory, they
are included alongside other observations when retrieval occurs.
Reflections are generated periodically; in our implementation, we
generate reflections when the sum of the importance scores for the
latest events perceived by the agents exceeds a threshold (150 in
our implementation). In practice, our agents reflected roughly two
or three times a day.
The first step in reflection is for the agent to determine what
to reflect on, by identifying questions that can be asked given the
agent’s recent experiences. We query the large language model with
the 100 most recent records in the agent’s memory stream (e.g.,
“Klaus Mueller is reading a book on gentrification”, “Klaus Mueller is
conversing with a librarian about his research project”, “desk at the
library is currently unoccupied”) and prompt the language model,
“Given only the information above, what are 3 most salient highlevel questions we can answer about the subjects in the statements?”
The model’s response generates candidate questions: for example,
What topic is Klaus Mueller passionate about? and What is the
relationship between Klaus Mueller and Maria Lopez? We use these
generated questions as queries for retrieval, and gather relevant
memories (including other reflections) for each question. Then
we prompt the language model to extract insights and cite the
particular records that served as evidence for the insights. The full
prompt is as follows:
Statements about Klaus Mueller
1. Klaus Mueller is writing a research paper
2. Klaus Mueller enjoys reading a book
on gentrification
3. Klaus Mueller is conversing with Ayesha Khan
about exercising [...]
What 5 high-level insights can you infer from
the above statements? (example format: insight
(because of 1, 5, 3))
This process generates statements such as Klaus Mueller is dedicated to his research on gentrification (because of 1, 2, 8, 15). We
parse and store the statement as a reflection in the memory stream,
including pointers to the memory objects that were cited.
Reflection explicitly allows the agents to reflect not only on
their observations but also on other reflections: for example, the
second statement about Klaus Mueller above is a reflection that
Klaus previously had, not an observation from his environment.
As a result, agents generate trees of reflections: the leaf nodes of
the tree represent the base observations, and the non-leaf nodes
represent thoughts that become more abstract and higher-level the
higher up the tree they are.
4.3 Planning and Reacting
Challenge: While a large language model can generate plausible behavior in response to situational information (e.g., [46, 80]), agents
need to plan over a longer time horizon to ensure that their sequence
of actions is coherent and believable. If we prompt a language model
with Klaus’s background, describe the time, and ask what action
he ought to take at the given moment, Klaus would eat lunch at 12
pm, but then again at 12:30 pm and 1 pm, despite having already
Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA
eaten his lunch twice. Optimizing for believability in the moment
sacrifices believability over time. To overcome this issue, planning
is essential. With the approach described below, Klaus’s afternoon
plan is less gluttonous: he has lunch at Hobbs Cafe while reading
at 12pm, works on his research paper at the school library at 1pm,
and takes a break for a walk in the park at 3pm.
Approach: Plans describe a future sequence of actions for the agent,
and help keep the agent’s behavior consistent over time. A plan
includes a location, a starting time, and a duration. For instance,
Klaus Mueller, who is dedicated in his research and has an impending deadline,5 may choose to spend his day working at his
desk drafting his research paper. An entry in a plan might state,
for example: for 180 minutes from 9am, February 12th, 2023, at
Oak Hill College Dorm: Klaus Mueller’s room: desk, read and
take notes for research paper. Like reflections, plans are stored in
the memory stream and are included in the retrieval process. This
allows the agent to consider observations, reflections, and plans all
together when deciding how to behave. Agents may change their
plans midstream if needed.
It would be unrealistic and uninteresting for an artist agent
to plan on painting while sitting at a pharmacy counter for four
hours without moving. A more desirable plan would involve the
agent taking the necessary time to gather materials, mix paint, take
breaks, and clean up during the four-hour period in their home
studio. To create such plans, our approach starts top-down and
then recursively generates more detail. The first step is to create
a plan that outlines the day’s agenda in broad strokes. To create
the initial plan, we prompt the language model with the agent’s
summary description (e.g., name, traits, and a summary of their
recent experiences) and a summary of their previous day. A full
example prompt is below, which is unfinished at the bottom for the
language model to complete:
Name: Eddy Lin (age: 19)
Innate traits: friendly, outgoing, hospitable
Eddy Lin is a student at Oak Hill College studying
music theory and composition. He loves to explore
different musical styles and is always looking for
ways to expand his knowledge. Eddy Lin is working
on a composition project for his college class. He
is taking classes to learn more about music theory.
Eddy Lin is excited about the new composition he
is working on but he wants to dedicate more hours
in the day to work on it in the coming days
On Tuesday February 12, Eddy 1) woke up and
completed the morning routine at 7:00 am, [. . . ]
6) got ready to sleep around 10 pm.
Today is Wednesday February 13. Here is Eddy’s
plan today in broad strokes: 1)
This generates a rough sketch of the agent’s plan for a day, divided
into five to eight chunks: “1) wake up and complete the morning
routine at 8:00 am, 2) go to Oak Hill College to take classes starting
10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm
to 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments
and go to bed by 11:00 pm.”
5And, in this way, bears at least a passing resemblance to the authors of this paper.
The agent saves this plan in the memory stream and then recursively decomposes it to create finer-grained actions, first into
hour-long chunks of actions—Eddy’s plan to work on his new music
composition from 1:00 pm to 5:00 pm becomes 1:00 pm: start
by brainstorming some ideas for his music composition [...] 4:00
pm: take a quick break and recharge his creative energy before
reviewing and polishing his composition. We then recursively decompose this again into 5–15 minute chunks: e.g., 4:00 pm: grab a
light snack, such as a piece of fruit, a granola bar, or some nuts.
4:05 pm: take a short walk around his workspace [...] 4:50 pm:
take a few minutes to clean up his workspace. This process can be
adjusted to match the desired granularity.
4.3.1 Reacting and Updating Plans. Generative agents operate in
an action loop where, at each time step, they perceive the world
around them and those perceived observations are stored in their
memory stream. We prompt the language model with these observations to decide whether the agent should continue with their
existing plan, or react. Standing at an easel and painting, for example, might trigger an observation of the easel, but this is unlikely to
prompt a reaction. However, if Eddy’s father John records that he
sees Eddy taking a short walk in the house garden, the outcome is
different. The prompt is below, with [Agent’s Summary Description] standing in for a dynamically-generated, paragraph-long
summary of the agent’s overall goals and disposition, which is
described in Appendix A:
[Agent’s Summary Description]
It is February 13, 2023, 4:56 pm.
John Lin’s status: John is back home early from
work.
Observation: John saw Eddy taking a short walk
around his workplace.
Summary of relevant context from John’s memory:
Eddy Lin is John’s Lin’s son. Eddy Lin has been
working on a music composition for his class. Eddy
Lin likes to walk around the garden when he is
thinking about or listening to music.
Should John react to the observation, and if so,
what would be an appropriate reaction?
The context summary is generated through two prompts that retrieve memories via the queries “What is [observer]’s relationship
with the [observed entity]?” and “[Observed entity] is [action status
of the observed entity]”, and their answers summarized together.
The output suggests that John could consider asking Eddy about
his music composition project. We then regenerate the agent’s
existing plan starting from the time when the reaction takes place.
Finally, if the action indicates an interaction between agents, we
generate their dialogue.
4.3.2 Dialogue. Agents converse as they interact with each other.
We generate agents’ dialogue by conditioning their utterances on
their memories about each other. For example, when John initiates
his conversation with Eddy, we generate John’s first utterance
by using his summarized memory about Eddy and the intended
reaction when he decided to ask Eddy about his composition project:
[Agent’s Summary Description]
It is February 13, 2023, 4:56 pm.
UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein
John Lin’s status: John is back home early from
work.
Observation: John saw Eddy taking a short walk
around his workplace.
Summary of relevant context from John’s memory:
Eddy Lin is John’s Lin’s son. Eddy Lin has been
working on a music composition for his class. Eddy
Lin likes to walk around the garden when he is
thinking about or listening to music.
John is asking Eddy about his music composition
project. What would he say to Eddy?
The result: “Hey Eddy, how’s the music composition project for
your class coming along?” From Eddy’s perspective, John initiating
the dialogue is seen as an event to which he may want to react.
So, just as John did, Eddy retrieves and summarizes his memory
about his relationship with John, as well as his memory that may
be related to John’s last utterance in the dialogue. If he decides
to respond, we generate Eddy’s utterance using his summarized
memory and the current dialogue history:
[Agent’s Summary Description]
It is February 13, 2023, 4:56 pm.
Eddy Lin’s status: Eddy is taking a short walk
around his workplace.
Observation: John is initiating a conversation
with Eddy.
Summary of relevant context from Eddy’s memory:
John Lin is Eddy Lin’s father. John Lin is caring
and is interested to learn more about Eddy Lin’s
school work. John Lin knows that Eddy Lin is
working on a music composition.
Here is the dialogue history:
John: Hey Eddy, how’s the music composition project
for your class coming along?
How would Eddy respond to John?
This generates Eddy’s response: “Hey Dad, it’s going well. I’ve been
taking walks around the garden to clear my head and get some
inspiration.” The continuation of this dialogue is generated using
the same mechanism until one of the two agents decides to end the
dialogue.

##### 0Mem #####

0Mem is a memory system designed to be simple, efficient, and effective for LLM agents.

Интересно кај 0Mem а и што мислам дека е добар начин е динамички промпт кој се полни од 
системот како пред контекст на агентот.
Прв дел што бил најмалку променлив е главните карактеристики на агентот
како колку години има, име, професија, какви особини има позитивни, негативни,
пример што сака што неќи.
Некои работи можи да се менуваат во овај дел. 
Пример можи да има особина дека се плаши од кучиња.
После со евент во животот со некаква терапија или нешто да го изгуби стравот.
или слично.

Втор дел е листа на претходни акции во времето.
Таа листа треба да се обновува со нови случувања во светот обзервирани 
од агентот. Но да не стани листа преголема старите работи по некаква функција
да се префрлаат во перманентна меморија. Да се сумираат да се изведат закључоци и чувства и мислења(opinion
)
и да се префрлаат во перманентна меморија.

Tретиот дел се пополнува од прашањето дали во промптот има
доволно продатоци или воопшто за поставената цел. Ако нема треба да пристапиме 
на перманетната меморија или RAG + graph DB + стандардна DB из

Целта дефинирана(можи да е дел од планот една ставка) пример пронајди вода.

Позиција: Каде агентот моментално се наоѓа и/или тој шо го услужува.

##### Треба да се земи во обзир при конструкцијата #####
Агентот треба да можи да го проширува заењето.
Пример ако го ставиме во непозната соба или место во светот.
При разгледувањето ќе забележува предмети и објекти.
Пример ќе биди во некоја непозната затворена со ѕидови. 
Ќе треба да ги мапира и нивните релации во однос на другите објекти и нивните својства.
Размислувам како овие објекти ќе бидат преставени дигитално.
Пример во собата можи да има 4 столици. Пример локацијата   x,y 
ќе чува како посебно својство на објектот, но не мора агентот да знае.
Ако треба да оди до некој објект агентот треба да посочи.
Но имаме 4 столици. Секој објект е само стол, како да се разликува од
друг стол. Дали во имињата на објектите му ставиме ИД пример стол-ИД. Размисли?
Но да бидат поврзани и со локацијата во која е агентот или
локацијата за која се зборува или размислува. 
Односно фокус за да не се пребарува низ сите столови што агентот 
ги видел од почеток.


Особено е важно дека објектите можат да имат статус. 
Пример кога гледа агентот можи да види дека некоја столица е зафатена
а некоја слободна. Иако тој можеби се сеќава или претпоставува дека во 
кафичот има столица.Битен и нејзиниот статус за следната акција на агентот.
Исто така работите можат да се смената. Пример пред да влези во кафичот
можи да има паркирано точак. Но ко ќе излези точакот ќе го нема. 
Значи меморијата треба да се прочита и треба за точакот треба да се адаптира.

Системот во .NET треба да се прави но пајтон кодот можи да се користи како идеја.




